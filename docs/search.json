[
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8_notebook.html",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8_notebook.html",
    "title": "Siyun Min",
    "section": "",
    "text": "Probability and Statistics Week 8\n\ntoc: true\nbadges: true\ncomments: true\ncategories: [jupyter]\n\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\nfrom scipy import special\n\n\n# exponential\n\nfor lam in [2.0, 1.0, 0.5]:\n    x = np.linspace(0, 3, 100)\n    y = lam * np.exp(-1 * lam * x)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('pdf')\n\nplt.show()\nplt.close()\n\n\n\n\n\n# exponential\n\nfor lam in [2.0, 1.0, 0.5]:\n    x = np.linspace(0, 3, 100)\n    y = 1 -  np.exp(-1 * lam * x)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('cdf')\n\nplt.show()\nplt.close()\n\n\n\n\n\n# exponential\n\nfor lam in [2.0, 1.0, 0.5]:\n    x = np.linspace(0, 3, 100)\n    y = np.exp(-1 * lam * x)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('ccdf')\n\nplt.yscale('log')\nplt.show()\nplt.close()\n\n\n\n\n\ndf = pd.read_csv('./LectureCode/babyboom.tsv', sep='\\t')\n\n\ndef value2cdf(values):\n    sorted_values = sorted(values)\n    total = len(values)\n    x = []\n    y = []\n    for i, value in enumerate(sorted_values):\n        x.append(value)\n        y.append(i / total)\n    \n    return x, y\n\n\ndef value2ccdf(values):\n    sorted_values = sorted(values)\n    total = len(values)\n    x = []\n    y = []\n    for i, value in enumerate(sorted_values):\n        x.append(value)\n        y.append(1 - i / total)\n    \n    return x, y\n\n\ndiffs = df['minutes'].diff().dropna()\nxval, cdf = value2cdf(diffs)\nplt.step(xval, cdf)\nplt.xlabel('time between brith (min)')\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\n\n\n\n\ndiffs = df['minutes'].diff().dropna()\nxval, ccdf = value2ccdf(diffs)\nplt.step(xval, ccdf)\nplt.xlabel('time between brith (min)')\nplt.ylabel('ccdf')\nplt.yscale('log')\nplt.show()\nplt.close()\n\n\n\n\n\ndef value2pdf(values, bins=10):\n    tot = len(values)\n    hist, bins = np.histogram(values, bins=bins)\n    xval = bins[:-1] + 0.5 * (bins[1:] - bins[:-1])\n    binsizes = (bins[1:] - bins[:-1])\n    pdf = hist / (binsizes * tot)\n    return xval, pdf\n\ndef value2pmf(values, bins=10):\n    tot = len(values)\n    hist, bins = np.histogram(values, bins=bins)\n    xval = bins[:-1] + 0.5 * (bins[1:] - bins[:-1])\n    pmf = hist / tot\n    return xval, pmf\n\n\nxval, pdf = value2pdf(diffs, [0, 10, 15, 30, 60, 120])\nplt.step(xval, pdf)\nplt.xlabel('time between births (min)')\nplt.xlabel('pdf')\nplt.yscale('log')\nplt.show()\nplt.close()\n\n\n\n\n\nmus = [1.0, 2.0, 3.0]\nsigmas = [0.5, 0.4, 0.3]\nfor mu, sigma in zip(mus, sigmas):\n    x = np.linspace(-1, 4, 100)\n    y = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-1 / 2 * ((x - mu) / sigma) ** 2)\n    plt.plot(x, y, label=f\"{mu}, {sigma}\")\n\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nmus = [1.0, 2.0, 3.0]\nsigmas = [0.5, 0.4, 0.3]\nfor mu, sigma in zip(mus, sigmas):\n    x = np.linspace(-1, 4, 100)\n    y = 1 / 2 * (1 + special.erf((x - mu) / (sigma * np.sqrt(2))))\n    plt.plot(x, y, label=f\"{mu}, {sigma}\")\n\nplt.legend()\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\n\n\n\n\npreg = pd.read_csv('./LectureCode/2002FemPreg.tsv', sep='\\t')\nweights = preg['totalwgt_lb'].dropna()\n\n\nx, pdf = value2pdf(weights, 100)\nplt.plot(x, pdf, label='data')\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\nmodel_x = x\nmodel_pdf = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(-1 / 2 * ((model_x - mean) / std) ** 2)\nplt.plot(model_x, model_pdf, label='model')\n\nplt.xlabel('birth weights')\nplt.ylabel('pdf')\nplt.show()\nplt.close()\n\n\n\n\n\nx, cdf = value2cdf(weights)\nplt.plot(x, cdf, label='data')\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\nmodel_x = np.linspace(0, 15, 100)\nmodel_cdf = 1 / 2 * (1 + special.erf((model_x - mean) / (std * np.sqrt(2))))\nplt.plot(model_x, model_cdf, label='model')\n\nplt.xlabel('birth weights')\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\n\n\n\n\ntrimmed_weights = sorted(weights)[int(0.01 * len(weights)):int((1 - 0.01) * len(weights))]\n\n\nx, pdf = value2pdf(trimmed_weights, 100)\nplt.plot(x, pdf, label='data')\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\nmodel_x = x\nmodel_pdf = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(-1 / 2 * ((model_x - mean) / std) ** 2)\nplt.plot(model_x, model_pdf, label='model')\n\nplt.xlabel('birth weights')\nplt.ylabel('pdf')\nplt.show()\nplt.close()\n\n\n\n\n\nx, cdf = value2cdf(trimmed_weights)\nplt.plot(x, cdf, label='data')\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\nmodel_x = np.linspace(0, 15, 100)\nmodel_cdf = 1 / 2 * (1 + special.erf((model_x - mean) / (std * np.sqrt(2))))\nplt.plot(model_x, model_cdf, label='model')\n\nplt.xlabel('birth weights')\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\n\n\n\n\nx, pdf = value2pdf(weights, 100)\n# plt.plot(x, pdf, label='data')\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\n# model_x = x\n# model_pdf = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(-1 / 2 * ((model_x - mean) / std) ** 2)\n# plt.plot(model_x, model_pdf, label='model')\n\nestimator = stats.gaussian_kde(weights)\nK = estimator(x)\nplt.plot(x, K, label='kde')\n\nplt.xlabel('birth weights')\nplt.ylabel('pdf')\nplt.show()\nplt.close()\n\n\n\n\n\nn = 100000\nmus = [1.0, 2.0, 3.0]\nsigmas = [0.5, 0.4, 0.3]\nfor mu, sigma in zip(mus, sigmas):\n    std_sample = np.random.normal(0, 1, n)\n    sample = np.random.normal(mu, sigma, n)\n    plt.plot(sorted(std_sample), sorted(sample))\n\nplt.legend()\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\nNo handles with labels found to put in legend.\n\n\n\n\n\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\n\nxs = [-4, 4]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color='lightgrey', linewidth=4)\n\nsample_xs = np.random.normal(0, 1, len(weights))\nplt.plot(sorted(sample_xs), sorted(weights))\n\nplt.xlabel('standard normal distribution')\nplt.ylabel('observed value')\nplt.show()\nplt.close()\n\n\n\n\n\nfull_term = preg[preg['prglngth'] >= 37]\nterm_weights = full_term['totalwgt_lb'].dropna()\n\nmean = np.mean(term_weights)\nstd = np.sqrt(np.var(term_weights))\n\nxs = [-4, 4]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color='lightgrey', linewidth=4)\n\nsample_xs = np.random.normal(0, 1, len(term_weights))\nplt.plot(sorted(sample_xs), sorted(term_weights))\n\nplt.xlabel('standard normal distribution')\nplt.ylabel('observed value')\nplt.show()\nplt.close()\n\n\n\n\n\ndf = pd.read_csv('./LectureCode/BRFSS.tsv', sep='\\t')\nweights = df['wtkg2'].dropna()\n\n\nx, pdf = value2pdf(weights, 100)\nplt.plot(x, pdf)\nplt.xscale('log')\n\n\n\n\n\nx, cdf = value2cdf(weights)\nplt.plot(x, cdf)\n\n\n\n\n\nx, cdf = value2cdf(weights)\nplt.plot(x, cdf)\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\nmodel_x = np.linspace(10, 250, 100)\nmodel_cdf = 1 / 2 * (1 + special.erf((model_x - mean) / (std * np.sqrt(2))))\nplt.plot(model_x, model_cdf)\nplt.plot(x, cdf)\n\n\n\n\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\n\nxs = [-5, 5]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color='lightgrey', linewidth=4)\nsample_xs = np.random.normal(0, 1, len(weights))\nplt.plot(sorted(sample_xs), sorted(weights))\n\nplt.xlabel('standard normal distribution')\nplt.ylabel('observed value')\nplt.show()\nplt.close()\n\n\n\n\n\nx, cdf = value2cdf(np.log10(weights))\nplt.plot(x, cdf, label='data')\n\nmean = np.mean(np.log10(weights))\nstd = np.sqrt(np.var(np.log10(weights)))\nmodel_x = np.linspace(1, 2.5, 100)\nmodel_cdf = 1 / 2 * (1 + special.erf((model_x - mean) / (std * np.sqrt(2))))\nplt.plot(model_x, model_cdf, label='model')\n\nplt.xlabel('birth weights')\nplt.ylabel('cdf')\nplt.show()\nplt.close()\n\n\n\n\n\nmean = np.mean(np.log10(weights))\nstd = np.sqrt(np.var(np.log10(weights)))\n\nxs = [-5, 5]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color='lightgrey', linewidth=4)\nsample_xs = np.random.normal(0, 1, len(np.log10(weights)))\nplt.plot(sorted(sample_xs), sorted(np.log10(weights)))\n\nplt.xlabel('standard normal distribution')\nplt.ylabel('observed value')\nplt.show()\nplt.close()\n\n\n\n\n\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 100)\n    model_pdf = alpha * (xmin ** alpha) / (model_x ** (alpha + 1))\n    plt.plot(model_x, model_pdf, label=alpha)\n\nplt.xscale('log')\nplt.yscale('log')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 100)\n    model_cdf = 1 - (xmin ** alpha) / (model_x ** (alpha + 1))\n    plt.plot(model_x, model_cdf, label=alpha)\n\n# plt.xscale('log')\n# plt.yscale('log')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 100)\n    model_ccdf = (xmin ** alpha) / (model_x ** (alpha + 1))\n    plt.plot(model_x, model_ccdf, label=alpha)\n\nplt.xscale('log')\nplt.yscale('log')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\npops = pd.read_csv('./LectureCode/PEP_2012_PEPANNRES_with_ann.tsv', sep='\\t')['Populations']\n\n\nx, ccdf = value2ccdf(pops.values)\nplt.plot(x, ccdf, label='data')\nplt.xscale('log')\nplt.yscale('log')\n\nalpha = 1.4\nxmin = 5000\nmodel_x = np.linspace(xmin, 1e07, 100)\nmodel_ccdf = xmin ** alpha / model_x ** alpha\n\nplt.plot(model_x, model_ccdf)\nplt.show()\nplt.close()\n\n\n\n\n\nx, pdf = value2pdf(pops.values, [5000 * 1.1 ** x for x in range(16)])\nplt.plot(x, pdf, label='data')\nplt.xscale('log')\nplt.yscale('log')\n\n\n\n\n\nlog10_pops = np.log10(pops)\nx, cdf = value2cdf(log10_pops)\n\nmean = np.mean(log10_pops)\nstd = np.sqrt(np.var(log10_pops))\nmodel_x = np.linspace(0, 8, 100)\nmodel_cdf = 1 / 2 * (1 + special.erf((model_x - mean) / (std * np.sqrt(2))))\n\nplt.plot(x, cdf)\nplt.plot(model_x, model_cdf)\n\n\n\n\n\nmean = np.mean(log10_pops)\nstd = np.sqrt(np.var(log10_pops))\n\nxs = [-5, 5]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color='lightgrey', linewidth=4)\nsample_xs = np.random.normal(0, 1, len(log10_pops))\nplt.plot(sorted(sample_xs), sorted(log10_pops))\n\nplt.xlabel('standard normal distribution')\nplt.ylabel('observed value')\nplt.show()\nplt.close()\n\n\n\n\n\ndef RawMoment(xs, k):\n    return sum(x ** k for x in xs) / len(xs)\n\n\ndf = pd.read_csv('./LectureCode/BRFSS.tsv', sep='\\t')\n\n\nfemale = df[df['sex'] == 2]\nfemale_heights = female['htm3'].dropna()\nmean, std = female_heights.mean(), female_heights.std()\nmean, std\n\n(163.22347500412215, 7.269156286642232)\n\n\n\nRawMoment(female_heights, 1)\n\n163.22347500412215\n\n\n\ndef CentralMonent(xs, k):\n    mean = RawMoment(xs, 1)\n    return sum((x - mean) ** k for x in xs) / len(xs)\n\n\nnp.sqrt(CentralMonent(female_heights, 2))\n\n7.269142017823925"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8.html",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8.html",
    "title": "Probability and Statistics Week 8",
    "section": "",
    "text": "image-20211023032525857\n\n\ni.i.d = independent and identical distribution\n\n\n\nimage-20211023032706354\n\n\n\n\n\nimage-20211023032730046\n\n\n\n\n\nimage-20211023032928880\n\n\n\n\n\n\n\n\nimage-20211023033038401\n\n\n\n\n\nimage-20211023033151602\n\n\n\n\n\nimage-20211023033153314\n\n\n\n\n\n\n\nimage-20211023033500356\n\n\n\n\n\n\n\n\nimage-20211023033522882\n\n\n\n\n\n\n\n\nimage-20211023033600360\n\n\n\n\n\n\n\n\n\nimage-20211023033619397\n\n\n\n\n\n\n\n\nimage-20211023033855628\n\n\n\n\n\nimage-20211023034007247\n\n\n\n\n\n\n\n\nimage-20211023034137186\n\n\n\n\n\nimage-20211023034206791\n\n\n\n\n\n\n\n\nimage-20211023034300759\n\n\n\n\n\n\n\nimage-20211023034322592\n\n\n\n\n\nimage-20211023034439368\n\n\n\n\n\n\n\n\nimage-20211023034540869\n\n\n\n\n\nimage-20211023034648436\n\n\n\n\n\n\n\n\n\nimage-20211023034848204\n\n\n\n\n\nimage-20211023034905643"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html",
    "title": "Siyun Min",
    "section": "",
    "text": "---\ntitle: \"Probability and Statistics Week 8 Homework\"\nauthor: \"Siyun Min\"\ndate: \"2021-10-23\"\nformat:\n    html:\n        code-fold: false\n---"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#exponential-distribution",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#exponential-distribution",
    "title": "Siyun Min",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nExponential distribution의 pdf는 아래와 같습니다.\n\\(f(x) = \\lambda e ^{-\\lambda x}\\)\nPMF와 다르게, PDF의 경우는 1을 넘는 값을 가질 수도 있다는 사실을 알아두세요.\n\nfor lam in [2.0, 1, 0.5]:\n    x = np.linspace(0, 3, 100) #np.linspace(a, b, n) = a와 b사이에 일정한 간격으로 n개의 요소를 만듭니다\n    y = lam * np.exp(-1 * lam * x)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"p.d.f.\")\nplt.show()\nplt.close()\n\n\n\n\nExponential distribution의 cdf는 아래와 같습니다.\n\\(F(x) = 1 - e^{-\\lambda x}\\)\n\nfor lam in [2.0, 1, 0.5]:\n    x = np.linspace(0, 3, 100) #np.linspace(a, b, n) = a와 b사이에 일정한 간격으로 n개의 요소를 만듭니다\n    y = 1 - np.exp(-1 * lam * x)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"c.d.f.\")\nplt.show()\nplt.close()\n\n\n\n\n현실 세계에서 연속적으로 일어나는 사건을 보고, 이 사이의 시간 간격을 재면 지수분포가 등장합니다. 사건이 대략적으로 균일하게 발생하는 것 같다면, 시간간격은 보통 지수분포를 띄게 됩니다.\n교과서 예제를 가져와봅시다. 1997년 12월 18일 호주 브리즈번 1구에서 에서 44 명 신생아가 출생했습니다. 모든 44명의 신생아 출생 시간을 가져와봅시다. 교과서의 소스의 dat파일을 tsv로 변환한 babyboom.tsv파일을 불러옵시다\n\ndf = pd.read_csv(\"./babyboom.tsv\", sep=\"\\t\")\n\n\ndef values2cdf(values):\n    sorted_values = sorted(values)\n    total = len(values)\n    x = []\n    y = []\n    for i, value in enumerate(sorted_values):\n        x.append(value)\n        y.append(i/total)\n    return x, y\n\n\ndef values2ccdf(values):\n    sorted_values = sorted(values)\n    total = len(values)\n    x = []\n    y = []\n    for i, value in enumerate(sorted_values):\n        x.append(value)\n        y.append(1-i/total)\n    return x, y\n\n\ndiffs = df.minutes.diff().dropna() # 각 출생 사이의 시간 간격을 재 봅시다. \nxval, cdf = values2cdf(diffs)\nplt.step(xval, cdf)\nplt.xlabel(\"Time between births (min)\")\nplt.ylabel(\"CDF\")\nplt.show()\n\n\n\n\n\nxval, ccdf = values2ccdf(diffs)\nplt.step(xval, ccdf)\nplt.xlabel(\"Time between births (min)\")\nplt.ylabel(\"CCDF\")\nplt.show()\n\n\n\n\nexpoential distribution은 df와 ccdf 모두 y를 log축으로 바꾸면 직선처럼 보입니다.\n\nxval, ccdf = values2ccdf(diffs)\nplt.step(xval, ccdf)\nplt.xlabel(\"Time between births (min)\")\nplt.ylabel(\"CCDF\")\nplt.yscale(\"log\")\nplt.show()\n\n\n\n\nHistogram을 통해서 pdf를 만들어봅시다. pdf는 pmf와 다르게 bin size로 histogram을 한 번 더 나누어줘야 합니다.\n\ndef values2pdf(values, bins=10):\n    import numpy as np\n    tot = len(values)   \n    hist, bins = np.histogram(values, bins=bins) \n    xval = bins[:-1] + 0.5 * (bins[1:] - bins[:-1]) # 두 bin 사이의 점을 고릅니다\n    binsizes = (bins[1:] - bins[:-1]) # binsize가 uniform이 아니더라도 쓸 수 있도록\n    pdf = (hist / tot) / binsizes\n    return xval, pdf\n    \ndef values2pmf(values, bins=10):\n    import numpy as np\n    tot = len(values)\n    hist, bins = np.histogram(values, bins=bins) \n    xval = bins[:-1] + 0.5 * (bins[1:] - bins[:-1]) # 두 bin 사이의 점을 고릅니다\n    return xval, hist/tot\n\n\nxval, pdf = values2pdf(diffs, [0, 10, 15, 30, 50, 120])\nplt.step(xval, pdf)\nplt.xlabel(\"Time between births (min)\")\nplt.ylabel(\"pdf\")\nplt.yscale(\"log\")\nplt.show()"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#gaussian-distribution",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#gaussian-distribution",
    "title": "Siyun Min",
    "section": "Gaussian distribution",
    "text": "Gaussian distribution\nGaussian distribution의 pdf는 아래와 같습니다.\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\nPDF를 그려봅시다\n\nmus = [1.0, 2.0, 3.0]\nsigmas = [0.5, 0.4, 0.3]\nfor mu, sigma in zip (mus, sigmas):\n    x = np.linspace(-1, 4, 100)\n    y = 1/(sigma*np.sqrt(np.pi)) * np.exp(-1/2 * ((x - mu)/sigma)**2)\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"p.d.f.\")\n\n\n\n\nc.d.f.는 아래와 같습니다.\n\\(F(x) = \\frac{1}{2}[1+erf(\\frac{x-\\mu}{\\sigma\\sqrt{2}})]\\)\n여기서 erf는 error function을 말합니다 (https://en.wikipedia.org/wiki/Error_function)\n\nmus = [1.0, 2.0, 3.0]\nsigmas = [0.5, 0.4, 0.3]\nfor mu, sigma in zip (mus, sigmas):\n    x = np.linspace(-1, 4, 100)\n    y = 1/2 * (1 + special.erf((x-mu)/(sigma*np.sqrt(2))))\n    plt.plot(x, y, label=f\"$\\lambda=${lam}\")\n    plt.legend()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"c.d.f.\")\n\n\n\n\n항상 하던 것 처럼 preg 데이터를 불러옵시다\n\npreg = pd.read_csv(\"./2002FemPreg.tsv\", sep = \"\\t\")\nweights = preg.totalwgt_lb.dropna()\n\n\nx, pdf = values2pdf(weights, 100)\nplt.plot(x, pdf, label=\"data\")\nplt.legend()\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\nmodel_x = np.linspace(0, 15, 100)\nmodel_pdf = 1/(std*np.sqrt(np.pi)) * np.exp(-1/2 * ((model_x - mean)/std)**2)\nplt.plot(model_x, model_pdf, label=\"model\")\n\nplt.xlabel(\"Birth weights\")\nplt.ylabel(\"PDF\")\nplt.legend()\nplt.show()\n\n\n\n\n\nx, cdf = values2cdf(weights)\nplt.plot(x, cdf, label=\"data\")\nplt.legend()\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\nmodel_x = np.linspace(0, 15, 100)\nmodel_cdf =  1/2 * (1 + special.erf((model_x-mean)/(std*np.sqrt(2))))\nplt.plot(model_x, model_cdf, label=\"model\")\n\nplt.xlabel(\"Birth weights\")\nplt.ylabel(\"CDF\")\nplt.legend()\nplt.show()\n\n\n\n\n조금 더 그래프를 개선하기 위해서 맨 위 아래의 1%정도를 잘라봅시다.\n\ntrimmed_weights = sorted(weights)[int(0.01*len(weights)):int((1-0.01)*len(weights))]\n\n\nx, pdf = values2pdf(trimmed_weights, 50)\nplt.plot(x, pdf, label=\"data\")\nplt.legend()\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\nmodel_x = np.linspace(0, 15, 50)\nmodel_pdf = 1/(std*np.sqrt(np.pi)) * np.exp(-1/2 * ((model_x - mean)/std)**2)\nplt.plot(model_x, model_pdf, label=\"model\")\n\nplt.xlabel(\"Birth weights\")\nplt.ylabel(\"PDF\")\nplt.legend()\nplt.show()\n\n\n\n\n\n# kernel density estimation을 통해서 pdf를 조금 더 깔끔하게 보이게 만들 수도 있습니다.\n\nx, pdf = values2pdf(trimmed_weights, 50)\nplt.plot(x, pdf, label=\"data\")\nplt.legend()\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\nmodel_x = np.linspace(0, 15, 50)\nmodel_pdf = 1/(std*np.sqrt(np.pi)) * np.exp(-1/2 * ((model_x - mean)/std)**2)\nplt.plot(model_x, model_pdf, label=\"model\")\n\n\n# Scipy stats는 gaussian kernal을 사용하는 kde를 지원합니다.\n\nestimator = stats.gaussian_kde(trimmed_weights)\nK = estimator(x)\nplt.plot(x, K, label=\"kde\")\nplt.legend()\n\nplt.xlabel(\"Birth weights\")\nplt.ylabel(\"PDF\")\nplt.legend()\nplt.show()\n\n\n\n\n\nx, cdf = values2cdf(trimmed_weights)\nplt.plot(x, cdf, label=\"data\")\nplt.legend()\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\nmodel_x = np.linspace(0, 15, 100)\nmodel_cdf =  1/2 * (1 + special.erf((model_x-mean)/(std*np.sqrt(2))))\nplt.plot(model_x, model_cdf, label=\"model\")\n\nplt.xlabel(\"Birth weights\")\nplt.ylabel(\"CDF\")\nplt.legend()\nplt.show()\n\n\n\n\n정규확률그림(A normal probability plot)은 Gaussian distribution이 이 데이터에 대해서 적합한지 눈으로 확인할 수 있는 방법입니다. 1. 먼저 보고자 하는 sample과 동일한 수의 표준 정규 분포 변수를 만듭니다. 2. 두 변수 리스트를 오른차순 정렬해서 쌍을 만듭니다. 3. 그리고 그 각각을 x축과 y축의 값으로 그래프를 그립니다.\n이 방법을 사용하면 normal distribution에서 나온 데이터는 그래프상에서 직선으로 보입니다. 실제로 Gaussian random number가 직선으로 보이는지 확인해봅시다.\n\nn = 1000\nmus = [0, 1, 5]\nsigmas = [1, 1, 2]\n\nfor mu, sigma in zip(mus, sigmas):\n    std_sample = np.random.normal(0, 1, n) # 표준 정규 분포\n    sample = np.random.normal(mu, sigma, n)\n    plt.plot(sorted(std_sample), sorted(sample))\n\nplt.xlabel(\"standard normal sample\")\nplt.ylabel(\"sample values\")\nplt.show()\n\n\n\n\n이제 birth weights에 대해서 같은 분석을 해 봅시다.\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\n\n# Mudel distribution의 Guideline을 그려봅시다\nxs = [-4, 4]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color=\"lightgrey\", linewidth=4)\n\nsample_xs = np.random.normal(0, 1, len(weights))\nplt.plot(sorted(sample_xs), sorted(weights), label=\"all live\")\n    \nplt.xlabel(\"observed values\")\nplt.ylabel(\"std normal\")\nplt.legend()\nplt.show()\n\n\n\n\n약간의 오차가 보이고, 양 끝단에서 잘 맞지 않는 것을 알 수 있습니다. 조산의 경우 때문에 그럴 가능성이 있으니, 37주 이후에 태어난 아이만 가지고 다시 분석해봅시다.\n\nfull_term = preg[preg.prglngth >= 37]\nterm_weights = full_term.totalwgt_lb.dropna()\n\n\nmean = np.mean(trimmed_weights)\nstd = np.sqrt(np.var(trimmed_weights))\n\n# Mudel distribution의 Guideline을 그려봅시다\nxs = [-4, 4]\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color=\"lightgrey\", linewidth=4)\n\nsample_xs = np.random.normal(0, 1, len(weights))\nplt.plot(sorted(sample_xs), sorted(weights), label=\"all live\")\n\nsample_xs2 = np.random.normal(0, 1, len(trimmed_weights))\nplt.plot(sorted(sample_xs2), sorted(trimmed_weights), label=\"full term\")\n\nplt.xlabel(\"std normal\")\nplt.ylabel(\"observed values\")\nplt.legend()\nplt.show()\n\n\n\n\n훨씬 더 정확해짐을 알 수 있습니다. 그래도 오차는 있네요. 원래 Real data가 그렇습니다."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#lognormal-distribution",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#lognormal-distribution",
    "title": "Siyun Min",
    "section": "Lognormal distribution",
    "text": "Lognormal distribution\nlognormal 분포는 값에 log를 취하는 경우 gaussian을 따라가는 분포를 말합니다.\nlognormal의 대표적인 예로 성인의 몸무게를 들 수 있습니다. BRFSS는 Behavioral Risk Factor Surveillance System의 약자로 CDC에서 전화로 조사한 건강 관련 기초 설문조사입니다.\n\ndf = pd.read_csv(\"./BRFSS.tsv\", sep=\"\\t\")\nweights = df.wtkg2.dropna()\n\n\n# pdf를 그려봅시다\nx, pdf = values2pdf(weights, 100)\nplt.plot(x, pdf)\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"pdf\")\nplt.show()\nplt.close()\n\n\n\n\n\n# cdf를 그려봅시다\nx, cdf = values2cdf(weights)\nplt.plot(x, cdf)\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"cdf\")\nplt.show()\nplt.close()\n\n\n\n\n\n# xscale을 log로 바꾸면 gaussian distribution같이 보입니다.\n# pdf를 그려봅시다\nx, pdf = values2pdf(weights, 30)\nplt.plot(x, pdf)\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"pdf\")\nplt.xscale(\"log\")\nplt.show()\nplt.close()\n\n\n\n\n\n# cdf를 그려봅시다\n# Gaussian model과는 잘 맞지 않는 것을 알 수 있습니다.\nx, cdf = values2cdf(weights)\n\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\n\nmodel_x = np.linspace(10, 150, 100)\nmodel_cdf =  1/2 * (1 + special.erf((model_x-mean)/(std*np.sqrt(2))))\n\nplt.plot(model_x, model_cdf, label=\"model\")\n\nplt.plot(x, cdf)\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"cdf\")\nplt.show()\nplt.close()\n\n\n\n\n\n# Normal Probability Plot으로도 잘 맞지 않는 것을 확인할 수 있습니다.\n\nxs = [-5, 5]\nmean = np.mean(weights)\nstd = np.sqrt(np.var(weights))\n\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color=\"lightgrey\", linewidth=4, label=\"model\")\n\nsample_xs = np.random.normal(0, 1, len(weights))\nplt.plot(sorted(sample_xs), sorted(weights), label=\"weights\")\n\nplt.xlabel(\"std normal\")\nplt.ylabel(\"observed values\")\nplt.legend()\nplt.show()\n\n\n\n\n\n# log weight를 활용해서 이 분포가 log normal인지 테스트 해 봅시다.\n# 먼저 CDF로 테스트 해 보겠습니다.\n\nlog10_weights = np.log10(weights)\nx, cdf = values2cdf(log10_weights)\n\nmean = np.mean(log10_weights)\nstd = np.sqrt(np.var(log10_weights))\n\nmodel_x = np.linspace(1.5, 2.5, 100)\nmodel_cdf =  1/2 * (1 + special.erf((model_x-mean)/(std*np.sqrt(2))))\n\nplt.plot(x, cdf, label=\"data\")\nplt.plot(model_x, model_cdf, label=\"model\")\n\nplt.xlabel(\"$log_{10}$(Weight)\")\nplt.ylabel(\"cdf\")\nplt.show()\nplt.close()\n\n\n\n\n\n# Normal Probability Plot으로 확인해봅시다.\n\nxs = [-5, 5]\nlog10_weights = np.log10(weights)\nx, cdf = values2cdf(log10_weights)\n\nmean = np.mean(log10_weights)\nstd = np.sqrt(np.var(log10_weights))\n\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color=\"lightgrey\", linewidth=4, label=\"model\")\n\nsample_xs = np.random.normal(0, 1, len(log10_weights))\nplt.plot(sorted(sample_xs), sorted(log10_weights), label=\"weights\")\n\nplt.xlabel(\"std normal\")\nplt.ylabel(\"observed values\")\nplt.legend()\nplt.show()\n\n# 훨씬 잘 맞는 것을 알 수 있습니다"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#pareto-distribution-power-law-distribution",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#pareto-distribution-power-law-distribution",
    "title": "Siyun Min",
    "section": "Pareto distribution / Power-law distribution",
    "text": "Pareto distribution / Power-law distribution\nPareto distribution은 아래와 같이 정의됩니다.\np.d.f. $ f(x) = $\nc.d.f. $ F(x) = 1 - ()^{}$\n\n# PDF를 그려봅시다\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 50)\n    model_pdf = alpha * (xmin ** alpha) / (model_x)**(alpha+1)\n    plt.plot(model_x, model_pdf, label=alpha)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.show()\n\n\n\n\n\n# log-log plot에서 직선같이 보이는 특징을 가졌습니다.\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 50)\n    model_pdf = alpha * (xmin ** alpha) / (model_x)**(alpha+1)\n    plt.plot(model_x, model_pdf, label=alpha)\nplt.xlabel(\"x\")\nplt.ylabel(\"p.d.f.\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.show()\n\n\n\n\n\n# CDF를 그려봅시다\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 50)\n    model_cdf = 1 - (xmin ** alpha) / (model_x)**(alpha)\n    plt.plot(model_x, model_cdf, label=alpha)\nplt.xlabel(\"x\")\nplt.ylabel(\"F(x)\")\nplt.show()\n\n\n\n\n\n# Pareto 분포를 설명하는 데에는 CDF보다 CCDF가 더 적합합니다. CCDF는 1-CDF로 정의합니다\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 50)\n    model_cdf = 1 - (xmin ** alpha) / (model_x)**(alpha)\n    plt.plot(model_x, 1 - model_cdf, label=alpha)\nplt.xlabel(\"x\")\nplt.ylabel(\"CCDF\")\nplt.show()\nplt.close()\n\n\n\n\n\n# CCDF또한 log-log에서 직선으로 보입니다.\nxmin = 0.5\nfor alpha in [2.0, 1.0, 0.5]:\n    model_x = np.linspace(0.5, 15, 50)\n    model_cdf = 1 - (xmin ** alpha) / (model_x)**(alpha)\n    plt.plot(model_x, 1 - model_cdf, label=alpha)\nplt.xlabel(\"x\")\nplt.ylabel(\"F(x)\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.show()\nplt.close()\n\n# 이 CCDF에서의 기울기가 exponent alpha입니다.\n# 보통 Pareto 분포라고 말을 할 때는, CCDF의 exponent를 alpha라고 하고\n# power-law인 경우는, CCDF의 exponent가 alpha+1 으로 정의합니다.\n# 두개는 완전히 동등한 분포이지만, 분야와 경우에 따라 다르게 부르고 다른 notation을 가집니다.\n\n\n\n\nPareto 분포가 나오는 대표적인 경우로는 도시들의 인구 분포를 들 수 있습니다.\nPEP_2012_PEPANNRES_with_ann.tsv 파일을 불러와서 분석해 봅시다\n이 파일은 2012년 미국의 도시와 마을들의 인구 분포를 나타냅니다.\n\npops = pd.read_csv(\"./PEP_2012_PEPANNRES_with_ann.tsv\").Populations\nprint('Number of cities/towns', len(pops))\n\nNumber of cities/towns 19516\n\n\n\n# cdf를 그려봅시다\n# Gaussian model과는 잘 맞지 않는 것을 알 수 있습니다.\nx, ccdf = values2ccdf(pops.values)\nplt.plot(x, ccdf, label=\"data\")\n\n# model을 plot 해 봅시다\nalpha = 1.4\nxmin = 5000\nmodel_x = np.linspace(xmin, 1e07, 100)\nmodel_ccdf = (xmin ** alpha) / (model_x)**(alpha)\nplt.plot(model_x, model_ccdf, label=\"model\")\n# 이 Exponent를 실제로 구하는 법은 Maximum-likelihood estimation을 배우면 알게 됩니다.\n\nplt.xlabel(\"Population\")\nplt.ylabel(\"ccdf\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.legend()\nplt.show()\nplt.close()\n\n#tail에 대해서 잘 맞는 것을 알 수 있습니다.\n\n\n\n\n\n# pdf를 볼까요?\nx, pdf = values2pdf(pops.values, 50)\nplt.plot(x, pdf, label=\"data\")\n\nxmin = 5000\nalpha = 1.4\nmodel_x = np.linspace(xmin, 1e07, 100)\nmodel_pdf = alpha * (xmin ** alpha) / (model_x)**(alpha+1)\nplt.plot(model_x, model_pdf, label=alpha)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"p.d.f.\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.show()\n\n\n\n\n\n# 잘 맞지 않는 것 처럼 보입니다. \n# 이유는 분포가 불균일하기 때문에, bin을 균등하게 나누면 제대로 보지 못하기 때문입니다.\n# x값에 따라 bin 사이즈를 2배씩 키우는 식으로 bin을 나눠봅시다.\n\nx, pdf = values2pdf(pops.values, [5000 * 2**x for x in range(16)]) \nplt.plot(x, pdf, label=\"data\")\n\nxmin = 5000\nalpha = 1.4\nmodel_x = np.linspace(xmin, 1e07, 100)\nmodel_pdf = alpha * (xmin ** alpha) / (model_x)**(alpha+1)\nplt.plot(model_x, model_pdf, label=\"model\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"p.d.f.\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.legend()\nplt.show()\n\n\n\n\nReal data에서 power-law와 log-normal을 구분하는 것은 어렵습니다\n참조: https://epubs.siam.org/doi/abs/10.1137/070710111\n저도 이것으로 논문을 하나 썼습니다: https://arxiv.org/abs/1810.08809\n이 논문은 아직 제 자리를 찾지 못해서 구천을 떠돌고 있습니다 -_-\nlognormal이 더 잘 맞을지 모르니 lognormal도 한 번 테스트를 해 봅시다.\n\n# log weight를 활용해서 이 분포가 log normal인지 테스트 해 봅시다.\n# 먼저 CDF로 테스트 해 보겠습니다.\n\nlog10_pops = np.log10(pops)\nx, cdf = values2cdf(log10_pops)\n\nmean = np.mean(log10_pops)\nstd = np.sqrt(np.var(log10_pops))\n\nmodel_x = np.linspace(0, 8, 100)\nmodel_cdf =  1/2 * (1 + special.erf((model_x-mean)/(std*np.sqrt(2))))\n\nplt.plot(x, cdf, label=\"data\")\nplt.plot(model_x, model_cdf, label=\"model\")\n\nplt.xlabel(\"$log_{10}$(Weight)\")\nplt.ylabel(\"cdf\")\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\n# Normal Probability Plot으로 확인해봅시다.\n\nxs = [-5, 5]\nlog10_pops = np.log10(pops)\nx, cdf = values2cdf(log10_pops)\n\nmean = np.mean(log10_pops)\nstd = np.sqrt(np.var(log10_pops))\n\nguide_xs = np.sort(xs)\nguide_ys = mean + std * guide_xs\nplt.plot(guide_xs, guide_ys, color=\"lightgrey\", linewidth=4, label=\"model\")\n\nsample_xs = np.random.normal(0, 1, len(log10_pops))\nplt.plot(sorted(sample_xs), sorted(log10_pops), label=\"weights\")\n\nplt.xlabel(\"std normal\")\nplt.ylabel(\"observed values\")\nplt.legend()\nplt.show()\n\n# Tail에서는 조금 덜 맞고, 중간 부분에서는 비슷한 것을 알 수 있습니다.\n# lognormal이라고 생각하면 tail은 모델 예측값보다 실제 도시가 더 크다는 뜻이겠네요."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#moments",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#moments",
    "title": "Siyun Min",
    "section": "Moments",
    "text": "Moments\n실제 변수에서 Raw moments는 power의 평균으로 정의할 수 있습니다.\n\ndef RawMoment(xs, k):\n    return sum(x**k for x in xs) / len(xs)\n\n\n# BRFSS를 다시 불러옵시다\ndf = pd.read_csv(\"./BRFSS.tsv\", sep=\"\\t\")\nfemale = df[df.sex==2]\nfemale_heights = female.htm3.dropna()\nmean, std = female_heights.mean(), female_heights.std()\nmean, std\n\n(163.22347500412215, 7.269156286641344)\n\n\n\n# 1st moment가 평균과 같은 것을 알 수 있습니다\nRawMoment(female_heights, 1)\n\n163.22347500412215\n\n\nCentral moment는 x에 평균값을 뺀 값들의 moment를 구하면 됩니다.\n\ndef CentralMoment(xs, k):\n    mean = RawMoment(xs, 1)\n    return sum((x-mean)**k for x in xs) / len(xs)\n\n\nnp.sqrt(CentralMoment(female_heights, 2))\n\n7.269142017823925"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#merge-and-groupby",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#merge-and-groupby",
    "title": "Siyun Min",
    "section": "Merge and Groupby",
    "text": "Merge and Groupby\n데이터를 분석하다보면 여러 DataFrame을 합치거나, 한 데이터프레임에서 같은 특성을 공유하는 일부에 대한 계산을 해야할 경우가 있습니다. 이 때 유용한 방법이 merge와 grupby입니다.\n먼저 공통된 행(열)값을 기준으로, 두 DataFrame을 합치는 방법을 merging이라고 합니다.\n일단 두 개의 dataframe을 만들어봅시다.\n\ndf1 = pd.DataFrame({\"key\": list(\"bbaaccaab\"), \"data1\": range(9)})\ndf1\n\n\n\n\n\n  \n    \n      \n      key\n      data1\n    \n  \n  \n    \n      0\n      b\n      0\n    \n    \n      1\n      b\n      1\n    \n    \n      2\n      a\n      2\n    \n    \n      3\n      a\n      3\n    \n    \n      4\n      c\n      4\n    \n    \n      5\n      c\n      5\n    \n    \n      6\n      a\n      6\n    \n    \n      7\n      a\n      7\n    \n    \n      8\n      b\n      8\n    \n  \n\n\n\n\n\ndf2 = pd.DataFrame({\"key\": list(\"abd\"), \"data2\": range(3)})\ndf2\n\n\n\n\n\n  \n    \n      \n      key\n      data2\n    \n  \n  \n    \n      0\n      a\n      0\n    \n    \n      1\n      b\n      1\n    \n    \n      2\n      d\n      2\n    \n  \n\n\n\n\n2개의 dataframe에는 key라는 공통 열이 있습니다. 이를 기준으로 합쳐봅시다.\n\n#df1.merge(df2) # 공통된 이름의 열이 있으면 그냥 이렇게 해도 됩니다.\ndf1.merge(df2, on=\"key\") # 공통된 이름의 열이 여러개거나, 정확히 하려면 on을 통해 합칩니다.\n\n\n\n\n\n  \n    \n      \n      key\n      data1\n      data2\n    \n  \n  \n    \n      0\n      b\n      0\n      1\n    \n    \n      1\n      b\n      1\n      1\n    \n    \n      2\n      b\n      8\n      1\n    \n    \n      3\n      a\n      2\n      0\n    \n    \n      4\n      a\n      3\n      0\n    \n    \n      5\n      a\n      6\n      0\n    \n    \n      6\n      a\n      7\n      0\n    \n  \n\n\n\n\n\n#양쪽에 합치려고 하는 기준 열의 이름이 다르면 어떻게 하면 될까요? 일단 df1의 열 이름을 바꿔봅시다 (key->key2)\ndf1.columns = [\"key2\", \"data1\"]\ndf1\n\n\n\n\n\n  \n    \n      \n      key2\n      data1\n    \n  \n  \n    \n      0\n      b\n      0\n    \n    \n      1\n      b\n      1\n    \n    \n      2\n      a\n      2\n    \n    \n      3\n      a\n      3\n    \n    \n      4\n      c\n      4\n    \n    \n      5\n      c\n      5\n    \n    \n      6\n      a\n      6\n    \n    \n      7\n      a\n      7\n    \n    \n      8\n      b\n      8\n    \n  \n\n\n\n\n\n# 이 때는 left_on, right_on 을 쓰면 됩니다\ndf1.merge(df2, left_on=\"key2\", right_on=\"key\")\n\n\n\n\n\n  \n    \n      \n      key2\n      data1\n      key\n      data2\n    \n  \n  \n    \n      0\n      b\n      0\n      b\n      1\n    \n    \n      1\n      b\n      1\n      b\n      1\n    \n    \n      2\n      b\n      8\n      b\n      1\n    \n    \n      3\n      a\n      2\n      a\n      0\n    \n    \n      4\n      a\n      3\n      a\n      0\n    \n    \n      5\n      a\n      6\n      a\n      0\n    \n    \n      6\n      a\n      7\n      a\n      0\n    \n  \n\n\n\n\n\n# how는 둘 중 하나의 datataframe에 기준열 값이 없을 때 어떤 식으로 처리하는지를 결정해줍니다.\n# 자세한 것은 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html \n# df1.merge(df2, left_on=\"key2\", right_on=\"key\", how=\"left\") # 왼쪽 기준 (df2에 c가 없으므로, c에 대한 값은 다 NaN으로 나옵니다)\n# df1.merge(df2, left_on=\"key2\", right_on=\"key\", how=\"right\") # 오른쪽 기준 (df1에 d가 없으므로, d에 대한 값은 다 NaN으로 나옵니다)\n# df1.merge(df2, left_on=\"key2\", right_on=\"key\", how=\"inner\") # 양쪽에 다 존재하는 것만 보여줍니다\ndf1.merge(df2, left_on=\"key2\", right_on=\"key\", how=\"outer\") # 양쪽 중 한쪽이라도 존재하면 보여줍니다\n\n\n\n\n\n  \n    \n      \n      key2\n      data1\n      key\n      data2\n    \n  \n  \n    \n      0\n      b\n      0.0\n      b\n      1.0\n    \n    \n      1\n      b\n      1.0\n      b\n      1.0\n    \n    \n      2\n      b\n      8.0\n      b\n      1.0\n    \n    \n      3\n      a\n      2.0\n      a\n      0.0\n    \n    \n      4\n      a\n      3.0\n      a\n      0.0\n    \n    \n      5\n      a\n      6.0\n      a\n      0.0\n    \n    \n      6\n      a\n      7.0\n      a\n      0.0\n    \n    \n      7\n      c\n      4.0\n      NaN\n      NaN\n    \n    \n      8\n      c\n      5.0\n      NaN\n      NaN\n    \n    \n      9\n      NaN\n      NaN\n      d\n      2.0\n    \n  \n\n\n\n\ngroupby는 한 열을 기준으로 같은 값을 가진 것을 그룹으로 묶어 연산을 하게 해줍니다. (Grouper를 통해서 규칙을 정해줄 수도 있습니다: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html)\n\n# df1.groupby(\"key2\").mean() # 평균을 구하거나\n#df1.groupby(\"key2\").var() # 분산을 구하거나\ndf1.groupby(\"key2\").count() # 숫자를 구할 수 있습니다.\n# 더 많은 기능을 보시려면 https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html을 참조하세요\n\n\n\n\n\n  \n    \n      \n      data1\n    \n    \n      key2\n      \n    \n  \n  \n    \n      a\n      4\n    \n    \n      b\n      3\n    \n    \n      c\n      2"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#excercises",
    "href": "posts/2021-10-23-probability-and-statistics-week-8/2021-10-23-probability-and-statistics-week-8-hw.html#excercises",
    "title": "Siyun Min",
    "section": "Excercises",
    "text": "Excercises\nExcercise: BRFSS데이터에 따르면, 남성의 신장은 평균 178 cm, 표준편차는 7.7 cm라고 하고, 여성의 경우는 평균 163 cm, 표준편차는 7.3cm라고 합니다.\n특수한 장비를 쓰는 특수부대에 지원하려면 남성의 키는 160cm 에서 180cm 사이여야 한다고 합니다.\n전체 인구중 얼마나 많은 비율이 남성이 이 조건에 해당하나요?\nscipy.stats.norm 을 활용하세요\n\n# Solution goes here\nstats.norm.cdf(180, 178, 7.7) - stats.norm.cdf(160, 178, 7.7)\n\n0.5927655466316241\n\n\n\nbrfss\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      wtyrago\n      finalwt\n      wtkg2\n      htm3\n    \n  \n  \n    \n      0\n      82.0\n      2\n      76.363636\n      185.870345\n      70.91\n      157.0\n    \n    \n      1\n      65.0\n      2\n      72.727273\n      126.603027\n      72.73\n      163.0\n    \n    \n      2\n      48.0\n      2\n      NaN\n      181.063210\n      NaN\n      165.0\n    \n    \n      3\n      61.0\n      1\n      73.636364\n      517.926275\n      73.64\n      170.0\n    \n    \n      4\n      26.0\n      1\n      88.636364\n      1252.624630\n      88.64\n      185.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      414504\n      23.0\n      1\n      84.090909\n      43.439320\n      88.64\n      191.0\n    \n    \n      414505\n      49.0\n      2\n      70.454545\n      6.216538\n      72.73\n      170.0\n    \n    \n      414506\n      45.0\n      1\n      86.363636\n      20.617560\n      90.91\n      178.0\n    \n    \n      414507\n      52.0\n      2\n      89.090909\n      11.188138\n      89.09\n      157.0\n    \n    \n      414508\n      38.0\n      1\n      75.000000\n      25.989792\n      75.00\n      178.0\n    \n  \n\n414509 rows × 6 columns\n\n\n\nExcercise: Pareto 분포에 대해서 이해해보도록 합시다. 사람의 키가 Pareto 분포라면 위의 결과는 어떻게 달라질까요?\n\\(x_{min} = 1\\) 이고 \\(\\alpha = 1.7\\)인 pareto 분포를 이용하면, 최하 키가 1m 이고 median이 1.5m인 분포를 얻을 수 있습니다.\nscipy.stats.Pareto를 활용해서 먼저 pareto 분포를 만들고, 그 평균과 median을 구해보세요.\n\n# Solution goes here\nstats.pareto.mean(1.7), stats.pareto.median(1.7)\n\n(2.428571428571429, 1.5034066538560549)\n\n\n얼마나 많은 사람들이 평균보다 작은 키를 가졌을까요? Gaussian이라면 절반이겠죠?\n\n# Solution goes here\nstats.pareto.cdf(stats.pareto.mean(1.7), 1.7)\n\n0.778739697565288\n\n\n왜 평균보다 작은 사람의 수가 많은데, 평균은 이렇게 클까요? 키가 1km를 넘는 사람의 비율을 구해봅시다.\n\n# Solution goes here\n1 - stats.pareto.cdf(1000, 1.7)\n\n7.943282347211422e-06\n\n\n70억 인구가 있다고 하면, 이중 가장 키가 큰 사람은 몇cm정도 될까요? dist.isf를 이용해보세요\n\n# Solution goes here\nstats.pareto.isf(7.943282347211422e-06, 1.7)\n\n1000.000000002325\n\n\n가장 씁쓸한 점은, 이런 pareto분포의 대표적인 예가 가지고있는 재산이라는 것입니다. 자본주의란 잔인하죠"
  },
  {
    "objectID": "posts/2021-12-06-probability-and-statistics-week-12/2021-12-06-probability-and-statistics-week-12.html",
    "href": "posts/2021-12-06-probability-and-statistics-week-12/2021-12-06-probability-and-statistics-week-12.html",
    "title": "Probability and Statistics Week 12",
    "section": "",
    "text": "image-20211207213035824\n\n\n\n\n\n\n\n\nimage-20211207213045022\n\n\n\n\n\nimage-20211207213103547\n\n\n\n\n\nimage-20211207213107964\n\n\n\n\n\n\n\n\nimage-20211207213124016\n\n\n\n\n\nimage-20211207213129120\n\n\n\n\n\n\n\n\nimage-20211207213139906\n\n\n\n\n\n\n\n\nimage-20211207213153536\n\n\n\n\n\nimage-20211207213158025\n\n\n\n\n\nimage-20211207213202267\n\n\n\n\n\n\n\n\nimage-20211207213214260\n\n\n\n\n\nimage-20211207213218442\n\n\n\n\n\nimage-20211207213224248\n\n\n\n\n\n\n\n\nimage-20211207214304570\n\n\n\n\n\n\n\nimage-20211207214315106\n\n\n\n\n\n\n\n\nimage-20211207214337740\n\n\n\n\n\n\n\n\n\nimage-20211207214350498\n\n\n\n\n\n\n\n\nimage-20211207214403873\n\n\n\n\n\n\n\n\nimage-20211207214653290\n\n\n\n\n\n\n\nimage-20211207214703584\n\n\n\n\n\n\n\n\n\nimage-20211207214718113\n\n\n\n\n\n\n\n\nimage-20211207215016626\n\n\n\n\n\nimage-20211207215022287\n\n\n\n\n\nimage-20211207215027555\n\n\n\n\n\nimage-20211207215033262\n\n\n\n\n\nimage-20211207215039667\n\n\n\n\n\nimage-20211207215046492\n\n\n\n\n\n\n\n\nimage-20211207215100819\n\n\n\n\n\nimage-20211207215413162\n\n\n\n\n\n\n\nimage-20211207215426195\n\n\n\n\n\n\n\n\n\nimage-20211207215445480\n\n\n\n\n\n\n\n\nimage-20211207215457831\n\n\n\n\n\n\n\nimage-20211207215508710\n\n\n\n\n\nimage-20211207215513528\n\n\n\n\n\nimage-20211207215518952\n\n\n\n\n\n\n\n\n\nimage-20211207215533834\n\n\n\n\n\n\n\n\nimage-20211207215542084\n\n\n\n\n\n\n\n\nimage-20211207220156824\n\n\n\n\n\n\n\nimage-20211207220207945\n\n\n\n\n\nimage-20211207220213811\n\n\n\n\n\n\n\n\nimage-20211207220225511\n\n\n\n\n\nimage-20211207220230448\n\n\n\n\n\n\n\n\n\nimage-20211207220242046\n\n\n\n\n\n\n\nimage-20211207220252551\n\n\n\n\n\nimage-20211207220258923\n\n\n\n\n\n\n\n\nimage-20211207220321657\n\n\n\n\n\nimage-20211207220326191\n\n\n\n\n\n\n\n\n\nimage-20211207220341943\n\n\n\n\n\n\n\n\nimage-20211207220536316\n\n\n\n\n\n\n\n\nimage-20211207220551634\n\n\n\n\n\n\n\nimage-20211207220609832\n\n\n\n\n\nimage-20211207220615398\n\n\n\n\n\n\n\n\n\nimage-20211207220625663\n\n\n\n\n\n\n\n\nimage-20211207220636924\n\n\n\n\n\nimage-20211207220646940\n\n\n\n\n\n\n\n\nimage-20211208031747080\n\n\n\n\n\nimage-20211208031754976\n\n\n\n\n\nimage-20211208031802400\n\n\n\n\n\nimage-20211208031810450\n\n\n\n\n\nimage-20211208031818330\n\n\n\n\n\nimage-20211208031826890\n\n\n\n\n\nimage-20211208031835825\n\n\n\n\n\nimage-20211208031846506\n\n\n\n\n\nimage-20211208031853865\n\n\n\n\n\nimage-20211208031900560"
  },
  {
    "objectID": "posts/2021-10-21-CFA-Level-2-Economics/2021-10-21-CFA-Level-2-Economics.html",
    "href": "posts/2021-10-21-CFA-Level-2-Economics/2021-10-21-CFA-Level-2-Economics.html",
    "title": "CFA Level 2 Economics",
    "section": "",
    "text": "The “bid-offer” spread is also known as the “bid-ask” spread.\n\n\na quote of 1.4126 USD/EUR means that each euro costs $1.4126. The euro is called the base currency and the USD the price currency.\n\n\n\nimage-20211021195336133\n\n\nA spot exchange rate is the currency exchange rate for immediate delivery, which for most currencies means the exchange of currencies takes place two days after the trade (2 business days). A forward exchange is a currency exchange rate for an exchange to be done in the future. Forward rates are quoted for various future dates.\n1 month = 30 days in foreign exchange\nDealer quotes often include both bid and offer (ask) rates. For example, the euro could be quoted as $1.4124 - $1.4128. The bid price ($1.4124) is price at which the dealer will buy euros, and the offer price ($1.4128) is the price at which the dealer will sell euros.\n\n\n\nThe difference between the offer and bid price is called the spread. Spread are often stated as ‘pips’. One pip is 1/10,000. Dealers manage their foreign currency inventories by transacting in the interbank market (think of this as a wholesale market for currency). Spread are narrow in the interbank market.\nThe spread quoted by a dealer depends on:\n\nThe spread in an interbank market for the same currency pair. - Dealer spreads vary directly with spreads quoted in the interbank market.\nThe size of transaction. - Larger, liquidity-demanding transactions generally get quoted a larger spread.\nThe relationship between the dealer and client. - Sometimes, dealer will give favorable rates to preferred clients based on other ongoing business relationships.\n\nThe interbank spread on a currency pair depends on:\n\nCurrencies involved - Similar to stocks, high-volume currency pairs command lower spreads than do lower-volume currency pairs.\nTime of day - The time overlap during the trading day when both the New York and London currency markets are open is considered the most liquidity time window; spreads are narrower during this period than at other times of the day.\nMarket volatility - Spreads are directly related to the exchange rate volatility of the currencies involved. Higher volatility leads to higher spreads to compensate market makers for the increased risk of holding those currencies.\n\nIn addition to these factors, spreads in forward exchange rate quotes increases with maturity. The reasons for this are: longer maturity contracts tend to be less liquid, counterparty credit risk in forward contracts increases with maturity, and interest rate risk in forward contracts increases with maturity.\n\n\n\nBuy the base currency at ask, and sell the base currency at bid\nBu the price currency at bid, and sell the price currency at ask\nup-the-bid-and-multiply, down-the-ask-and-divide rule\n\n\n\nimage-20211021200556985\n\n\n\n\n\nimage-20211021200603432\n\n\n\n\n\n\n\n\n\nimage-20211021200643474\n\n\n\n\n\n\n\nimage-20211021200635872\n\n\n\n\n\n\n\n\nimage-20211021200704736\n\n\n\n\n\n\n\n\nimage-20211021200719992\n\n\n\n\n\nimage-20211021200727698\n\n\n\n\n\n\nForward premium - if the forward price (in units of the second currency) is greater than the spot price.\nForward discount - if the forward price (in units of the second currency) is less than the spot price. \\[\n\\\\\\operatorname{forward\\, premium\\, (discount)} = F - S_{0}\n\\] Given a quote of A/B, if the above equation results in a positive value, we say that currency B is trading at a premium in the forward market.\n\n\n\nimage-20211022024433366\n\n\n\n\n\nimage-20211022024447525\n\n\n\n\n\n\n\nThe value of a forward currency contract prior to expiration is also known as the mark-to-market value. \\[\n\\\\\\text{long position}\n\\\\V_{t} = \\frac{(FP_{t} - FP)(\\operatorname{contract size})}{1 + R\\frac{days}{360}}\n\\\\FP_{t} = \\text{forward price (to sell base currency) at time t in the market for a new contract maturing at time T}\n\\\\FP = \\text{forward price specified in the contract at inception (to buy the base currency)}\n\\] \n\n\n\nimage-20211022024801807\n\n\n\n\n\n\n\n\nCovered interest rate parity holds when any forward premium or discount exactly offsets differences in interest rates, so that an investor would earn the same return investing in either currency. If euro interest rates are higher than dollar interest rates, the forward discount on the euro relative to the dollar will just offset the higher euro interest rate.\n \\[\n\\\\F = \\frac{1 + R_{A}\\frac{days}{360}}{1 + R_{B}\\frac{days}{360}}S_{0}\n\\]  \\[\n\\\\\\text{forward premium (discount)} = F - S_{0} = [\\frac{1 + R_{A}\\frac{days}{360}}{1 + R_{B}\\frac{days}{360}} - 1]S_{0}\n\\] \n\n\n\nimage-20211022025307548\n\n\n\n\n\nimage-20211022025316269\n\n\n\n\n\nIf forward currency contracts are not available, or if capital flows are restricted so as to prevent arbitrage, the relationship need not hold. Uncovered interest parity refers to such a situation; \\[\n\\\\E(\\%\\Delta{S})_{A/B} = R_{A} - R_{B}\n\\] \n\n\n\nimage-20211022025448953\n\n\n\n\n\nimage-20211022025454793\n\n\nAn investor that choose to invest in the foreign currency without any additional return (the interest rate differential is offset by currency value changes) is not demanding a risk premium for the foreign currency risk. Hence, uncovered interest rate parity assume that the investor is risk-neutral.\nIf the forward rate is equal to the expected future spot rate, we say that the forward rate is an unbiased predictor of the future spot rate. In such an instance, \\(F = E(S_{1})\\); this is called forward rate parity. In this special case, if covered interest parity holds (and it will; by arbitrage) uncovered interest parity would also hold (and vice versa).\nThere is no reason that uncovered interest rate parity must hold in the short run, and indeed it typically does not. There is evidence that it does generally hold in the long run, so longer-term expected future spot rates based on uncovered interest rate parity are often used as forecasts of future exchange rates.\n\n\n\n\\[\n\\\\R_{nominal} = R_{real} + E(Inflation)\n\\\\(1 + R) = (1 + r)(1 + i_{e})\n\\]\n\n\n\nUnder real interest rate parity, real interest rates are assumed to converge across different markets. Taking the Fisher relation and real interest rate parity together gives us the international Fisher effect. \\[\n\\\\R_{nominal\\, A} - R_{nominal\\, B} = E(inflation_{A}) - E(inflation_{B})\n\\] The argument for the equality of real interest rates across countries is based on the idea that with free capital flows, funds will move to the country with a higher real rate until real rates are equalized.\n\n\n\nThe law of one price states that identical goods should have the same price in all locations.\nHowever, that the law of one price does not hold in practice, due to the effects of frictions such as tariffs and transportation costs.\nInstead of focusing on individual products, absolute purchasing power parity (absolute PPP) compares the average price of a representative basket of consumption goods between countries. Absolute PPP requires only that the law of one price be correct on average, that is, for like baskets of goods in each country. \\[\n\\\\S(A/B) = CPI(A) / CPI(B)\n\\] In practice, even if the law of one price held for every good in two economies, absolute PPP might not hold because the weights (consumption patterns) of the various goods in the two economies may not be the same.\n\n\n\nRelative purchasing power parity (relative PPP) states that changes in exchange rates should exactly offset the price effects of any inflation differential between two countries. \\[\n\\\\\\%\\Delta{S_{A/B}} = Inflation_{A} - Inflation_{B}\n\\] \n\n\n\nThe ex-ante version of purchasing power parity is the same as relative purchasing power parity except that it uses expected inflation instead of actual inflation.\nBecause there is no true arbitrage available to force relative PPP to hold, violations of relative PPP in the short run are common. However, because the evidence suggests that the relative form of PPP holds approximately in the long run, it remains a useful method for estimating the relationship between exchange rates and inflation rates.\n환율예측모형\n\n\n\nimage-20211022062201634\n\n\n\n\n\nimage-20211022062215873\n\n\n\n\n\n\n\nCovered interest parity holds by arbitrage. If forward rate parity holds, uncovered interest rate parity also holds (and vice versa). - Forward rate parity \\(F = E(S)\\)\nInterest rate differentials should mirror inflation differentials. This holds true if the international Fisher relation holds. If that is true, we can also use inflation differentials to forecast future exchange rates-which is the premise of the ex-ante version of PPP. - Real interest rate parity \\(r_{Y} = r_{X}\\)\nIf the ex-ante version of relative PPP as well as the international Fisher relation both hold, uncovered interest rate parity will also hold.\n\n\n\n\n\n\n\n\n\n\nimage-20211022062252136\n\n\n\n\n\n\n\nIf uncovered IRP does not hold\nUncovered interest rate parity states that a currency with a high interest rate should depreciated relative to a currency with a lower interest rate, so that an investor would earn the same return investing in either currency.\nIn a FX carry trade, an investor invests in a higher yielding currency using funds borrowed in a lower yielding currency. The lower yielding currency is called the funding currency.\nThe FX carry trade attempts to capture an interest rate differential and is a bet against uncovered interest rate parity. Carry trades typically perform well during low-volatility periods. Sometimes, higher yields attract larger capital flows, which in turn lead to an economic boom and appreciation (instead of depreciation) of the higher yielding currency. This could make the carry trade even more profitable, because the investor earns a return from currency appreciation in addition to the return from the interest rate spread.\n\n\n\nThe risk is the funding currency may appreciate significantly against the currency of the investment, which would reduce a trader’s profit-or even lead to a loss. Furthermore, the return distribution of the carry trade is not normal; it is characterized by negative skewness and excess kurtosis (i.e., fat tails), meaning that the probability of a large loss is higher than the probability implied under a normal distribution. We call this high probability of a large loss the crash risk of the carry trade.\nAs more investors follow and adopt the same strategy, the demand for high-yielding currency actually pushes it value up. However, with this herding behavior comes the risk that all investors may attempt to exit the trade at the same time. (This is especially true if investors use stop-loss orders in their carry trade.) During turbulent times, as investors exit their positions (i.e., a flight to safety), the high-yielding currency can experience a steep decline in value, generating large losses for traders pursuing FX carry trades.\n\n\n\n\n\n\n\nimage-20211022062322338\n\n\n\n\nBalance-of-payments accounting is a method used to keep track of transactions between a country and its international trading partners.\nThe current account measure the exchange of goods, the exchange of services, the exchange of investment income, and unilateral transfers (gifts to and from other nations).\nThe financial account (also known as the capital account) measures the flow of funds for debt and equity investment into and out of the country.\nWhen a country experiences a current account deficit, it must generate a surplus in its capital account (or see its currency depreciate). Capital flows tend to be the dominant factor influencing exchange rates in the short term, as capital flows tend to be larger and more rapidly changing than goods flows.\n\n\n\n\n\nCurrent account deficits lead to a depreciation of domestic currency via a variety of mechanisms:\n\nFlow supply / demand mechanisms - Current account deficits in a country increase the supply of that currency in the market (As exporters to that country convert their revenues into their own local currency). This puts downward pressure on the exchange value of that currency. The decrease in the value of the currency may restore the current account deficit to a balance-depending on the following factors:\n\nThe initial deficit - The larger the initial deficit, the larger the depreciation of domestic currency needed to restore current account balance.\nThe influence of exchange rates on domestic import and export prices - As a country’s current depreciates, the cost of imported goods increase. However, some of the increase in cost may not be passed on to consumers.\nPrice elasticity of demand of the traded goods - If the most important imports are relatively price inelastic, the quantity imported will not change.\n\n\n\n\n\nimage-20211022032755373\n\n\n\nPortfolio balance mechanism - Countries with current account surpluses usually have capital account deficits, which typically take the form of investments in countries with current account deficits. As a result of these flows of capital, investor countries may find their portfolios’ composition being dominated by few investee currencies. When investor countries decide to rebalance their investment portfolios, it can have a significant negative impact on the value of those investee country currencies.\nDebt sustainability mechanism - A country running a current account deficit may be running a capital account surplus by borrowing from abroad. When the level of debt gets too high relative to GDP, investors may question the sustainability of this level of debt, leading to a rapid depreciation of the borrower’s currency.\n\n\n\n\n\nCapital account flows are one of the major determinants of exchange rates. As capital flows into a country, demand for that country’s currency increase, resulting in appreciation.\nExcessive capital inflows into emerging markets create problems for those countries such as:\n\nExcessive real appreciation of the domestic currency.\nFinancial asset and / or real estate bubbles.\nIncreases in external debt by businesses or government.\nExcessive consumption in the domestic market fueled by credit.\n\nEmerging market government often counteract excessive capital inflows by imposing capital controls or by direct intervention in the foreign exchange markets.\n\n\n\n\n\n\n\nimage-20211022062338751\n\n\n\n\n\n\n\n\n\n\nExpansionary monetary policy and expansionary fiscal policy are likely to have opposite effects on exchange rates.\n\n\n\nimage-20211022033436603\n\n\n\n\n\n\n\n\nimage-20211022033448969\n\n\n\n\n\n\n\n\nimage-20211022033523985\n\n\nAn expansionary (restrictive) monetary policy would lead to depreciation (appreciation) of the domestic currency as stated previously. Under a fixed rate regime, the government would then have to purchase (sell) its own currency in the foreign exchange market. This action essentially reverses the expansionary (restrictive) stance.\n\n\n\nMonetary models only take into account the effect of monetary policy on exchange rates (fiscal policy effects are not considered). With the Mundell-Fleming model, we assume that inflation (price levels) play no role in exchange rate determination. Under monetary models, we assume that output is fixed, so that monetary policy primarily affects inflation, which in turn affects exchange rates.\n\nPure monetary model - Therefore an x% increase in the money supply leads to an x% increase in price levels and then to an x% depreciation of domestic currency.\nDornbusch overshooting model - This model assumes that prices are sticky (inflexible) in the short term and, hence, do not immediately reflect changes in monetary policy (in other words, PPP does not hold in the short term). The model concludes that exchange rates will overshoot the long-run PPP value in the short term. In the case of expansionary monetary policy, prices increase, but over time. Expansionary monetary policy leads to a decrease in interest rates-and a larger-than-PPP-implied depreciation of the domestic currency due to capital outflows. In the long term, exchange rates gradually increase toward their PPP implied values.\n\n\n\n\nimage-20211022034120321\n\n\n\n\n\nThe portfolio balance approach focuses only on the effects of fiscal policy (and not monetary policy). While the Mundell-Fleming model focuses on the short-term implication of fiscal policy, the portfolio balance approach takes a long-term view and evaluates the effects of a sustainable fiscal deficit or surplus on currency values.\nCombining the Mundell-Fleming and portfolio balance approaches, we find that in the short term, with free capital flows, an expansionary fiscal policy leads to domestic currency appreciation (via high interest rates). In the long term, the government has to reverse course (through tighter fiscal policy) leading to depreciation of the domestic currency. If the government does not reverse course, it will have to monetize its debt, which would also lead to depreciation of the domestic currency.\n\n\n\n\n\n\nThe objectives of capital controls or central bank intervention in FX markets are to:\n\nEnsure that the domestic currency does not appreciate excessively.\nAllow the pursuit of independent monetary policies without being hindered by their impact on currency value.\nReduce the aggregate volume of inflow of foreign capital.\n\n\n\n\nFor developed market countries, the volume of trading in a country’s currency is usually very large relative to the foreign exchange reserves of it central bank. Evidence has shown that for developed markets, central banks are relatively ineffective at intervening in the foreign exchange markets due to lack of sufficient resources. Evidence in the case of emerging markets is less clear: central banks of emerging market countries may be able to accumulate sufficient foreign exchange reserves (relative to trading volume) to affect the supply and demand of their currencies in the foreign exchange markets.\n\n\n\n\nThe following conditions have been identified as warning signs in the period leading up to a currency crisis:\n\nTerms of trade (i.e., ratio of exports to imports) deteriorate\nFixed or partially-fixed exchange rates (versus floating exchange rates)\nOfficial foreign exchange reserves dramatically decline\nCurrency value that has risen above its historical mean\nInflation increases\nLiberalized capital markets, that allows for the free flow of capital\nMoney supply relative to bank reserves increases\nBanking crises (may also be coincident)\n\n\n\n\n\n\n\nimage-20211022062357431\n\n\nA country’s standard of living, however, is best measured by GDP per capita.\n\n\n\nSavings and investment - If a country has insufficient domestic savings, it must attract foreign investment in order to grow.\nFinancial markets and intermediaries\nPolitical stability, rule of law, and property rights\nInvestment in human capital - Consequently, countries that invest in education and health care systems tend to have higher growth rates. Developed countries benefit the most from post-secondary education spending, which has been shown to foster innovation. Less-developed countries benefit the most from spending on primary and secondary education, which enables the workforce to apply the technology developed elsewhere.\nTax and regulatory systems - Lower levels of regulation foster entrepreneurial activity (startups), which have been shown to be positively related to the overall level of productivity.\nFree trade and unrestricted capital flows\n\n\n\n\n\nTo understand this, consider that the growth in aggregate stock market valuation is a function of GDP growth, growth in earnings relative to GDP, and growth in the price to earing ratio: \\[\n\\\\\\Delta{P} = \\Delta{GDP} + \\Delta{(E/GDP)} + \\Delta{(P/E)}\n\\\\\\text{long term: } \\Delta{(E/GDP)} = \\Delta{(P/E)} = 0\n\\] Over the long-term, we have to recognize that growth in earnings relative to GDP is zero; growth in the P/E ratio will also be zero over the long term; Hence over a sufficiently long time horizon, the potential GDP growth rate equals the growth rate of aggregate equity valuation.\n\n\n\n\n\n\nimage-20211022040003130\n\n\nPositive growth in potential GDP indicates that future income will rise relative to current income. When consumers expect their incomes to rise, they increase current consumption and save less for future consumption (i.e., they are less likely to worry about funding their future consumption). To encourage consumers to delay consumption (i.e., to encourage savings), investments would have to offer a higher real rate of return. Therefore, higher potential GDP growth implies higher interest rates and higher real estate returns in general.\nIn the short term, the relationship between actual GDP and potential GDP may provide insight to both equity and fixed-income investors as to the state of the economy. For example, since actual GDP in excess of potential GDP results in rising prices, the gap between the two can be used as a forecast of inflationary pressures-useful to all investors but of particular concern to fixed-income investors. Furthermore, central banks are likely to adopt monetary policies consistent with the gap between potential output and actual output. When actual GPD growth rate is higher (lower) than potential GDP growth rate, concerns about inflation increase (decrease) and the central bank is more likely to follow restrictive (expansionary) monetary policy.\n\n\n\n\n\nOutput (Y) is a function of labor (L) and capital (K), given a level of technology (T)\nCapital investment on economic growth and labor productivity, consider a Cobb-Douglas production function which takes the form: \\[\n\\\\Y = TK^{\\alpha}L^{1 - \\alpha}\n\\\\\\alpha\\, and (1 - \\alpha) = \\text{the share of output allocated to capital (K) and labor (L), respectively } [\\alpha\\, and (1 - \\alpha) \\text{ are also referred to as capital's and labor's share of total factor cost, where } \\alpha < 1]\n\\\\T = \\text{a scale factor that represents that technological progress of the economy, often referred to as total factor productivity (TFP)}\n\\] \nThe Cobb-Douglas function essentially states that output (GDP) is a function of labor and capital inputs and their productivity. It exhibits constant returns of scale; increasing both inputs by a fixed percentage leads to the same percentage increase in output.\nDividing both sides by L in the Cobb-Douglas production function, we can obtain the output per worker (labor productivity). \\[\n\\\\\\text{output per worker} = Y / L = T(K / L)^{\\alpha}\n\\] Labor productivity is similar to GDP per capital, a standard of living measure. Assuming the number of workers and \\(\\alpha\\) remain constant, increasing in output can be gained by increasing capital per worker (capital deepening) or by improving technology (increasing TFP).\n\n\n\nimage-20211022050248516\n\n\nHowever, since \\(\\alpha\\) is less than one, additional capital has a diminishing effect on productivity: the lower the value of \\(\\alpha\\), the lower the benefit of capital deepening. Developed markets typically have a high capital to labor ratio and a lower \\(\\alpha\\) compared to developing markets, and therefore developed markets stand to gain less in increased productivity from capital deepening.\nIn stead state (i.e., equilibrium), the marginal product of capital (MPK = \\(\\alpha\\)Y/K) and marginal cost of capital (i.e., the rental price of capital, r) are equal; hence: \\[\n\\\\\\alpha Y / K = r\n\\\\\\alpha = r K / Y\n\\\\MPK = \\frac{dY}{dK} = \\alpha T K^{\\alpha - 1} L^{1 - \\alpha} = \\frac{\\alpha T K^{\\alpha} L^{1 - \\alpha}}{K} = \\frac{\\alpha Y}{K}\n\\] Economies will increase investment in capital as long as MPK > r. At the level of K / L for which MPK = r, capital deepening stops and labor productivity becomes stangnant.\n \\[\n\\\\\\operatorname{labor productivity growth rate} = \\operatorname{growth due to technological change} + \\operatorname{growth due to capital deepening}\n\\\\\\frac{\\Delta{Y}}{Y} = \\frac{\\Delta{T}}{T} + \\alpha\\frac{\\Delta{K}}{K} + (1 - \\alpha)\\frac{\\Delta{L}}{L}\n\\\\(y = \\frac{Y}{L}, k = \\frac{K}{L})\n\\\\y = TK^{\\alpha}\n\\\\\\frac{\\Delta{y}}{y} = \\frac{\\Delta{T}}{T} + \\alpha\\frac{\\Delta{K}}{K}\n\\] \n\n\n\n\n\n\n\\[\n\\\\\\Delta{Y}/Y = \\Delta{A} / A + \\alpha * \\Delta{K} / K + (1 - \\alpha) * \\Delta{L} / L\n\\\\A = \\text{technology}\n\\\\\\alpha = \\text{elasticity of output with respect to capital} = \\text{share of income paid to capital}\n\\\\(1 - \\alpha) = \\text{elasticity of output with respect to labor} = \\text{share of income paid to labor}\n\\\\\\text{growth rate in potential GDP} = \\text{long-term growth rate of technology} + \\alpha (\\text{long-term growth rate of capital}) + (1 - \\alpha) (\\text{long-term growth rate of labor})\n\\]\nAnother approach to forecasting potential GDP growth is the labor productivity growth accounting equation, which focuses on changes in labor as follows: \\[\n\\\\\\text{growth rate in potential GDP} = \\text{long-term growth rate of labor force} + \\text{long-term growth rate in labor productivity}\n\\\\\\frac{\\Delta{y}}{y} = \\frac{\\Delta{Y}}{Y} - \\frac{\\Delta{L}}{L}\n\\]\n\n\n\n\nNatural resources include both renewable resources, such as timber, and non-renewable resources, such as oil and gas.\nOne reason that limited natural resources do not necessarily constrain economic growth is that access to natural resources does not require ownership of resources.\nThe so-called “Dutch disease” refers to a situation where global demand for a country’s natural resources drives up the country’s currency values, making all exports more expensive and rendering other domestic industries uncompetitive in the global markets.\n\n\n\nAn increase in the quantity of labor will increase output, but not per capital output. Labor force is defined as the number of working age (ages 16-64) people available to work, both employed and unemployed.\n\n\n\nDemographics - Countries with younger population have higher potential growth.\nLabor force participation \\[\n\\\\\\text{labor force participation} = \\frac{\\text{labor force}}{\\text{working age population}}\n\\] Labor force participation can increase as more women enter the workforce.\nImmigration - Immigration poses a potential solution to declining labor force. Countries with low population growth or adverse demographic shifts (older population) may find their growth constrained. Since developed countries tend to have lower fertility rates than less developed countries, immigration represents a potential source of continued economic growth in developed countries.\nAverage hours worked - For most countries, the general trend in average hours worked in downward. Possible explanations include legislation limiting the number of hours worked, the “wealth effect” which includes individuals to take more leisure time, high tax rates on labor income, and an increase in part-time and temporary workers.\n\n\n\n\n\n\n\nHuman capital is knowledge and skills individual possess.\n\n\n\nPhysical capital is generally separated into infrastructure, computers, and telecommunication capital (ICT) and non-ICT capital (i.e., machinery, transportation, and non-residential construction). Empirical research has found a strong correlation between investment in physical capital and GDP growth rates.\nThis result may seen inconsistent given our previous discussion about capital deepening and diminishing marginal returns to capital. First, many countries (e.g., developing economies have relatively low capital to labor ratios, so increases in capital may still have significant impact on economic growth. Second, capital investment can take different forms. Some capital investment actually influence technological progress, thereby increasing TFP and economic growth. For example, acceleration of spending in the IT sector has created what are termed network externalities. Investment in IT networks may have multiplicative effects on productivity since IT network investment actually becomes more valuable as more people are connected to the network.\n\n\n\nResearchers have examined proxies for investment in technology such as research and development (R&D) spending or number of patents issued.\n\n\n\n\n\n\n\n\n\nBased on Malthusian economics, classical growth theory posits that, in the long-term, population growth increases whenever there are increases in per capita income above subsistence level due to an increase in capital or technological progress. Subsistence level is the minimum income needed to maintain life. Classical growth theory contends that growth in real GDP per capita is not permanent, because when real GDP per capita rises above the subsistence level, a population explosion occurs. Population growth leads to diminishing marginal returns to labor, which reduces productivity and drives GDP per capita back to the subsistence level. This mechanism would prevent long-term growth in per capita income. Classical growth theory is not supported by empirical evidence.\n\n\n\n\n\n\nimage-20211022062446187\n\n\nNeoclassical growth theory’s primary focus is on estimating the economy’s long-term steady state growth rate (sustainable growth rate or equilibrium growth rate). The economy is at equilibrium when the output-to-capital ratio is constant. Under neoclassical theory, population growth is independent of economic growth.\n\nSustainable growth of output per capita (or output per worker)(\\(g^{*}\\)) is equal to the growth rate in technology (\\(\\theta\\)) divided by labor’s share of GDP (\\(1 - \\alpha\\)) \\[\n\\\\g^{*} = \\frac{\\theta}{1 - \\alpha}\n\\]\nSustainable growth rate of output (\\(G^{*}\\)) is equal to the sustainable growth rate of output per capita, plus the growth of labor (\\(\\Delta{L}\\)). \\[\n\\\\G^{*} = \\frac{\\theta}{1 - \\alpha} + \\Delta{L}\n\\]\n\n\n\n\nimage-20211022053626358\n\n\nUnder neoclassical theory:\n\nCapital deepening affects the level of output but not the growth rate in the long run. Capital deepening may temporarily increase the growth rate, but the growth rate will revert back to the sustainable level if there is no technological progress.\nAn economy’s growth rate will move towards its steady state regardless of the initial capital to labor ratio or level of technology.\nIn the steady state, the growth rate in productivity is a function only of the growth rate of technology and labor’s share of total output.\nIn the steady state, marginal product of capital (MPK) = \\(\\alpha\\) Y / K is constant, but marginal productivity is diminishing.\nAn increase in savings will only temporarily raise economic growth. However, countries with higher savings rates will enjoy higher capital to labor ratio and higher productivity.\nDeveloping countries (with a lower level of capital per worker) will be impacted less by diminishing marginal productivity of capital, and hence have higher growth rates as compared to developed countries; there will be eventual convergence of per capita incomes.\n\n\n\n\nEndogenous growth theory contends that technological growth emerges as a result of investment in both physical and human capital (hence the name endogenous which means coming from within). Technological progress enhances productivity of both labor and capital. Unlike the neoclassical model, there is no steady state growth rate, so that increased investment can permanently increase the rate of growth.\nIncreasing R&D investments, for example, results in benefits that also external to the firm making the R&D investments. Those benefits raise the level of growth for the entire economy.\nThe endogenous growth model theorize that returns to capital are constant. The endogenous growth model implies that an increase in savings will permanently increase the growth rate.\nEndogenous growth theory, on the other hand, assumes that capital investment (R&D expenditures) may actually improve total factor productivity.\n\n\n\n\nThe absolute convergence hypothesis states that less developed countries will achieve equal living standards over time. The conditional convergence hypothesis states that convergence in living standards will only occur for countries with the same savings rates, population growth rates, and production functions. Under the conditional convergence hypothesis, the growth rate will be higher for less developed countries until they catch up. Under the neoclassical model, once a developing country’s standard of living converges with that of developed countries, the growth rate will then stabilize to the same steady state growth rate as that of developed countries.\nClub convergence. Under this hypothesis, countries may be part of a ‘club’ (i.e., countries with similar institutional features such as savings rates, financial markets, property rights, health and educational services, etc.). Under club convergence, pooper countries that are part of the club will grow rapidly to catch up with their richer peers. Countries can ‘join’ the club by making appropriate institutional changes. Those countries that are not part of the club may never achieve the higher standard of living.\n\n\n\nUnder endogenous growth theory, private sector investments in R&D and knowledge capital benefit the society overall. The effects of ‘social returns’ of externalities are captured in the endogenous growth theory model, which concludes that economies may not reach a steady state growth but may permanently increase growth by expenditures that provide both benefits to the company (private benefits) and benefits to society (externalities).\n\n\n\nRemoving trade barriers and allowing for free flow of capital is likely to have the following benefits for countries:\n\nIncreased investment from foreign savings\nAllows focus on industries where the country has a comparative advantage\nIncreased markets for domestic products, resulting in economies of scale\nIncreased sharing of technology and higher total factor productivity growth\nIncreased competition leading to failure of inefficient firms and reallocation of their assets to more efficient uses\n\nThe neoclassical model’s predictions in an open economy (i.e., an economy without any barriers to trade or capital flow) focus on the convergence. Since developing economies have not reached the point of significant diminishing returns on capital, they can attract capital through foreign investment and experience productivity growth as a result. Eventually, these economies will develop; their growth will slow and will converge to the steady state growth rate of developed economies.\nThe endogenous growth model also predicts greater growth with free trade and high mobility of capital since open markets foster increased innovation. As foreign competition increases, more efficient and innovation firms will survive. Those firms permanently increase the growth rate of the international economy by providing benefits beyond those simply captured by the firm. Economies of scale also increase output as firms serve larger markets and become more efficient.\n\n\n\n\n\nRegulations are often required when markets cannot provide efficient solutions (also known as Pareto optimal, which means that one cannot make any participant better off without making some other participant worse off) for all problems.\nInformational frictions - information asymmetry\n\n\n\nimage-20211022055838539\n\n\nExternalities are costs or benefits that affect a party that did not choose to incur that cost or benefit.\nWeak competition can lead to fewer choices, higher prices, and lack of innovation. Antitrust regulations seek to mitigate this problem.\nSocial objectives are achieved via provision of public goods. A public good is a resource that can be enjoyed by a person without making it unavailable to others.\n\n\n\nimage-20211022060018319\n\n\n\n\n\n\n\nRegulating commerce\nRegulating financial markets - The objectives of securities regulations include three interrelated goals: protect investors, create confidence in the markets, and enhance capital formation.\n\n\n\n\nDisclosure requirements\nRegulations imposing fiduciary duties seek to mitigate such agency problems.\nRegulations have historically focused on protecting small (retail) investors.\n\n\n\n\nPrudential supervision refers to the monitoring and regulation of financial institutions to reduce system-wide risks to protect investors. Prudential supervision is important because the failure of one financial institution can have a far-reaching impact and may result in a loss of confidence.\n\n\n\n\nAntitrust regulation\n\n\n\nStatutes - laws made by legislative bodies\nAdministrative regulations - rules issued by government agencies or other bodies authorized by the government\nJudicial law - findings of the court\n\n\nRegulators can be government agencies or independent regulators. Independent regulators are given recognition by government agencies and have power to make rules and enforce them. However, independent regulators are usually not funded by the government and hence are politically independent.\nIndustry self-regulatory bodies (SRBs) are private organization that represent as well as regulate their members. Members of SRBs have to adhere to its rules. SRBs nonetheless are attractive in that they increase the overall level of regulatory resources, utilize the industry professionals with the requisite expertise, and allow regulators to devote resources to other priorities.\nSRBs that are recognized by the government and given enforcement powers are self-regulating organizations (SROs). SROs are also independently funded and, as such, are politically independent. SROs regulate the behavior of their members and often provide public goods in the form of standards. Because of their recognition by the government, SROs fall within the category of independent regulators.\nOutside bodies are not regulators themselves, but their product is referenced by regulators. Examples of outside bodies include FASB and IASB.\n\n\n\n\n\n\n\n\n\nThe regulatory capture is based upon the assumption that, regardless of the original purpose behind its establishment, a regulatory body will, at some point in time, be influenced or even possibly controlled by the industry that is being regulated. The rationale behind the theory is that regulators often have experience in the industry, and this affects the regulators’ ability to render impartial decisions.\nRegulatory differences between jurisdictions can lead to regulatory competition, in which regulators compete to provide the most business-friendly regulatory environment. Regulatory arbitrage occurs when businesses shop for a country that allows a specific behavior rather than changing the behavior. Regulatory arbitrage also entails exploiting the difference between the economic substance and interpretation of a regulation.\n\n\n\n\n\n\n\nPrice mechanisms - Price mechanisms such as taxes and subsidies can be used to further specific regulatory objectives; for example, sin taxes are often used to deter consumption of alcohol.\nRestricting or requiring certain activities - Regulators may ban certain activities or require that certain activities be performed to further their objectives.\nProvision of public goods or financing of private projects - Regulators may provide public goods or fund private projects depending on their political priorities and objectives.\n\n\n\n\n\n\n\nThe costs and benefits of regulations may be easy to view but difficult to quantify. An analyst should also consider the cost of the regulation to the private sector.\nRegulatory burden (also known as government burden) refers to the cost of compliance for the regulated entity. Regulatory burden minus the private benefits of regulation is known as the net regulatory burden.\nRegulators should be aware of unintended consequences of regulations.\nRegulatory costs are difficult to assess before a regulation is put in place. For this reason, many regulatory provisions include a sunset clause that requires regulators to revisit cost benefit analysis based on actual outcomes before renewing the regulation.\n\n\n\n\nRegulations can help or hinder a company or industry. Regulations may shrink the size of one industry."
  },
  {
    "objectID": "posts/2021-12-10-probability-and-inferential-statistics-week-13/2021-12-10-probability-and-inferential-statistics-week-13.html",
    "href": "posts/2021-12-10-probability-and-inferential-statistics-week-13/2021-12-10-probability-and-inferential-statistics-week-13.html",
    "title": "Probability and Inferential Statistics Week 13",
    "section": "",
    "text": "prob_infer_stats_week_13_4\n\n\n\n\n\nprob_infer_stats_week_13_3\n\n\n\n\n\nprob_infer_stats_week_13_2\n\n\n\n\n\nprob_infer_stats_week_13_1\n\n\n\n\n\nprob_infer_stats_week_13_6\n\n\n\n\n\nprob_infer_stats_week_13_5"
  },
  {
    "objectID": "posts/2021-09-30-probability-and-statistics-week-5/2021-09-30-probability-and-statistics-week-5.html",
    "href": "posts/2021-09-30-probability-and-statistics-week-5/2021-09-30-probability-and-statistics-week-5.html",
    "title": "Probability and Statistics Week 5",
    "section": "",
    "text": "특정 사건이 일어난 것을 알고 있을 때, 다른 사건이 동시에 발생하는 확률 \\[\n\\\\P(A | B) = \\frac{P(A \\cap B)}{P(B)}\\\\\nif\\, A \\cap B = \\empty: P(A | B) = \\frac{P(A \\cap B)}{P(B)} = 0\\\\\nif\\, A \\cap B = B: P(A | B) = \\frac{P(A \\cap B)}{P(B)} = 1\\\\\nP(A | B) + P(A^{`} | B) = \\frac{P(A \\cap B)}{P(B)} + \\frac{P(A^{`} \\cap B)}{P(B)} = 1\n\\] B로 sample space를 줄여서 B가 일어났을 때 A가 일어날 확률을 구하는 개념. 즉, \\(P(A | B) + P(A | B^{`}) = 1\\) \\[\n\\\\A \\cap (B \\cup C) \\ne A \\cap B \\cup C\\\\\nP(A | B) = \\frac{P(A \\cap B)}{P(B)}, \\, P(B | A) = \\frac{P(A \\cap B)}{P(A)}\\\\\nP(A \\cap B) = P(B)P(A | B) = P(A)P(B | A)\n\\]\n\n\n\n\\[\n\\\\P(B | A) = P(B)\n\\\\P(A | B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A)\\\\\n\\]\n아래 것 중 하나라도 성립하면 나머지도 성립 \\[\nP(A | B) = P(A)\\\\\nP(B | A) = P(B)\\\\\nP(A \\cap B) = P(A)P(B)\n\\] 한 사건이 발생한 사실을 아는 것이 다른 사건의 확률에 아무 영향을 주지 못함\n\n\n\nimage-20210930234612081\n\n\n\n\n\n\n\n\nimage-20210930234748270\n\n\n \\[\n\\\\P(claim) = P(\\operatorname{plant I}) * P(claim | \\operatorname{plant I}) + P(\\operatorname{plant II}) * P(claim | \\operatorname{plant II}) + P(\\operatorname{plant III}) * P(claim | \\operatorname{plant III}) + P(\\operatorname{plant IV}) * P(claim | \\operatorname{plant IV})\n\\]\n\n\n\n\n\n\nimage-20210930235347107\n\n\n\n\n\n\n복윈: \\(N = n^{k}\\)\n비복원\n\n순서가 있는 경우: Permutation\n순서가 없는 경우: Combination\n\n\n\n\n\n\n\n\nimage-20210930235631538\n\n\n\n\n\n\n\n\nimage-20210930235730247\n\n\n\n\n\n\\[\n\\\\P(A_{i}): \\text{prior probability}\n\\\\P(B | A_{i}): \\text{likelihood probability}\n\\\\P(A_{i} | B) = \\frac{P(A_{i})P(B | A_{i})}{\\sum_{j = 1}^{n}{P(A_{j})P(B | A_{j})}}: \\text{posterior probability}\n\\]\n\n\n\n빈도주의 vs. 베이지안\n아는 확률을 통해서 모르는 확률을 추론하는 것 - 베이지안 이론\n\n\n\n\n\nimage-20211001000304270\n\n\n\n\n\n\n표본공간 S에 대해서 대응하는 real number를 연결하는 함수\n\n\n\nimage-20211001000842677\n\n\n변수지만 사실상 함수의 개념. output을 실수와 mapping\n\n\n\n\n\nimage-20211001000928761\n\n\n\n\n\n\n\ndiscrete random variable - 불연속적인 특정 값만 가지는 경우 -> probability mass function\ncontinuous random variable - 어떤 구간 내에서 임의의 실수 값을 가지는 경우 -> probability density function\n\n\n\n\n이산형 확률변수 X가 어떤 값 \\(x_{i}\\)를 가질 확률 \\(p_{i}\\) \\[\n\\\\P(X = x_{i}) = p_{i}\n\\] Histogram을 총 sample의 수로 나눈 것과 사실 상 같음\n아래의 두 조건을 만족해야 함 \\[\n\\\\0 \\le p_{i} \\le 1\n\\\\\\sum_{i}{p_{i}} = 1\n\\] Hisgotram처럼 bar 형태로 그릴 수 있음\n\n\n\n\n\n\nimage-20211001001839852\n\n\n\n\n\nimage-20211001001903135\n\n\n\n\n\nimage-20211001001935111\n\n\nmapping된 특정 real number에만 값이 있는 것임\n\n\n\nimage-20211001002008036\n\n\n\n\n\n\\[\n\\\\F(x) = P(X \\le x)\n\\\\F(x) = \\sum_{y:y<=x}{P(X = y)}\n\\]\n\n\n\n\n\nimage-20211001002205724\n\n\n \\[\n\\\\0 \\le F_{X}(x) \\le 1\n\\\\F_{X}(\\infty) = 1\n\\\\F_{X}(-\\infty) = 0\n\\\\F_{X}(x_{1}) \\le F_{X}(x_{2}) for x_{1} < x_{2}\n\\]\n\n\n\n\n\\[\n\\\\1 - F_{X}(x) = P(X >= x)\n\\]\n파레토 분포 등을 볼 때 유용하게 쓰임\n20:80법칙 생각하면 됨\n\n\n\n\\[\n\\\\P(x_{1} < X <= x) = F(x_{2}) - F(x_{1})\n\\]\n\n\n\nimage-20211001002524888\n\n\n\n\n\nimage-20211001002656255"
  },
  {
    "objectID": "posts/2021-12-10-probability-and-inferential-statistics-week-14/2021-12-10-probability-and-inferential-statistics-week-14.html",
    "href": "posts/2021-12-10-probability-and-inferential-statistics-week-14/2021-12-10-probability-and-inferential-statistics-week-14.html",
    "title": "Probability and Inferential Statistics Week 14",
    "section": "",
    "text": "prob_infer_stats_week_13_14\n\n\n\n\n\nprob_infer_stats_week_13_13\n\n\n\n\n\nprob_infer_stats_week_13_12\n\n\n\n\n\nprob_infer_stats_week_13_11\n\n\n\n\n\nprob_infer_stats_week_13_10\n\n\n\n\n\nprob_infer_stats_week_13_9\n\n\n\n\n\nprob_infer_stats_week_13_8\n\n\n\n\n\nprob_infer_stats_week_13_7\n\n\n\n\n\nprob_infer_stats_week_13_17\n\n\n\n\n\nprob_infer_stats_week_13_16\n\n\n\n\n\nprob_infer_stats_week_13_15"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html",
    "title": "Siyun Min",
    "section": "",
    "text": "---\ntitle: \"Probability and Statistics Week 7\"\nauthor: \"Siyun Min\"\ndate: \"2021-10-23\"\nformat:\n    html:\n        code-fold: false\n---"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-1-a",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-1-a",
    "title": "Siyun Min",
    "section": "Question 1 (a)",
    "text": "Question 1 (a)\n\\[\nX = \\text{학생 한 명과 대화하는 데 걸리는 시간} \\\\\nY = 5X = \\text{학생 5명과 대화하는데 걸리는 시간} \\\\\n\\mu = 8, \\sigma = 2 \\\\\nX \\thicksim N(8, 2^{2}) \\\\\nE(Y) = E(5X) = 5E(X) \\\\\nVar(Y) = Var(5X) = 25Var(X) \\\\\n\\mu_{X} = 8, \\sigma_{X} = 2 \\\\\n\\mu_{Y} = 40, \\sigma_{Y} = 5 * 2 = 10 \\\\\nP(Y \\geq 45) \\\\\n\\]\n따라서 Y = 45일 때 cdf 값을 구하고 1 - cdf하면 학생 5명과 대화를 할 때 걸린 총 시간이 45분 이상일 확률이 구해진다.\n\nmu = 8\nstd = 2\n\nanswer_1_a = 1 - stats.norm.cdf(45, mu * 5, std * 2)\nanswer_1_a\n\n0.10564977366685535\n\n\n\\[\nP(Y \\geq 45) = 0.002796018584074135\n\\]"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-1-b",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-1-b",
    "title": "Siyun Min",
    "section": "Question 1 (b)",
    "text": "Question 1 (b)\n\\[\nH = \\text{첫 학생의 상담 시작과 두통이 생기기 시작하는 시간 사이의 경과하는 시간} \\\\\n\\mu_{H} = 28, \\sigma_{H} = 5 \\\\\nY_{3} = 3X = \\text{학생 3명과 대화하는데 걸리는 시간} \\\\\n\\mu_{Y_{3}} = 8 * 3 = 24, \\sigma_{Y_{3}} = 2 * 3 = 6 \\\\\nT = H - Y_{3}\\\\\n\\mu_{T} = 28 - 24 = 4, \\sigma_{T} = 5 + 6 = 11 \\\\\nP(T \\geq 0)\n\\]\n\nmu_t = 3\nsigma_t = 11\n\nanswer_1_b = 1 - stats.norm.cdf(0, mu_t, sigma_t)\nanswer_1_b\n\n0.6074685657262202\n\n\n\\[\nP(T \\geq 0) = 0.6074685657262202\n\\]"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#이항분포의-가우시안-근사",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#이항분포의-가우시안-근사",
    "title": "Siyun Min",
    "section": "이항분포의 가우시안 근사",
    "text": "이항분포의 가우시안 근사\n\\[\nB(n, p) \\to N(np, np(1 - p)) \\\\\nX \\thicksim B(n, p) \\\\\n\\Rightarrow P(X \\leq x) \\simeq \\Phi{(\\frac{x + 0.5 - np}{\\sqrt{np(1 - p)}})}\\, and\\, P(X \\geq x) \\simeq 1 - \\Phi{(\\frac{x - 0.5 - np}{\\sqrt{np(1 - p)}})} \\\\\nnp \\geq 5\\, and\\, n(1 - p) \\geq 5\\, \\text{에서 잘 작동}\n\\]"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-a",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-a",
    "title": "Siyun Min",
    "section": "Question 2 (a)",
    "text": "Question 2 (a)\n\\(P(X \\geq 7)\\, where\\, X \\thicksim B(10, 0.3)\\)\n\nn = 10\np = 0.3\nq = 1 - p\n\nmu = n * p\nvar = n * p * q\n\nanswer_2_a_norm = 1 - stats.norm.cdf((7 - 0.5 - mu) / np.sqrt(var))\nanswer_2_a_bi = 1 - stats.binom.cdf(7, n, p)\n\nanswer_2_a_norm, answer_2_a_bi\n\n(0.007862649877252714, 0.0015903863999999768)\n\n\n정규분포 근사값은 0.6567208193227195이고, 이항분포 값은 0.0015903863999999768이다."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-b",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-b",
    "title": "Siyun Min",
    "section": "Question 2 (b)",
    "text": "Question 2 (b)\n\\(P(9 \\leq X \\leq 12)\\, where\\, X \\thicksim B(21, 0.5)\\)\n\nn = 21\np = 0.5\nq = 1 - p\n\nmu = n * p\nvar = n * p * q\n\nanswer_2_b_norm = stats.norm.cdf((12 + 0.5 - mu) / np.sqrt(var), 0, np.sqrt(var)) - stats.norm.cdf((9 - 0.5 - mu) / np.sqrt(var))\nanswer_2_b_bi = stats.binom.cdf(12, n, p) - stats.binom.cdf(9, n, p)\n\nanswer_2_b_norm, answer_2_b_bi\n\n(0.4570141638579211, 0.4765329360961914)\n\n\n정규분포 근사값은 0.4570141638579211이고, 이항분포 값은 0.4765329360961914이다."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-c",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-c",
    "title": "Siyun Min",
    "section": "Question 2 (c)",
    "text": "Question 2 (c)\n\\(P(X \\leq 3)\\, where\\, X \\thicksim B(7, 0.2)\\)\n\nn = 7\np = 0.2\nq = 1 - p\n\nmu = n * p\nvar = n * p * q\n\nanswer_2_c_norm = stats.norm.cdf((3 + 0.5 - mu) / np.sqrt(var))\nanswer_2_c_bi = stats.binom.cdf(3, n, p)\n\nanswer_2_c_norm, answer_2_c_bi\n\n(0.9763895479982114, 0.966656)\n\n\n정규분포 근사값은 0.9763895479982114이고, 이항분포 값은 0.966656이다."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-d",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-2-d",
    "title": "Siyun Min",
    "section": "Question 2 (d)",
    "text": "Question 2 (d)\n\\(P(9 \\leq X \\leq 11)\\, where\\, X \\thicksim B(12, 0.65)\\)\n\nn = 12\np = 0.65\nq = 1 - p\n\nmu = n * p\nvar = n * p * q\n\nanswer_2_d_norm = stats.norm.cdf((11 + 0.5 - mu) / np.sqrt(var)) - stats.norm.cdf((9 + 0.5 - mu) / np.sqrt(var))\nanswer_2_d_bi = stats.binom.cdf(11, n, p) - stats.binom.cdf(9, n, p)\n\nanswer_2_d_norm, answer_2_d_bi\n\n(0.1391995944105242, 0.14559956927216167)\n\n\n정규분포 근사값은 0.1391995944105242이고, 이항분포 값은 0.14559956927216167이다."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-a",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-a",
    "title": "Siyun Min",
    "section": "Question 3 (a)",
    "text": "Question 3 (a)\n\n하루 후에 예상되는 배양물의 평균값은 얼마인가요?\n\nlog-normal distribution에서 평균은 \\(exp(\\mu + \\frac{\\sigma^{2}}{2})\\)이다.\n\nmu = 2.3\nsigma = 0.2\n\nanswer_3_a = np.exp(mu + (sigma ** 2) / 2)\nanswer_3_a\n\n10.175674306073333\n\n\n하루 후 예상되는 배양물의 크기 평균 값 = 10.175674306073333"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-b",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-b",
    "title": "Siyun Min",
    "section": "Question 3 (b)",
    "text": "Question 3 (b)\n\n하루 후의 배양물의 중간값은 얼마인가요?\n\nlog-normal distribution에서 중간값은 \\(exp(\\mu)\\)이다.\n\nmu = 2.3\nsigma = 0.2\n\nanswer_3_b = np.exp(mu)\n\nanswer_3_b\n\n9.974182454814718\n\n\n하루 후 예상되는 배양물의 크기 중간 값 = 9.974182454814718"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-c",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-c",
    "title": "Siyun Min",
    "section": "Question 3 (c)",
    "text": "Question 3 (c)\n\n하루 후의 배양물 크기의 3rd Quartile은 얼마인가요?\n\nlog-normal distribution에서 quantile은 \\(exp(\\mu + \\sqrt{2\\sigma^{2}}\\, erf^{-1}(2p - 1))\\)이다.\n따라서 p가 0.75일 때 3rd Quartile이다.\n\nfrom scipy import special\n\nanswer_3_c = np.exp(mu + np.sqrt(2 * (sigma ** 2)) / special.erf(2 * 0.75 - 1))\n\nanswer_3_c\n\n17.174159769935745\n\n\n하루 후 예상되는 배양물의 크기 3rd Quartile = 17.174159769935745"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-d",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-3-d",
    "title": "Siyun Min",
    "section": "Question 3 (d)",
    "text": "Question 3 (d)\n\n하루 후에 배양물 크기가 15보다 클 확률은 얼마인가요?\n\n\\(P(X > 15) = 1 - \\Phi{(\\frac{ln(15) - \\mu}{\\sigma})}\\)\n\nanswer_3_d = 1 - stats.norm.cdf((np.log(15) - mu) / sigma)\n\nanswer_3_d\n\n0.020662665969527705\n\n\n\\(P(X > 15) = 0.020662665969527705\\)"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-4-e",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7-hw.html#question-4-e",
    "title": "Siyun Min",
    "section": "Question 4 (e)",
    "text": "Question 4 (e)\n\n하루 후에 배양물 크기가 6보다 작을 확률은 얼마인가요?\n\n\\(P(X \\leq 6) = \\Phi{(\\frac{ln(6) - \\mu}{\\sigma})}\\)\n\nanswer_3_e = stats.norm.cdf((np.log(6) - mu) / sigma)\n\nanswer_3_e\n\n0.005523593376350241\n\n\n\\(P(X \\leq 6) = 0.005523593376350241\\)"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7.html",
    "href": "posts/2021-10-23-probability-and-statistics-week-7/2021-10-23-probability-and-statistics-week-7.html",
    "title": "Probability and Statistics Week 7",
    "section": "",
    "text": "\\[\n\\\\f(x) = \\frac{\\lambda(\\lambda x)^{k - 1}e^{-\\lambda x}}{\\Gamma{(k)}}\n\\]\n\n\n\nimage-20211023021832347\n\n\n\n\n\n\n\nimage-20211023021847507\n\n\nGamma function 부분적분 \\[\n\\\\\\int{u(x)v^{`}(x)dx} = u(x)v(x) - \\int{u^{`}(x)v(x)dx}\n\\]\n\n\n\n\n\n\n\nimage-20211023022413944\n\n\n\n\n\nimage-20211023022504335\n\n\n\n\n\nimage-20211023022806970\n\n\n\n\n\nimage-20211023022901414\n\n\n\n\n\nimage-20211023023038476\n\n\n\n\n\nimage-20211023023100228\n\n\n\n\n\nimage-20211023023132879\n\n\n\n\n\n\n\n\nimage-20211023023255476\n\n\nexponential distribution의 특별한 형태라고 보면 됨\n\n\n\nimage-20211023023357229\n\n\n\n\n\nimage-20211023023429445\n\n\n\n\n\nimage-20211023023455454\n\n\n\n\n\n\n\n\nimage-20211023023551383\n\n\nbinomial distribution의 특수한 경우\n횟수는 정해져있고, 확률이 미지수\n\n\n\nimage-20211023023702158\n\n\n\n\n\nimage-20211023023720452\n\n\n\n\n\n\n\n\nimage-20211023023745117\n\n\n분포를 모르는 상태에서 normal distribution으로 근사할 수 있기 때문에 자주 사용\n\\(\\mu\\)에 대해 대칭인 함수\n\n\n\nimage-20211023023826385\n\n\n\n\n\nimage-20211023023856524\n\n\nshape이 변하지 않고, \\(\\mu\\)에 따라 shifting이 발생, \\(\\sigma\\)에 따라 scale이 변함\n\n\n\nimage-20211023023907323\n\n\n\n\n\nimage-20211023023934785\n\n\n\n\n\nimage-20211023024013805\n\n\n\n\n\n\n\n\nimage-20211023024106277\n\n\n\n\n\n\n\nimage-20211023024226388\n\n\n\n\n\n\n\n\nimage-20211023024502108\n\n\n\n\n\nimage-20211023024511257\n\n\n\n\n\n\n\n\nimage-20211023024531671\n\n\n독립이 아닌 경우 covariance를 통해 variance를 보정해야 함\n\n\n\nimage-20211023024610848\n\n\n\n\n\nimage-20211023024637157\n\n\n\n\n\nimage-20211023024645346\n\n\n\n\n\n\n\n\nimage-20211023024730273\n\n\n\n\n\nimage-20211023024755095\n\n\n\n\n\n\n\n\n\nimage-20211023024809348\n\n\n\n\n\n\n\n\nimage-20211023025611636\n\n\n\n\n\nimage-20211023025741865\n\n\n\n\n\n\n\n\nimage-20211023025853940\n\n\n\n\n\nimage-20211023030023367\n\n\n\n\n\nimage-20211023030035973\n\n\n\n\n\n\n\nimage-20211023030107470\n\n\n\n\n\n\n\n\nimage-20211023030202474\n\n\n\n\n\n\n\n\n\nimage-20211023030411773\n\n\n\n\n\nimage-20211023030837950\n\n\n\n\n\nimage-20211023030929548\n\n\n\n\n\n\n\n\nimage-20211023031028447\n\n\n\n\n\n\n\n\nimage-20211023031142932\n\n\n\n\n\nimage-20211023031202696\n\n\n\n\n\nimage-20211023031340673\n\n\n\n\n\n\n\n\nimage-20211023031447915\n\n\n\n\n\n\n\n\nimage-20211023031507129\n\n\n\n\n\n\n\n\nimage-20211023031542261\n\n\n\n\n\n\n\nimage-20211023031635507\n\n\n\n\n\n\n\n\n\nimage-20211023031652493\n\n\n\n\n\nimage-20211023031719192\n\n\n\n\n\n\n\nimage-20211023031756575\n\n\n\n\n\nimage-20211023031805423\n\n\n\n\n\n\n\n\n\nimage-20211023032010570\n\n\n\n\n\nimage-20211023032025780\n\n\n\n\n\n\n\nimage-20211023032034214"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2021-12-10-probability-and-inferential-statistics-week-12/2021-12-10-probability-and-inferential-statistics-week-12.html",
    "href": "posts/2021-12-10-probability-and-inferential-statistics-week-12/2021-12-10-probability-and-inferential-statistics-week-12.html",
    "title": "Probability and Inferential Statistics Week 12",
    "section": "",
    "text": "prob_infer_stats_week_12_1\n\n\n\n\n\nprob_infer_stats_week_12_2\n\n\n\n\n\nprob_infer_stats_week_12_3\n\n\n\n\n\nprob_infer_stats_week_12_4\n\n\n\n\n\nprob_infer_stats_week_12_5\n\n\n\n\n\nprob_infer_stats_week_12_6\n\n\n\n\n\nprob_infer_stats_week_12_7\n\n\n\n\n\nprob_infer_stats_week_12_8"
  },
  {
    "objectID": "posts/2021-10-05-CFA-Level-2-Portfolio-Management/2021-10-05-CFA-Level-2-Portfolio-Management.html",
    "href": "posts/2021-10-05-CFA-Level-2-Portfolio-Management/2021-10-05-CFA-Level-2-Portfolio-Management.html",
    "title": "CFA Level 2 Portfolio Management",
    "section": "",
    "text": "Unlike open-end mutual funds, ETFs are traded on secondary markets.\nThe ETF issuer designates authorized participants (APs). APs are permitted to create additional shares, or redeem existing shares, for a service fee payable to the ETF manager.\nThe in-kind creation / redemption process serves three purposes:\n\nLower cost\nTax efficiency\nKeeping market prices in line with NAV\n\nThis implies that ETFs should trade within a price band of the NAV, known as the arbitrage gap.\nInvestor -> ETF (상장) -> 추종하려는 Index (결국 ETF 가격은 Index 가격에 따라 움직임)\nAuthorized Participants (AP)라는 역할 존재\n상장이니까 가격 변동성 존재 (NAV에서 가격이 멀어질 수 있음) -> AP가 조절 by 증자, 감자\n장점\n\nLower Cost - 증자, 감자는 AP가 해주니 ETF Manager는 신경쓰지 않음\nTax Efficiency: ETF의 증자 / 감자는 비과세\nNAV와 가격 같게 유지 -> 그렇지 않으면 arbitrage gap 발생 가능\n\n\n\n\nIn the United States, the National Security Clearing Corporation (NSCC) guarantees the performance of parties to a trade on an exchange. The Depository Trust Company (DTC), a subsidiary of NSCC, transfers the securities from the account of the seller’s broker to the account for buyer’s broker at the end of the two-day settlement period. Market makers, due to their special significance, and due to the time required by the creation / redemption process, are afforded up to six days to settle their trades.\nETF는 주식처럼 상장되어 거래, NSCC가 성과 보증하고 DTC가 거래 소관 (2영업일 내에 거래 처리)\nAP는 증자 / 감자 거래 처리를 6영업일 안에 처리\n\n\n\nTracking difference is the divergence between an ETF’s return (based on its NAV) and the return on the tracked index. Tracking error is the annualized standard deviation of the daily tracking difference. Tracking error does not indicate whether the ETF under- or outperformed the index.\nETFs generally underperform the benchmark by their expense ratio.\nSources of tracking error include the following:\n\nFees and expense\nSampling and optimization\nDepository receipts (DRs) - due to time zone differences in capturing price data\nIndex changes\nRegulatory and tax requirements\nFund accounting practices\nAsset manager operations\n\nTracking difference: BM과 ETF return의 차이\nTracking error는 ETF가 Index보다 성과가 좋다 나쁘다 개념이 아님 - 잘 복제했느냐 개념\nETF가 BM보다 성과가 보통 낮은 이유 -> Fees, Sampling, DR, Index 변화, 규제 변화, Fund accounting, asset manager operation\n\n\n\nThe primary factors affecting ETF spread are the liquidity and the market structure of the underlying securities.\n\nSpreads on fixed-income ETFs tends to be larger than hose for large-cap equity ETFs.\nThe spreads are narrower during the overlapping time period when both markets are open.\nSpecialized ETFs, such as those that track commodities, volatility futures, or small-cap stocks, tend to have wider spreads.\nThinly-traded ETFs, regardless of the liquidity of the underlying, also command a higher spread.\n\nMaximum spread = creation (redemption) fees plus other trading costs + spread of the underlying securities + risk premium for carrying the trade until close of trading + AP’s normal profit margin - discount based on probability of offsetting the trade in secondary market\nBid-Ask spread가 영향을 미치는 주요 요인 - 해당 시장의 유동성, underlying security\n채권 ETF spread가 더 큼, specialized ETF spread가 더 큼\nMaximum spread = Creation fee + spread (기초자산) + risk premium + AP’s margin - discount (유통시장에서 거래되어 offsetting 될 수 있는 비용)\n\n\n\nThe NAV of an ETF is generally its fair value.\nETF premium (discount) % = (ETF price - NAV per share) / NAV per share\n\nTiming differences - ETFs on foreign securities may experience gaps between the time the ETF is traded and the time when the underlying trades in a foreign market.\nStale pricing - Infrequently traded ETFs may reflect noncurrent prices and, therefore, their value may differ from NAV.\n\nETF prices may be more informative than NAV or iNAV when 1) the market for the underlying is closed, 2) underlying securities are highly volatile or illiquid, or 3) there is a time lag between the pricing of the ETF and the pricing of underlying.\nETF는 일반적으로 fair value에 거래됨 - 거래소가 NAV를 공시\nETF premium = (ETF price - NAV price) / NAV price\npremium이나 discount 발생 이유 - timing difference, stale pricing\n하지만 NAV보다 ETF price가 더 의미있을 때도 있음 - 장전, 기초자산이 너무 변동성이 심할 때 등\n\n\n\nETF costs include management fees and trading costs.\nTrading costs include brokerage or commission fees and bid-ask spreads.\nFor shorter holding periods, trading cost dominates the cost of ETF ownership. Conversely, long-term investors are likely to seek out ETFs with low management fees.\nManagement fee + trading costs -> 경쟁이 치열하고 + passive라서 fee수준은 낮은 편\ntrading costs는 거래할 때만 발생 -> 보유기간이 길 수록 거래비요으이 비중은 감소 -> 장기투자자는 운용보수가 낮은 ETF 선호\n\n\n\n\n\nExchange-traded notes (ETNs), for example, have high counterparty risk. In the case of an ETN, an issuer (typically a bank) issues unsecured debt obligations that promise to pay the return on an index less management fees (just like a regular ETF).\nThe concern here is that the bank may default, resulting in losses for the ETN investor.\n\n\n\nETFs using OTC derivative contracts as part of their strategy expose investors to the settlement risk of such contracts.\n\n\n\nLike mutual funds, ETFs may lend their securities to short sellers for a fee.\n\n\n\n\n\n\nThe outcomes may differ from investors’ expectations\nThe compounding effects of leveraged ETFs make them unsuitable for buy-and-hold investors with investment horizons exceeding one month.\nCounterparty risk, settlement risk, security lending, fund closures, expectation-related risk (기대와 다른 효과)\n\n\n\n\nDue to their low costs, tax efficiency, and wide variety, ETFs are suitable for numerous portfolio strategies.\n\n\n\nPortfolio liquidity management\nPortfolio rebalancing\nPortfolio completion\nTransition management\n\n\n\n\nAllow a manager to implement a variety of strategies suitable for their clients.\n\nCore exposure to an asset class or sub-asset class\nTactical strategies\n\n\n\n\nNewer varieties of ETFs with an active component have gained traction, especially for fixed income.\n\nFactor (smart beta) ETFs\nRisk management\nAlternatively weighted ETFs\nDiscretionary active ETFs\nDynamic asset allocation and multi-asset strategies\n\nEfficient Portfolio management 활용 - liquidity, rebalancing, completion, transition\nassets class exposure management 활용 - core exposure, tactical strategy\nactive investing - factor ETF, risk management, alternatively weighted ETF, Discretionary active ETF, Dynamic asset allocation -> ETF는 유동성이 있으니 자유자재로 active하게 활용해서 \\(\\alpha\\)추구\n\n\n\n\nArbitrage pricing theory (APT) was developed as an alternative to the capital asset pricing model. However, unlike CAPM, APT does not identify the specific risk factors (or even the number of factors).\n\n\n\nUnsystematic risk can be diversified away in a portfolio\nReturns are generated using a factor model\nNo arbitrage opportunities exist\n\n\n\n\n\\[\n\\\\E(R_{P}) = R_{F} + \\beta_{P, 1}(\\lambda_{1}) + \\beta_{P, 2}(\\lambda_{2}) + ... +  \\beta_{P, k}(\\lambda_{k})\n\\]\n\\(\\lambda_{j}\\) equals the risk premium for a portfolio (called a pure factor portfolio) with factor sensitivity equal to 1 to factor j and factor sensitivities of zero for the remaining factors.\nUnlike the CAPM, the APT does not require that one of the risk factors is the market portfolio. This is a major advantage of the arbitrage pricing model.\nArbitrage pricing model -> CAPM 왜 한 개인지 의문이 발생 -> 수익률에 영향을 주는 게 더 많지 않을까라는 의문에서 개발 (베타 말고 다른 요인 찾기)\n가정\n\n비체계적 위험은 분산 가능\nReturn은 Factor model로 측정 가능\nArbitrage 기회 없음\n\n이름이 왜? - 위 2번 가정을 모두가 쓰면, 모두가 같은 \\(E(r)\\)을 계산 -> mispricing이 생겨도 금방 수정됨 -> arbitrage 기회 없음\nFactor Portfolio = 다른 Factor는 다 0으로 가정, Factor 1개만 1로 측정 -> 그래서 그 factor가 얼마나 영향을 주는지를 측정하기 ㅜ이함\n장점 - Factor를 다중 분석할 수 있음\n단점 - 어느 Factor를 써야하지? 통계적으로 맞다 하여도 쌩뚱맞은 변수를 쓰면 무슨 소용이지?\n\n\n\n\nThe APT assumes there are no market imperfections preventing investors from exploiting arbitrage opportunities.\nOver-valued는 short, 그 돈으로 under-valued 매수 -> 그 차이가 이익\n\n\n\n\n\n\nA multifactor model assumes asset returns are driven by more than one factor. There are three general classifications of multifactor models: 1) macroeconomic factor models, 2) fundamental factor models, and 3) statistical factor models.\n\nMacroeconomic factor models assume that asset returns are explained by surprises (or shocks) in macroeconomic risk factors. Factors surprises are defined as the difference between the realized value of the factor and its consensus predicted value.\nFundamental factor models assume asset returns are explained by multiple firm-specific factors.\nStatistical factor models use statistical methods to explain asset returns. Two primary types of statistical factor models are used: factor analysis and principal component models. The major weakness is that the statistical factors do not lend themselves well to economic interpretation. Therefore, statistical factors are mystery factors.\n\n\n\n\\[\n\\\\R_{i} = E_{R_{i}} + b_{i1}F_{GDP} + b_{i2}F_{QS} + \\epsilon_{i}\n\\\\R_{i} = \\text{return for Asset i}\n\\\\E_{R_{i}} = \\text{expected return for Asset i (in the absence of any surprises)}\n\\\\F_{FDP} = \\text{surprise in the GDP rate}\n\\\\F_{QS} = \\text{surprise in the credit quality spread (BB-rated bond yield - Treasury bond yield)}\n\\\\b_{i1} = \\text{GDP surprise sensitivity of Asset i}\n\\\\b_{i2} = \\text{credit quality spread surprise sensitivity of Asset i}\n\\\\\\epsilon_{i} = \\text{firm-specific surprise (unrelated to the two macro factors)}\n\\]\n\nEach “F” is a factor surprise, the difference between the predicted value of the factor and the realized value.\nThe firm-specific surprise captures the part of the return that can’t be explained by the model.\n\nBe careful to interpret the signs properly.\n\n\nA risk does not affect many assets can usually be diversified away in a portfolio and will not be priced by the market.\nThe factors in our example model, GDP and credit quality spread shocks, are systematic risk factors, meaning that they will affect even well-diversified portfolios.\n\n\n\nIn a macroeconomic multifactor model, asset returns are a function of unexpected surprises to systematic factors, and different assets have different factor sensitivities.\nThe factor sensitivities of the model can be estimated by regressing historical asset returns on the corresponding historical macroeconomic factors.\n\n\n\n\n\\[\n\\\\R_{i} = a_{i} + b_{i1}F_{P/E} + b_{i2}F_{SIZE} + \\epsilon_{i}\n\\\\R_{i} = \\text{return for stock i}\n\\\\F_{P/E} = \\text{return associated with the P/E factor}\n\\\\F_{SIZE} = \\text{return associated with the SIZE (market capitalization) factor}\n\\\\a_{i} = \\text{intercept}\n\\\\b_{i1} = \\text{standardized sensitivity of stock i to the P/E factor}\n\\\\b_{i2} = \\text{standardized sensitivity of stock i to the SIZE factor}\n\\\\\\epsilon_{i} = \\text{portion of asset i return not explained by the factor model}\n\\]\nStandardized sensitivities (\\(b_{i1}\\) and \\(b_{i2}\\)). Sensitivities in most fundamental factor models are not regression slopes. Instead, the fundamental factor sensitivities are standardized attributes. \\[\n\\\\b_{i1} = \\frac{(P/E)_{i} - \\bar{P/E}}{\\sigma_{P/E}}\n\\] Factor returns (\\(F_{P/E}\\) and \\(F_{SIZE}\\)). The fundamental factors are rates of return associated with each factor. The dependent variable is the set of returns for all stocks and the independent variables are the standardized sensitivities.\n\n\n\n\nSensitivities - The standardized sensitivities in the fundamental factor model are calculated directly from the attribute data-they are not estimated. The macroeconomic model, in which the sensitivities are regression slope estimates.\nInterpretation of factors - The macroeconomic factors are surprises in the macroeconomic variables. In contrast, the fundamental factors are rates of return associated with each factor and are estimated using multiple regression.\nIntercept term - The intercept in the macroeconomic factor model equals the stock’s expected return. In contrast, the intercept of a fundamental factor model with standardized sensitivities has no economic interpretation.\n\n\n\n\nimage-20211005210656567\n\n\n\n\n\nimage-20211005210702810\n\n\n\n\n\nimage-20211005210710178\n\n\n\n\n\nimage-20211005210715799\n\n\n\n\n\n\nActive return equals the differences in returns between a managed portfolio and its benchmark: \\[\n\\\\\\operatorname{Active return} = R_{P} - R_{B}\n\\] Active risk (also known as tracking error or tracking risk) is defined as the standard deviation of the active return: \\[\n\\\\\\operatorname{Active risk} = \\operatorname{tracking error} = \\sigma_{R_{P} - R_{B}}\n\\]\n\n\nActive return alone is insufficient for measuring an investment manager’s performance over a series of measurement periods.\nTo demonstrate a manager’s consistency in generating active return, we use the information ratio, which standardizes average active return by dividing it by its standard deviation. \\[\n\\\\\\operatorname{IR} = \\frac{\\bar{R_{P}} - \\bar{R_{B}}}{\\sigma_{R_{P} - R_{B}}}\n\\] The higher the IR, the more active return the manager earned per unit of active risk.\n\n\n\nimage-20211005211415233\n\n\n\n\n\n\n\\[\n\\\\\\operatorname{active return} = R_{P} - R_{B}\n\\\\\\operatorname{factor return} = \\sum_{i = 1}^{k}{(\\beta_{pi} - \\beta_{bi}) * \\lambda_{i}}\n\\\\\\operatorname{security selection return} = \\operatorname{active return} - \\operatorname{factor return}\n\\\\\\beta_{pi} = \\text{factor sensitivity for the ith factor in the active portfolio}\n\\\\\\beta_{bi} = \\text{factor sensitivity for the ith factor in the benchmark portfolio}\n\\\\\\lambda_{i} = \\text{factor risk premium for factor i}\n\\]\n\n\n\\[\n\\\\\\operatorname{active risk} = \\operatorname{tracking error} = \\sigma_{(R_{P} - R_{B})}\n\\]\nThe active risk of a portfolio can be separated into two components:\n\nActive factor risk\nActive specific risk\n\n\\[\n\\\\\\operatorname{active risk squared} = \\operatorname{active factor risk}^{2} + \\operatorname{active specific risk}^{2}\n\\\\\\operatorname{active factor risk}^{2} = \\operatorname{active risk squared} - \\operatorname{active specific risk}^{2}\n\\]\n\n\n\nimage-20211006144910041\n\n\n\n\n\n\n\nManagers seeking to track a benchmark can construct a tracking portfolio. Tracking portfolios have a deliberately designed set of factor exposures.\n\n\n\nActive managers use factor models to make specific bets on desired factors while hedging (or remaining neutral) on other factors. A factor portfolio is a portfolio that has been constructed to have sensitivity of one to just one risk factor and sensitivities of zero to the remaining factors. Factor portfolios are particularly useful for speculation or hedging purposes.\n\n\n\nThese strategies use rules to mechanically tilt factor exposures when constructing portfolios.\n\n\n\n\n\\[\n\\\\E(R) = R_{F} + \\beta_{1}\\operatorname{RMRF} + \\beta_{2}\\operatorname{SMB} + \\beta_{3}\\operatorname{HML} + \\beta_{4}\\operatorname{WML}\n\\\\\\operatorname{RMRF} = \\text{return on value-weighted equity index - the risk-free rate}\n\\\\\\operatorname{SMB} = \\text{average return on small cap stocks - average return on large cap stocks}\n\\\\\\operatorname{HML} = \\text{average return on high book-to-market stocks - average return on low book-to-market stocks}\n\\\\\\operatorname{WML} = \\text{average returns on past winners - average returns on past losers}\n\\]\n\n\n\nimage-20211006145212417\n\n\n\n\n\n\nUnder the CAPM framework, investors choose a combination of the market portfolio and the risk-free asset depending on their risk tolerance. By including more risk factors, multifactor models enable investors to zero in on risks that the investor has a comparative advantage in bearing and avoid the risks that the investor is incapable of absorbing.\n\n\n\nimage-20211006145246246\n\n\n\n\n\nValue at risk (VaR) measures downside risk of a portfolio. It has three components: the loss size, the probability (of a loss greater than or equal to the specified loss size), and a time frame.\nVaR can also be expressed in percentage terms so that for a portfolio, we could state that the 5% monthly VaR is 3%, meaning that 5% of the time the monthly portfolio value will fall by as least 3%.\n\n\n\nimage-20211006153109480\n\n\n\n\n\nimage-20211006153117871\n\n\n\n\n\nimage-20211006153122956\n\n\n\n\n\n\n\n\n\n\nOne method of estimating VaR is the parametric or vairance-covariance method. Assuming normally allows us to estimate the risk of the portfolio based only on the means, variances, and covariances (or correlations) of the various risk factors. (skewness and kurtosis)\nAssuming normality, we can use the portfolio variance formula to estimate the mean and variance of portfolio returns. \\[\n\\\\\\sigma_{Portfolio}^{2} = W_{A}^{2}\\sigma_{A}^{2} + W_{B}^{2}\\sigma_{B}^{2} + 2W_{A}W_{B}Cov_{AB}\n\\] lookback period - The important point is that the parametric estimates we use should be those we expect over the period for which we are estimating the VaR.\nThe parametric method is relatively simple to apply under the assumption of normally distributed returns. The length of the lookback period will affect the parameter estimates. In cases where normality cannot be reasonably assumed, such as when the portfolio contains options, the parametric method has limited usefulness.\n\n\n\nimage-20211006153135086\n\n\n\n\n\nThe historical simulation method of estimating VaR is based on the actual periodic changes in risk factors over a lookback period.\nUnder the historical simulation method, no adjustment are made for the difference between the results for the lookback period and the results over a longer prior period.\nOne positive aspect of the historical simulation method is that we do not need the assumption of normality, or any other distributional assumption, to estimate VaR.\nVaR estimates will depend on the lookback period and, as with any forecasts, will vary with the characteristics of the sample data used.\n\n\n\nimage-20211006153141597\n\n\n\n\n\nA third method of VaR estimation is Monte Carlo simulation. Monte Carlo simulation is based on an assumed probability distribution for each risk factor.\nThis procedure is repeated thousands of times. As with the other methods, the data used and the assumptions about the distributions of the risk factors will have significant effects on the estimated VaR.\n\n\n\nimage-20211006153148238\n\n\n\n\n\n\n\n\n\nThe concept of VaR is simple and easy to explain.\nVaR allows the risk of different portfolios, asset classes, or trading operations to be compared to gain a sense of relative riskiness.\nVaR can be used for performance evaluation.\nWhen allocating capital to various trading units, a firm’s risk managers can also look at the allocation of VaR, and optimize the allocation of capital given the firm’s determination of the maximum VaR.\nGlobal banking regulators accept VaR as a measure of financial risk.\nReliability of VaR as a measure of risk can be verified by backtesting.\n\n\n\n\n\nVaR estimation requires many choices (loss percentage, lookback period, distribution assumptions, and parameter estimates) and can be very significantly affected by these choices.\nThe assumption of normality leads to underestimates of downside (tail) risk because actual returns distributions frequently have “fatter tails” than a normal distribution. Although the assumption of normality is not a requirement of VaR, it is almost always used, especially with the parametric method.\nLiquidity often falls significantly when asset prices fall.\nIt is well known that correlations increases, or spike, during periods of financial stress. VaR measures based on normal levels of correlation.\nWhile VaR is a single number that can be used to quantify risk, as with any summary measure, many aspects of risk are not quantified or included.\nVaR focuses only on downside risk and extreme negative outcomes. Including consideration of right-hand tail values will give a better understanding of the risk-return trade-off.\n\n\n\n\nimage-20211006153747046\n\n\n\n\n\nimage-20211006153754436\n\n\n\n\n\n\n\n\nAnother measure based on Var is the conditional VaR (CVaR). The CVaR is the expected loss, given that the loss is equal to or greater than the VaR. For this reason, the CVaR is also referred to as the expected tail loss or expected shortfall.\nWhen the VaR is estimated using the historical simulation method or Monte Carlo simulation, we have all the losses greater than the VaR loss, so it is straightforward to take the average of these to get the CVaR. With the parametric method, we don’t know the magnitude of losses greater than the VaR, so calculating the expected loss in the left-hand tail is mathematically complex.\n\nVaR을 넘어선 손실에 대한 측정 - VaR을 넘어갔다고 가정\nHistorical & Monte Carlo - 계산이 어려움\nParametric - 계산 쉬움\n\n\n\n\n\nSecurity가 추가(감소) 될 때 증분 VaR\n\n\n\n\nWe can interpret it as the change in VaR for a 1% increase in the security’s weight. Thus, both the MBaR and IVaR can be used to estimate the change in VaR that will result from a change in the weight of a single security.\n\nPortfolio 안에 weight 들이 미치는 VaR 측정 for 위험관리\n직관적 이해 - Before & After = IVaR, 시점전날 전력투구 과목 고르기 = MVaR\n\n\n\n\n\nEx ante tracking error measures the VaR of the difference between the return on a portfolio and the return on its manager’s benchmark portfolio.\nBenchmark와 Portfolio의 차이 VaR\n\n\n\n\n\nRisk assessment using sensitivity analysis focuses on the effect on portfolio value given a small change in one risk factor. Portfolio risk can be better understood and more effectively managed. Unlike VaR, it does not invlove any prediction of the probability of losses of any specific amount.\n\n민감도 분석, 각 Factor의 영향력 분석\nspecific 확률을 제공하진 않음\n\n\n\n\nScenario analysis provides an estimate of the effect on portfolio value of a set of changes of significant magnitude in multiple risk factors.\n\n각 factor들의 가정들 set -> 결과값 -> 분석\nHistorical scenario - 역사적 기반\nHypothetical scenario - 가상값\nStress testing - 극단값\n\n\n\nA historical scenario approach uses a set of changes in risk factors that have actually occurred in the past.\n\n\n\nWith a hypothetical scenario approach, any set of changes in risk factors can be used, not just one that has happened in the past.\n\n\n\nStress tests examine the effect on value (or solvency) of a scenario of extreme risk-factor changes.\n\n\n\n\n\nThe risk factors used to measure the risks of equities, fixed-income securities, and options are all different. For equities, the most often used risk factor is beta. \\[\n\\\\E(R_{i}) = R_{f} + Beta_{i}[E(R_MKT) - R_{f}]\n\\] For fixed-income securities and portfolios, duration provides an estimate of how market values are affected by changes in interest rates (yield to maturity). For larger changes in interest rates, including the effects of convexity on fixed-income security values improves estimates of the sentisivity of the values of fixed-icnome securities to changes in interest rates. \\[\n\\\\\\operatorname{Change in price} = -Duration(\\Delta{Y}) + \\frac{1}{2}Convexity(\\Delta{Y})^2\n\\] Several risk factors affect the values of options positions. Delta is an estimate of the sensitivity of options values to changes in the value of the underlying asset.\n\nDelta, Gamma, Vega\n\n\nEquity - Beta\n\nCAPM\n\nFixed Income\n\nDuration, Convexity\n\nOption\n\nDelta, Gamma, Vega\n\n\n\n\nSensitivity risk measures can inform a portfolio manager about a portfolio’s exposure to various risk factors to facilitate risk management. Of course, eliminating all risk is not the goal.\nFactor sensitivities can be used to estimate the effects of small changes in risk factors for these securities.\nPricing models can be quite accurate when all of the relevant characteristics of a security are specified.\nScenario analysis is often performed as if the scenario changes were instantaneous.\nIn reverse stress testing, the first step is to identify a portfolio’s largest risk exposures. The question then becomes how likely such scenarios are. Using scenario analysis in this way can be beneficial in helping risk managers identify the vulnerabilities of a portfolio and perhaps mitigate the risk exposures identified.\nSensitivity risk measures - manager에게 risk들을 선택할 수 있게 활용 (베팅, 헷징)\nScenario analysis - 상황변화에 대해 즉각적인 대응을 가능하게 함\nReverse stress testing - testing을 통해 원인 찾기 -> 실제 일어날 수 있을지 여부 검토\n\n\n\n\nVaR, sensitivity analysis, and scenario analysis complement each other, and a risk manager should not rely on only one of these measures.\nThe problem with using duration as the risk measure is that the yield volatility of one portfolio may be quite different from the yield volatility of the other.\nVaR = %, loss 제공\nSensitivity analysis = 각 factor 별 영향 제공, %는 제공 안함\nScenario analysis = 각 factor set 별 영향 제공, %는 제공 안함\n서로 모두 보완역할\n\n\n\nFor each type of organization, differences among firms will result in differences in the risk measures used.\nBanks typically use sensitivity measures (duration of held-to-maturity securities and foreign exchange risk exposure), scenario analysis and stress testing (for their full balance sheets), leverage risk measures, and VaR (especially for trading securities). Banks also estimate risk from asset-liability mismatches, estimate VaR for economic capital, and disaggregate risk by both geographic location and business unit type.\nTraditional (long-only) asset managers typically focus on relative risk measures unless their goal is an absolute return target. A risk measure more specific to asset management is active share: the difference between the weight of a security in the portfolio and its weight in the benchmark.\nEx-post tracking error (backward looking) and ex ante tracking error (forward looking) measures provide different information. Ex-post tracking error is used for performance attribution and to assess manager skill over prior periods. Traditional asset managers mostly use ex ante tracking error for risk estimation. Managers with an absolute return target may use VaR instead.\nFor hedge funds, the risk measures used depend, to some extent, on the strategy employed.\nHedge funds with significantly non-normal returns distributions use a risk measure referred to as maximum drawdown: the largest decrease in value over prior periods of a specific length.\nDefined benefit pension funds calculate the difference between the present value of their assets (often market values) and the present value of their estimated future liabilitieis (payments to retirees and heirs). A risk measure used by pension funds is surplus-at-risk, a VaR for plan assets minum liabilities.\nInsurance companies are often subject to significant regulation of their products and their investment portfolios (reserves). Property and casualty insurers sell auto, home, boat, liability, and health insurance. Insurance risks are reduced by pruchasing reinsurance (from another insurance company) and by geographical diversification.\nP&C insurers use sensitivities of their exposures to market risk factors in their investment portfolios for risk management.\nThe insurance risk of life insurers is more highly correlated with the market risk exposures of their investment portfolios than it is for P&C insurers. The present value of these liabilities are quite sensitive to the discount factors used.\nBank - 보수적, 다 사용한다고 보면 됨\nTraditional (long-only) manager - 보통 relatvie risk measure (benchmark랑 비교)\n\nex-post tracking error - 성과평가 용으로 활용\nex-ante trackign error - risk 미리 측정으로 활용\nabsolute return은 VaR 활용\n\nHedge fund - 전략마다 다 위험지표 다름\n\n정규분포가 아닌 경우 -> Maximum drawdown 사용\n\nDB pension fund\n\nP(A) - P(L) = surplus -> 위험관리 대상\n\nInsurance\n\n손보사\n\nrisk가 market과 상대적으로 연관이 적음\n관리대상 - 보험료 + 운용수익으로 총 비용 cover가 가능한지 여부\n\n생보사\n\nlife time annuity 판매 -> 부채로 계상\n시장 할인율이 어떠냐에 다라 부채의 변동성이 심해지는 risk가 market과 상대적으로 연관이 높음\n\n\n\n\n\nRisk limits that are often imposed include following.\nRisk budgeting refers to a risk management process that first determines the acceptable total risk for an organization, and then allocates that risk to different activities, strategies, or asset classes as appropriate.\nPosition limits are one way to limit risk because they ensure some minimum level of diversification by limiting risk exposures. For example, position limit may be imposed on allocations to individual securities within an asset class, asset classes such as equities or high-yield bonds, investments in a single country, securities in a single currency or the differences between long and short positions for a hedge fund manager.\nScenario limits are limits on expected loss for a given scenario.\nStop-loss limits require that a risk exposure be reduced if losses exceed a specified amount over a certain period of time.\n\nRisk budgeting - firm의 risk를 정하고 -> allocation to 각 부서\nPosition limit - 특정행위 금지 (투기채, 특정주식, 특정국가 등)\nScenario limit - 주어진 시나리오에서의 제한행위\nStop-loss limit - 손실이 특정 범위를 넘어서면 risk exposure 감소시킴\n\n\n\n\nCapital aalocation decisions refer to how the capital of a firm is used to fund its various business units or activities, analogous to asset aalocation for a portfolio manager. Risk management, however, requires that the risk exposure for each use of firm capital be considered.\nOne way to introduce risk exposures to various activities into the capital allocation decision is to limit the overall risk of all the activities.\nrisk 무시하고 return 극대화를 위한 allocation? -> risk exposure를 고려해야 함 ex) VaR 배분\n\n\n\nThe value of any asset can be computed as the present value of its expected future cash flows discounted at an appropriate risk-adjusted discount rate.\nComponents of the discount rate are:\n\nReal risk-free discount rate (R)\nExpected inflation(\\(\\pi\\))\nRisk premium reflecting the uncertainty about the cash flow (RP)\n\nThe value of an asset will change if either the cash flow forecasts change or any of the components of the discount rate changes.\nValue = Cash flow / (1 + Discount Rate)\n\nValue는 CF가 변해도, 할인율이 변해도 변할 수 있음\n\nDiscount rate = real rate + inflation + risk premium\n\n할인율 역시 자산이 변해도, Investor의 기대가 변해도 변할 수 있음\n\n\n\n\nThe value of an asset depends on 1) its expected future cash flows and 2) the discounted rate used to value those cash flows. As market participants receive new information, the timing and amounts of expected future cash flows are revised and valuations change as a result. The impact of new information will depend on its effect on current expectations.\n기존의 expectation -> CF 추정 / 할인율 추정 -> 결과\n그런데 새로운 정보가 유입되어서 CF와 할인율이 변한다면 -> 기존의 결과가 변함\n\n\n\nThe real risk-free rate of interest derives from the inter-temporal rate of substitution, which represents an investr’s trade-off between real consumption now and real consumption in the future. \\[\n\\\\\\operatorname{inter-temporal\\, rate\\, of\\, substitution} = m_{t} = \\frac{\\operatorname{marginal utility of consuming 1 unit in the future}}{\\operatorname{marginal utility of current comsumption of 1 unit}} = \\frac{u_{t}}{u_{0}}\n\\] Investors always prefer current comsumption over future consumption (\\(u_{0}>u_{t}\\)) and \\(m_{t} < 1\\) as a result. \\[\n\\\\P_{0} = E(m_{t})\n\\\\R = \\frac{1 - P_{0}}{P_{0}} = \\frac{1}{E(m_{t})} - 1\n\\]\n\nThe higher the utility investors attach for current consumption relative to future consumption, the higher the real rate.\nInvestor’s marginal utility of consumption declines as wealth increases.\nIf investors expect higher incomes in the future, their expected marginal utility of future consumption is decreased relatvie to current consumption.\nInvestors increase their savings rate when expected returns ar high or when uncertainty about their future income increases.\n\nReal risk-free rate -> inter-temporal rate of subsitution (현재의 소비 vs 미래의 소비)\nMt = 미래효용 / 현재효용\n투자자는 항상 현재를 더 선호 -> Mt < 1\n\n\n\nimage-20211007163653421\n\n\n기억해야 할 것\n\n현재가 더 중요한 투자자 -> 더 높은 real rate을 가짐\n부가 증가하면 소비효용은 감소한다 -> 소비효용이 증가하면 부가 감소하는 것을 의미 (경기 수축)\n미래 수입의 증가를 기대 -> 미래 현재효용이 감소\n미래 불확실성이 증대, 기대 return이 증가 -> 투자자의 saving 증가\n\n\n\nThe risk aversion of investors can be explained by the covariance of an investor’s inter-temporal marginal rate of subsitution and expected on savings. Investors experience a large loss of utility for a loss in wealth as compared to a gain in utility for an equivalent gain in wealth. This property is called as risk-aversion.\nAn investor’s absoulte risk-aversion declines with their wealth; wealther investors are less risk-averse and more willing to take risk relative to their poorer counterparts. However, the marginal utility of holding risky assets declines as an investor holds more risky assets in her portfolio. \\[\n\\\\P_{0} = \\frac{E(P_{1})}{1 + R} + cov(P_{1}, m_{1})\n\\\\R = \\text{the real risk-free rate}\n\\] The covariance between the expected future price of the bond and the investor’s inter-temporal rate of substitution can be viewed as a risk premium. For risk-averse investors, the covariance is negative; when the expected future price of the asset is high, the marginal utility of future consumption relative oto current consumption is low. Everything else constant, the lower current price(\\(P_{0}\\)) increases expected return.\nrisk aversion\n\n같은 gain loss에서 loss에 더 고통스러워하는 자\n부자일수록 absolute risk aversion은 감소\n부자일수록 less risk averse, willingness to take risk 가능성 증가\nrisky asset이 많을수록, risk asset의 한계효용은 감소\n\n\n\n\nimage-20211007163853789\n\n\n\n\n\nIf GDP growth is forecasted to be high, the utility of consumption in the future (when incomes will be high) will be low and the inter-temporal rate of substitution will fall; investors will save less, increase real interest rates. Therefore, real interest rates will be positively correlated with real GDP growth rates.\nGDP growth rate이 높은 것으로 기대\n\n미래소비 효용 감소, less save\n자금이 귀해지고 real interest rate 증가\n결국 real interest rate과 GDP 변동성은 상관계수가 positive 관계\n\n\n\n\n\nNomial risk-free interest rates include a premium for expected inflation (\\(\\pi\\)). However, actual inflation is uncertain.\nFor short-term risk-free securities, the uncertainty about inflation is negligible. \\[\n\\\\r(\\operatorname{short-term}) = R + \\pi\n\\\\r(\\operatorname{long-term}) = R + \\pi + \\theta\n\\] For longer term bonds, we add the risk premium for uncertainty about inflation, \\(\\theta\\).\nActual inflation을 고려하자 (nominal)\n\nShort-term = R + inflation\nLong-term = R + inflation + actual\n\n\n\nCentral banks are usually charged with setting policy rates so as to 1) maintain price stability and 2) achieve the maximum sustainable level of employment. \\[\n\\\\r = R_{n} + \\pi + 0.5(\\pi - \\pi^{*}) + 0.5(y - y^{*})\n\\\\\\pi^{*} = \\text{central bank's target inflation rate}\n\\\\y = \\text{log of current level of output}\n\\\\y^{*} = \\text{log of central bank's target (sustainable) output}\n\\]\n\nprice stability를 유지하며, 고용수준 최대 달성목표\n중앙은행의 정책 이자율\n\n\n\n\nWhen the economy is in recession, policy rates tend to be low. Leads to higher longer-term rates. This results in a positively sloped yield curve. Conversely, expectations of a decline in GDP growth results in a negatively sloped (inverted) yield curve. For this reason, an inverted yield curve is often considered a predictor of future recessions.\nA term spread is the difference between the yield on a longer-term bond yield and the yield on a short-term bond. Evidence suggests that normal term spread is positive so the yield curve is upward sloping.\n\nrecession -> policy rate은 low\n경기침체 벗어나기 위해 GDP 기대 올리고 + inflation 적절히 관리\nYC가 우상향 (positively sloped YC)\nGDP 감소 trend -> negatively sloped YC\nInverted YC가 종종 경기침체의 signal로 관찰됨\nTerm spread = LT bond - ST bond\nNominal term이 + -> upward YC\n+라는 것은 -도 존재한다는 것을 의미\n\n\n\n\n\nThe difference between the yield of a non-inflation-indexed risk-free bond and the yield of an inflation-indexed risk-free bond of the same maturity is the break-even inflation rate (BEI). \\[\n\\\\BEI = \\operatorname{yield\\, on\\, non-inflation-indexed\\, bond} - \\operatorname{yield\\, on\\, inflation-indexed\\, bond} = \\pi + \\theta\n\\] BEI is composed of two elements: expected inflation (\\(\\pi\\)) and a risk premium for uncertainty about actual inflation (\\(\\theta\\)).\n\n\n\nThe required rate of return for bonds with credit risk includes an additional risk premium. This credit risk premium (credit spread) is the difference in yield between a credit risky bond and a default-free bond of the same maturity. \\[\n\\\\\\operatorname{required rate of return for credit risky bonds} = R + \\pi + \\theta + \\gamma\n\\\\\\gamma = \\text{additional risk premium for credit risk} = \\text{credit spread}\n\\] Creadit spreads tend to rise during times of economic downturns and fall during expansions.\nWhen credit spreads narrows, credit risky bonds will outperform default-free bonds. Overall, lower rated bonds tend to benefit more then higher rated bonds from a narrowing of credit spreads (their yields fall more). Conversely, when credit spreads widen, higher rated bonds will outperform lower rated bonds on a relative basis (because their yields will rise less).\nCredit RP = 같은 만기 credit risky bond - default free bond\nrequired rate = r + inflation + risk premium + yield curve credit spread\neconomic downterm -> spread 증가\neconomic expansion -> spread 감소\n경제 위축 -> default 증가 -> credit loss 증가\nbond 성과 -> spread가 좁아질 때 -> lower rate > higher rate\nspread가 넓어질 때 -> lower rate < higher rate\n\n\n\nSpreads differ among sectors and over time. Differences in credit spreads are primarily due to differences in indsutry products and services and the financial leverage of the firms in the industry. Spreads for issuers in the consumer cyclical sector invrease significantly during economic downturns compared to spreads for isseurs in the consumer non-cyclical sector.\n산업별로, 시간별로 spread 차이가 발생 - product, service, leverage 차이\n\n\n\nCyclical industries tend to be relatively more sensitive to the phase of the business cycle. Companies in these industries have revenues and earnings that rise and fall with the rate of economic growth. Defensive or non-cyclical industries tend to be relatively immune to fluctuations in economic activity; their earnings tend to be relatively stable throughout the business cycle.\n\ncyclical industry -> business cycle에 더 민감, economic growth rate이 중요\ndefensive -> 상대적으로 덜 영향\n\n\n\n\nThe discount rate used to value equity secutiries includes an additional risk premium, the equity risk premium because equity is more risky than debt. \\[\n\\\\\\operatorname{Discount rate for equity} = R + \\pi + \\theta + \\gamma + \\kappa\n\\\\\\kappa = \\text{additional risk premium relative to risky debt for an investment in equities}\n\\\\\\lambda = \\text{equity risk premium} = \\gamma + \\kappa\n\\] Assets that provide a higher payoff during economic downturns are more highly valued because of the consumption hedging property of the asset. Equity prices are generally cyclical, with higher values during economic expansions when the marginal utility of consumption is lower. Equity investments, therefore, are not the most effective hedge against bad consumption outcomes.\nDiscount rate = R + inflation + risk premium + y + k\n\n\n\n경기침체 시 더 payoff 하는 자산이 good\n이런 자산은 risk premium이 감소\n경기확장 시 equity는 가격증가 - cyclical -> 주식투자는 경기침체 시 좋은 hedge 수단이 아님\n\n\n\n\n\nPrice multiples such as P/E and P/B are often used in determining the relative values of companies, of sectors, or of the overall market from a historical perspective. However, it is inappropriate to judge the multiple in a historical context only.\nPrice multiples are positively correlated with expected earnings growth rates and negatively correlated to required returns.\nShiller’s CAPE (real cyclically adjusted P/E) ratio reduces the volatility of unadjusted P/E ratios by using real prices in the numerator and a 10-year moving average of real earnings in the denominator.\nPrice multiple (P/E, P/B)\n\n회사별 sector별 역사적 관점으로 판단 x\nearning growth %와 price multiple은 상관계수가 +\nrequired return과 price multiple은 상관계수가\n\n\n\n\nGrowth stocks are characterized by high P/Es and by low dividend yields and tend to be in immature markets with high growth prospects. Value stocks tend to have low P/Es, have high dividend yields, and are generally found in established and mature markets. A value strategy performs well during and immediately following recessionary conditions, while growth strategy performs well during economic expansions.\n\nGrowth = High P/E, Low dividend, 경기확장 시 good\nValue = low P/E, high dividend, stable earning, 경기침체 시 good\n\n\n\n\nEx post risk premiums on equity sectors can be computed as the difference between the average return on a sector and the short-term risk free rate. Getting the timing right is of course very difficult. The point, however, is that understanding and forecasting the relationship between the equity market performance of different sectors and the business cycle would help analysts enhance their sector rotation strategies.\nRotation = cyclical vs. non-cyclical\n분석 필요 - correlation, risk premium 등\n\n\n\nCommercial real estate investments have\n\nBond-like characteristics. The steady rental income stream is similar to cash flows from a portfolio of bonds. The credit quality of tenants affects the value of commercial real estate.\nEquity-like characteristics. The value of commercial real estate is influenced by many factors, including the state of the economy, the demand for rental properties, and property location. Uncertainty about the value of the property at the end of the lease term gives commercial properties an equity-like character.\nIlliquidity.\n\n\n\n\\[\n\\\\\\operatorname{Discount rate for commercial real estate} = R + \\pi + \\theta + \\gamma + \\kappa + \\phi\n\\\\\\kappa = \\text{risk premium for uncertainty about termainal value of property (similar to the equity risk premium)}\n\\\\\\phi = \\text{risk premium for illiquidity}\n\\]\n성격\n\nbond-like - 안정적 임대수입, 임차인에 따라 신용도 영향\nequity-like - value가 경기에 영향 받음, 임대기간 말에 불확실성 존재\n\nvaluation = R + inflation + risk premium + r + k + Rp(비유동성 프리미엄)\n상업용 부동산은 가치가 cyclical -> 다른자산과 상관계수 +\n\n\n\n\nThe information ratio is used to evaluate active managers and can be used to make portfolio allocation decisions for an investor.\nActive management seeks to add value by outperforming a passively managed benchmark portfolio.\n\n\nActive return(\\(R_{A}\\)) is the value added by active management. Active return can be measured ex-ante or ex-post.\nActive weights in a portfolio determine the amount of value added. Active weights must sum to zero.\nExpected returns on the active and benchmark portfolios can be computed as the weighted average of securities returns: \\[\n\\\\E(R_{P}) = \\sum{w_{Pj}E(R_{Pj})}\\, and\\, E(R_{B}) = \\sum{w_{Bj}E(R_{Bj})}\n\\\\E(R_{A}) = \\sum{w_{Pj}E(R_{Pj})} - \\sum{w_{Bj}E(R_{Bj})}\n\\] Active return can be decomposed into two parts:\n\nAsset allocation return\nSecurity selection return\n\n사전적, 사후적으로 측정 가능\n\n\n\n\nThe Sharpe ratio (SR) is calculated as excess return per unit of risk (standard deviation): \\[\n\\\\SR = \\frac{R_{P} - R_{F}}{\\sigma_{P}}\n\\] An important attribute of the Sharpe ratio is that it is unaffected by the addition of cash or leverage in the portfolio.\nThe information ratio (IR) is the ratio of the active return to the standard deviation of active returns, which is known as active risk: \\[\n\\\\IR = \\frac{R_{P} - R_{B}}{\\sigma_{(R_{P} - R_{B})}} = \\frac{R_{A}}{\\sigma_{A}} = \\frac{\\operatorname{active return}}{\\operatorname{active risk}}\n\\]\n\n\n\nThe information ratio that we are considering is usually the ex-ante information ratio. The ex-ante information ratio is generally positive, while ex-post information ratios will often turn out to be negative.\nA closet index fund is a fund that is purposed to be actively managed but in reality closely tracks the underlying benchmark index. These funds will have a Sharpe ratio similar to that of the benchmark index, a very low information ratio, and little active risk. After fees, the information ratio of a closet index fund is often negative.\nA fund with zero systematic risk that uses the risk-free rate as its benchmark would have an information ratio that is equal to its Sharpe ratio.\nUnlike the Sharpe ratio, the information ratio will change with the addition of cash or the use of leverage. Adding cash to a portfolio is likely to lower active return.\nThe information ratio of an unconstrained portfolio is unaffected by the aggressiveness of the active weights. If the active weights of a portfolio are tripled, the active return and the active risk both triple, leaving the information ratio unchanged.\nThe resulting blended portfolio will have the same information ratio as the original actively managed portfolio, leaving the information ratio unchanged.\nInvestors can select an appropriate amount of active risk by investing a portion of their assets in the active portfolio and the remaining portion in the benchmark.\n\nFor an unconstrained active portfolio, the optimal amount of active risk is the level of active risk that maximizes the portfolio’s Sharpe ratio. This optimal amount of active risk can be calculated as: \\[\n\\\\\\sigma_{A}^{*} = \\frac{IR}{SR_{B}}\\sigma_{B}\n\\\\SR_{P} = \\sqrt{SR_{B}^{2} + IR^{2}}\n\\] \n\n\n\nimage-20211008062659880\n\n\n\n\n\n\n\nThe information coefficient (IC) is a measure of a manager’s skill. IC is the ex-ante, risk-weighted correlation between active returns and forecasted active returns.\nThe transfer coefficient (TC) can be thought of as the correlation between actual active weights and optimal active weights. For an constrained active portfolio, the active weights will be equal to the optimal weight and TC = 1.\nBreadth (BR) is the number of independent active bets taken per year.\n\n\\[\n\\\\IR = TC * IC * \\sqrt{BR}\n\\\\E(R_{A}) = TC * IC * \\sqrt{BR} * \\sigma_{A}\n\\]\nIR의 3가지 구성항목\n\nInformation coefficient - manager skill\nTransfer coefficient - 자유도 정도\nbreadth - number of independent active part\n\nGrinold rule - modified active return 개념\nIR을 늘리려면? - IC를 증가시키거나, BR을 늘리거나\n\n\n\nPortfolio theory concludes that investors choose some combination of the risk-free asset and an optimal risky portfolio, with the weights determined by their preferences (risk tolerance). The optimal risky portfolio is the portfolio with the highest Sharpe ratio. The portfolio with the highest information ratio will also be the portfolio with the highest Sharpe ratio, so investors will choose the active manager with the highest Sharpe ratio. \\[\n\\\\E(R_{A}) = IR * \\sigma_{A}\n\\] 결론\n\noptimization risk portfolio - highest Sharpe ratio\nHighest IR portfolio - highest Sharpe ratio portfolio\n\n\n\n\nMarket timing is simply a bet on the direction of the market (or a segment of the market). \\[\n\\\\IC = 2(\\operatorname{\\%\\, of\\, correct}) - 1\n\\]\n\n\nMarket timing can also be used to make sector rotation decisions. \\[\n\\\\\\sigma_{C} = [\\sigma_{X}^{2} - 2\\sigma{_X}\\sigma_{Y}\\rho_{XY} + \\sigma_{Y}^2]^{1/2}\n\\] 시장방향에 베팅\n우수한 성과를 보이는 sector에 자산 할당\n\n\n\n\nThe practical limitations of the fundamental law of active management can be summarized as “garbage in, garbage out;” poor input estimates lead to incorrect evaluations. In the case of unconstrained optimization, the two components (inputs) that determine the information ratio are 1) the information coefficient (IC) and 2) the breadth (BR) of the manager’s strategy.\nThe limitations are generally derived from inaccurate estimates of the two inputs:\n\nEx-ante measurement of skill - Managers tend to overestimate their ability to outperform the market and, hence, overestimate their IC.\nIndependence - If two or more decisions rely on same (or similar) information, then they are not independent.\n\n\\[\n\\\\BR = \\frac{N}{1 + (N - 1)r}\n\\\\N = \\text{number of decisions}\n\\\\r = \\text{correlation between the decisions}\n\\]\nLimitation - ex ante manager skill - 고평가\nindependence - 진짜 결정이 다 독립적?\n그래도 성과평과 input이 적으니 편함\n\n\n\nEvaluation of trade execution should not only consider explicit costs such as brokerage fees but also implicit costs.\n\n\nExplicit trading costs include brokerage, taxes, and fees.\nImplicit costs are harder to measure, and include the bid-ask spread, market or price impact costs, opportunity costs, and delay costs (or slippage).\n\nThe bid-ask spread is the highest potential cost for a small trade.\nMarket impact (or price impact) is the impact of demanding illiquidity in the market.\nDelay cost (or slippage) is the cost of an adverse price movement during the lag in executing a large trade.\nOpportunity cost arises from unfilled orders or failed trading opportunities.\n\nNote that there is a tradeoff between the market impact cost on one hand and the delay plus opportunity cost on the other.\n\nExplicit cost - brokerage, taxes, fee\nImplicit cost - Bid-ask spread, market impact, delay cost, opportunity cost\n\ntradeoff 존재 - market impact vs delay + opportunity cost\n\n\n\n\nLarger orders have a price impact as they move down the order book (and, therefore, execute at worse prices). This price impact depends on the size of the order and the relative liquidity in the market.\nDealer는 거래중개인\nbid = 삽니다 / ask = 팝니다 -> 둘 사이 차이는 inside spread, 중간은 midquote\nlarge order는 price impact 존재 (해당 단가에 물량이 별로 없다면) -> 대안으로 standing limit order (지정가 주문), 그러나 체결이 안 될 위험있음\n\n\nThe implicit costs of a trade are measured relative to a benchmark. Several benchmarks can be used to evaluate trades, including the effective spread, VWAP, and implementation shortfall.\n\n\nThe effective spread transactions cost uses the midquote price as the benchmark price. \\[\n\\\\\\operatorname{per-share\\, effective\\, spread\\, transaction\\, cost} = \\operatorname{side} + (\\operatorname{transaction price} - \\operatorname{midquote price})\n\\\\\\operatorname{side} = \\text{+1 for buy orders and -1 for sell orders}\n\\\\\\operatorname{effective spread} = 2 * \\operatorname{per-share\\, effective\\, spread\\, transaction\\, cost}\n\\] If the trade occurs at a better price, there will be a price improvement.\nSince the effective spread is less than the quoted spread, there was not a price improvement on that trade.\nLimitations of effective spread\n\nWhen a larger order is split into smaller orders, the effective spread is a poor indicator of trade performance.\nEffective spread also does not account for slippage or delay costs when part of the order does not get filled at desired prices.\nEffective spread does not capture the opportunity cost of a trade.\n\nSpread는 낮을수록 거래를 잘한 것\n단점 - 거래량을 무시 -> delay cost, opportunity cost 반영 못함 (거래가 이루어지지 않았을 경우)\n\n\n\nVWAP is easy to interpret-it evaluates the price at which an order was executed relative to other trades occurring during the same time period.\nTo evaluate a trade, the VWAP of the trade is compared to the benchmark VWAP. \\[\n\\\\\\operatorname{VWAP transaction cost} = \\operatorname{trade size} * \\operatorname{side} * (\\operatorname{trade VWAP} - \\operatorname{benchmark VWAP})\n\\] Limitations of VWAP transaction cost\n\nVWAP is not useful if the trade being evaluated is a significant part of the trading volume.\nVWAP does not capture the price impact cost.\n\n하루에 거래한 내역들을 가중평균(거래량 기준)한 가격\nBM VWAP과 비교\n단점 - 거래량이 영향 -> 거래량이 너무 많으면 BM과 별 차이 없어짐, price impact cost 반영 못함 (단 1개의 거래로 가격이 상승했을 경우)\nVWAP의 탄생 -> 장 마감 직전까지 기다렸다가 거래를 하려는 manager들이 생김 -> implementation shortfall 탄생\n\n\n\n\n\n\n\nImplementation shortfall is a conceptual approach that measure transactions costs as the difference between the value of the actual portfolio and the value of hypothetical paper portfolio.\nIt measures the total cost of trading by capturing all three implicit costs (i.e., price impact, slippage, and opportunity costs).\npaper portfolio와 actual portfolio를 비교\npaper portfolio는 가상의 portfolio로 trade가 모두 원래 계왹한 가격에 이루어졌다고 가정 -> actual과의 비교로 price impact, slippage, opportunity costs 모두 잡아냄\n\n\n\n\nThe use of information technology in the development of electronic markets has resulted in lower trading costs and improved execution efficiency.\nFactors driving development of electronic trading systems are\n\nCost\nAccuracy\nAudit trails\nFraud prevention\nContinuous market\n\nBond market trades remain largely over the counter.\n기술발달 -> 수요공급자들을 보다 빠르게 연결 -> trading cost 감소, 거래효율성 증가\n장점 - costs, accuracy, audit trail, fraud prevention, continuous market\n주식시장 대부분 electronic trading, 그러나 아직 채권시장은 dealer에게 많이 의존 (OTC)\n\n\n\nMarket fragmentation occurs when a security trades in multiple markets.\nAutomated trading strategies for large orders seek out liquidity across all markets to minimize the price impact of their trades.\n특정주식이 여러시장에서 거래될 때 -> market fragmentation 발생 -> 각 시장의 liquidity 차이로 cost 발생 가능\n대안 - 자동화 기술 -> liquidity aggregation - 모든 시장을 다 뒤짐 / smart order - 제일 효율적인 시장 고름\n\n\n\n\n\nThese analyze high-speed news feeds and submit market orders (as opposed to limit orders) based on the analysis.\n뉴스를 빠르게 분석하여 시장 거래, AI 사용하기도 함\n\n\n\nThese post bid and offer prices to profit from the spread. Dealer profit depends on the frequency with which they trade, Electronic dealers maintain a low inventory of actively traded securities and quickly adjust their positions based on market information.\nBid-ask spread로 이익 추구, low inventory (빠르게 position이 adjusted되어서)\n\n\n\nThese trade in multiple markets seeking to exploit price discrepancies.\nprice discrepancies 찾아다님\n\n\n\nThese use artificial intelligence to sniff out large trades (or many small trades) on the same side and then use low latency to trade ahead of them. Front runners search for patterns in order submissions or relationships between orders and events.\n빠르게 행동하는 trader, 대량거래 발생 전에 해당주식 매수하여 매도 (pattern과 event를 분석)\n\n\n\nStanding limit orders provide valuable information to other traders; they disclose the intend of the trader posting the order to buy or sell the specified quantity.\n특정가격에 매수/매도 주문 -> 그 자체만으로도 다른 trader들에게 의미있는 정보\n\n\n\nThese execute trades on behalf of their portfolio managers.\nportfolio manager의 주문을 받아 대신 거래하는 trader\n\n\n\n\nIn addition to lower trading costs, electronic trading system allow low-latency traders a competitive advantage by jumping the order queue. In other words, electronic traders have to be faster than their competitors to capitalize on market opportunities.\nLow-latency traders focus on minimizing the latency involved in three phases of a trade:\n\nThe time gap between the publication of actionable news and the receipt of that news by the trader.\nThe time gap between the receipt of actionable news and deciding on an appropriate action.\nThe time gap between order determination and communicating that order to the exchange.\n\n\nCommunication speed\nComputation speed\n\nelectronic trading -> low latency, 대응속도가 빨라짐 -> 과거보다 빠르게 시장의 개회를 capture 가능\n1: 시장에서 정보의 발생 -> 2: trader가 정보를 취득 -> 3: trader가 의사결정 후 명령 -> 4: 거래 수행\n2, 4: communication speed\n3: computation speed\n\n\n\nAdvanced order types are limit orders with a dynamic limit price that varies with a benchmark.\nTrading tactics are plans calling for the submission of multiple orders.\nAlgorithms (or algos) are programmed execution strategies using multiple orders, sequencing of orders, and trading tactics to achieve specific goals.\n\n\n\nHidden orders\nLeapfrog - This is the practice of beating the best bid or ask price. A leapfrogging limit order therefore narrows the inside spread.\nFlickering quotes - These are exposed limit orders that are submitted and canceled almost immediately.\nElectronic arbitrage\n\nTake liquidity on both sides - This is an arbitrage trade to buy and sell the same security in different markets to take advantage of mispricing across markets.\nOffer liquidity on one side\nOffer liquidity on both sides - This strategy is fraught with risks; after one leg of the order is filled, if the other leg does not fill, the trader is exposed to risk of adverse price movement.\n\nMachine learning - Also known as data mining, machine learning involves using statistical modeling techniques that allow the model to evolve based on new data.\n\nadvanced order type - limit order with a dynamic limit price\n\n\n\nimage-20211008074458164\n\n\nalgorithms - 미리 설정&프로그램 된 전략으로 trading\nElectronic trading의 영향\n\nhidden orders - 기존 거래방식(bid-ask)을 따르지 않고, 거래정보(단가, 규모)를 공개하지 않은 상태로 거래\nleapfrog - ask 관련 dealer들의 경쟁 심화 -> bid-ask spread가 좁혀짐\nflickering quotes - 주문 후 바로 취소, 상대방이 해당 주문 가격으로 먼저 제안하기를 바라는 전략 (우리 물량은 공개하기 싫을 때)\narbitrage\n\ntake liquidity on both sides - 다른 시장에서 다른 가격으로 판매되는 security들을 동시에 buy&sell (market order)\noffer liquidity on one side - 특정주식을 싼 시장에서 사서 비싼시장에 팔기 (limit order)\noffer liquidity on both sides - 특정주식을 best 가격보다 비싸게 사서 다른 시장에 팔기 (limit order) -> 위험노출\n\nmachine learning - data mining, 통계, 새로운 data로 알고리즘 개발 -> 극단 case에는 안 들어 맞을수도\n\n\n\n\n\n\nHigh-frequency traders (HFT) arms race - This arms race offsets some of the lower cost benefits of electronic trading.\nSystemic risk\n\nRunaway algorithms - These produce a series of unintended orders. The flash crash of May 2010 was caused by a trading algorithm.\nFat finger errors - These are input errors.\nOvercharging orders - These demand liquidity significantly higher than what is available in the market.\nMalevolent orders - These are created to specifically manipulate the market. Examples includes aggrieved employees programming rogue trades and traders seeking to conduct denial-of-service attacks on their competitors with excessive submission of quotes.\n\n\nhigh-frequency trader (HFT) arms race - 최첨단 시설 구축으로 인한 비용 (cost 증가)\nsystematic risk - 기술의 발달로 인해 피해자들이 발생할 수 있음\n\nrunaway algorithms, fat finger errors, overcharge orders, malevolent orders\n\n\n\n\n\nFront running -This is low-latency trading ahead of known large trades.\nMarket manipulation - This includes activities that produce false market data, including price and volume data.\n\nTrading for market impact - These are trades designed to change the price.\nRumormongering - This involves dissemination of fake information to affect the target trader’s value assessments.\nWash trading - This is trading between commonly controlled accounts to create an impression of false liquidity.\nSpoofing or layering - These are fake limit orders posted to create fake optimism or pessimism about the security.\nBluffing - This involves preying on momentum traders.\nGunning the market - This focuses other traders into bad trades.\nSqueezing and cornering\n\n\nfront running - low-latency trading head of known large trade\nmarket manipulation - 거짓된 data로 시장 조작\n\ntrading for market impact (시장가격 영향주기)\nrumormongering (루머)\nwash trading (자전거래)\nspoofing & layering (optimism 또는 pessimism으로 시장가보다 다른 가격 제시, 시장 흔들기)\nbluffing - 모멘텀 투자자들\ngunning the market - stop-loss 주문한 투자자들 노리기, 대량 short하여 나오는 물량 사기\nsqueezing & cornering - 상대방으로 default 위험에 처하게 해서 이익 추구"
  },
  {
    "objectID": "posts/2021-11-05-system-programming-week-10/2021-11-05-system-programming-week-10.html",
    "href": "posts/2021-11-05-system-programming-week-10/2021-11-05-system-programming-week-10.html",
    "title": "System Programming Week 10",
    "section": "",
    "text": "image-20211105044533457\n\n\n\n\n\n\n\n\nimage-20211105044722848\n\n\n\n\n\n\n\n\nimage-20211105044811282\n\n\n\n\n\n\n\n\nimage-20211105045057795\n\n\n\n\n\n\n\n\nimage-20211105045208162\n\n\n\n\n\nimage-20211105045212633\n\n\n\n\n\nimage-20211105045222250\n\n\n\n\n\nimage-20211105045228105\n\n\n\n\n\nimage-20211105045233171\n\n\n\n\n\nimage-20211105045237172\n\n\n\n\n\n\n\n\nimage-20211105045349573\n\n\n\n\n\n\n\n\nimage-20211105045402870\n\n\n\n\n\nimage-20211105045523609\n\n\n\n\n\n\n\n\nimage-20211105045534589\n\n\n\n\n\n\n\n\nimage-20211105045825242\n\n\n\n\n\n\n\n\nimage-20211105045954225\n\n\n\n\n\n\n\n\nimage-20211105050057842\n\n\n\n\n\n\n\n\nimage-20211105050244248\n\n\n\n\n\n\n\n\nimage-20211105050303081\n\n\n\n\n\n\n\n\nimage-20211105050312625\n\n\n\n\n\nimage-20211105050317570\n\n\n\n\n\nimage-20211105050322008\n\n\n\n\n\nimage-20211105050327362\n\n\n\n\n\nimage-20211105050333026\n\n\n\n\n\nimage-20211105050341107\n\n\n\n\n\nimage-20211105050346698\n\n\n\n\n\nimage-20211105050353387\n\n\n\n\n\nimage-20211105050402110\n\n\n\n\n\n\n\n\nimage-20211105050412211\n\n\n\n\n\nimage-20211105050619395\n\n\n\n\n\n\n\n\nimage-20211105050750980\n\n\n\n\n\n\n\n\nimage-20211105051056494\n\n\n\n\n\n\n\n\nimage-20211105051108819\n\n\n\n\n\nimage-20211105051239548\n\n\n\n\n\nimage-20211105051248387\n\n\n\n\n\n\n\n\nimage-20211105051431580\n\n\n\n\n\n\n\n\nimage-20211105051715488\n\n\n\n\n\n\n\n\nimage-20211105051857954\n\n\n\n\n\n\n\n\nimage-20211105052014751\n\n\n\n\n\n\n\n\nimage-20211105052109127\n\n\n\n\n\n\n\n\nimage-20211105052027082\n\n\n\n\n\nimage-20211105052047171\n\n\n\n\n\n\n\n\nimage-20211105052421509\n\n\n\n\n\n\n\n\nimage-20211105052507969\n\n\n\n\n\nimage-20211105052519356\n\n\n\n\n\n\n\n\nimage-20211105052533333\n\n\n\n\n\n\n\n\nimage-20211105052923955\n\n\n\n\n\n\n\n\nimage-20211105053032275\n\n\n\n\n\nimage-20211105053055659\n\n\n\n\n\nimage-20211105053103069\n\n\n\n\n\n\n\n\nimage-20211105053123155"
  },
  {
    "objectID": "posts/2021-10-27-system-programming-midterm/2021-10-27-system-programming-midterm.html",
    "href": "posts/2021-10-27-system-programming-midterm/2021-10-27-system-programming-midterm.html",
    "title": "System Programming midterm",
    "section": "",
    "text": "Abstraction have limits\n\nEspecially in the presence of bugs\nNeed to understand details of underlying implementations\n\nUseful outcomes from taking system programming\n\nAble to find and eliminate bugs efficiently\nAble to understand and tune for program performance\n\n\n\n\n\n\n\nimage-20211027041921386\n\n\n\n\n\nimage-20211027041925730\n\n\n\n\n\nDoes not generate random values\nCannot assume all usual mathematical properties\n\nDue to finiteness of representations\nInteger operations satisfy ‘ring’ properties\n\nCommutativity, associativity, distributivity\n\nFloating point operations satisfy ‘ordering’ properties\n\nMonotonicity, values of signs\n\n\nImportant issues for compiler writers and serious application programmers.\n\n\n\nUnderstanding assembly is key to machine-level execution model\n\nBehavior of programs in presence of bugs\n\nHigh-level language models break down\n\nTuning program performance\n\nUnderstand optimizations done / not done by the compiler\nUnderstanding sources of program inefficiency\n\nImplementing system software\n\nCompiler has machine code as target\nOperating systems must manage process state\n\n\n\n\n\nMemory is not unbounded\n\nIt must be allocated and managed\nMany applications are memory dominated\n\nMemory referencing bugs especially pernicious\n\nEffects are distant in both time and space\n\nMemory performance is not uniform\n\nCache and virtual memory effects can greatly affect program performance\nAdapting program to characteristics of memory system can lead to major speed improvements\n\n\n\n\nimage-20211027042541071\n\n\n\n\n\nimage-20211027042551119\n\n\n\n\n\n\nC and C++ do not provide any memory protection\n\nout of bounds array refences\nInvalid pointer values\nAbuses of malloc / free\n\nCan lead to nasty bugs\n\nWhether or not bug has any affect depends on system and compiler\nAction at a distance\n\nCorrupted object logically unrelated to one being accessed\nEffect of bug may be first observed long after it is generated\n\n\n\n\n\n\nConstant factors matter too\nAnd even exact op count does not predict performance\n\nEasily see 10:1 performance range depending on how code written\nMust optimize at multiple levels - algorithm, data representations, procedures, and loops\n\nMust understand system to optimize performance\n\nhow programs compiled and executed\nhow to measure program performance and identify bottlenecks\nhow to improve performance without destroying code modularity and generality\n\n\n\n\nimage-20211027042938550\n\n\nHierarchical memory organization\nPerformance depends on access patterns\n\nIncluding how step through multi-dimensional array\n\n\n\n\nThey need to get data in and out\n\nI/O system critical to program reliability and performance\n\nThey communicate with each other over networks\n\nMany system-level issues arise in presence of network\n\nConcurrent operations by autonomous processes\nCoping with unreliable media\nCross platform compatibility\nComplex performance issues\n\n\n\n\n\nEach bits is 0 or 1\nBy encoding / interpreting sets of bits in various ways\nWhy bits? - Electronic implementation\n\nEasy to store with bistable elements\nReliability transmitted on noisy and inaccurate wires\n\n\n\n\nByte = 8 bits\n\n\n\n\n\nimage-20211027043303747\n\n\n\n\n\n\nAlgebraic representation of logic\n\nEncode true as 1 and false as 0\n\n\n\n\nimage-20211027043347856\n\n\n\n\nOperate on bit vectors\n\nOperations applied bitwise\n\n\n\n\nimage-20211027043430808\n\n\nAll of the properties of boolean algebra apply\n\n\n\nimage-20211027043452466\n\n\n\n\n\nimage-20211027044238795\n\n\n\n\n\n\n\n\n\nimage-20211027044259735\n\n\n\n\n\nimage-20211027044309599\n\n\n\n\n\n\n\n\nimage-20211027044328800\n\n\n\n\n\nimage-20211027044336152\n\n\n\n\n\n\n\n\nimage-20211027044421381\n\n\n\n\n\n\n\n\n\n\nimage-20211027044701071\n\n\n\n\n\nimage-20211027044716057\n\n\n\n\n\nimage-20211027044757931\n\n\n\n\n\n\n\n\nimage-20211027044826093\n\n\n\n\n\n\n\n\nimage-20211027044913286\n\n\n\n\n\nEquivalence\n\nsame encodings for nonnegative values\n\nUniqueness\n\nEvery bit pattern represents unique integer value\nEach representable integer has unique bit encoding\n\nCan inverting mappings\n\n\n\nimage-20211027045039055\n\n\n\n\n\nimage-20211027045044999\n\n\n\n\n\n\n\n\n\nimage-20211027045107523\n\n\nMappings between unsigned and two’s complement numbers: Keep bit representations and reinterpret\n\n\n\nimage-20211027045221125\n\n\n\n\n\nimage-20211027045230990\n\n\n\n\n\n\n\nimage-20211027045250508\n\n\n\n\n\nimage-20211027045308052\n\n\n\n\n\nConstants\n\nBy default are considered to be signed integers\nUnsigned if have “U” as suffix\n\n\nCasting\n\nExplicit casting between signed & unsigned same as U2T and T2U\n\n\n\n\nimage-20211027045359829\n\n\n\nImplicit casting also occurs via assignment and procedure calls\n\n\n\n\nimage-20211027045429660\n\n\n\n\n\nExpression evaluation\n\nIf there is a mix of unsigned and signed in single expression, signed values implicitly cast to unsigned\nIncluding comparison operations\n\n\n\n\nimage-20211027045512058\n\n\n\n\n\n\nBit pattern is maintained\nbut reinterpreted\ncan have unexpected effects: adding or subtracting \\(2^{w}\\)\nExpression containing signed and unsigned int\n\nint is cast to unsigned!!\n\n\n\n\n\nTask\n\nGiven w-bit signed integer x\nConvert it to w + k-bit integer with same value\n\n\n\n\nimage-20211027045810403\n\n\n\n\n\nimage-20211027045831561\n\n\n\nConverting from smaller to larger integer data type\nC automatically performs sign extension\n\n\n\n\nUnsigned: zeros added\nSigned: sign extension\nBoth yield expected result\n\n\n\n\n\nUnsigned / signed: bits are truncated\nResult reinterpreted\nUnsigned: mod operation\nSigned: similar to mod\nFor small numbers yield expected behavior\n\n\n\n\n\n\n\n\nimage-20211027050155034\n\n\nStandard addition function\n\nignores carry output\n\nImplementations modular arithmetic\n\n\n\nimage-20211027050240445\n\n\n\n\n\nimage-20211027050335775\n\n\n\n\n\nimage-20211027050350882\n\n\n\n\n\n\n\n\nimage-20211027050417014\n\n\n\n\n\nimage-20211027050425030\n\n\n\n\n\nimage-20211027050435241\n\n\n\n\n\nimage-20211027050459580\n\n\n\n\n\n\n\n\nimage-20211027050529807\n\n\nMaintaining exact results\n\nwould need to keep expanding word size with each product computed\nis done in software, if needed\n\n\n\n\nimage-20211027050615217\n\n\n\n\n\nimage-20211027050623133\n\n\n\n\n\nimage-20211027050636927\n\n\n\n\n\nimage-20211027050645206\n\n\n\n\n\n\n\n\nimage-20211027050706187\n\n\n\n\n\nimage-20211027050710469\n\n\n\n\n\nimage-20211027050743532\n\n\n\n\n\nimage-20211027050810877\n\n\n\n\n\n\n\n\nUnsigned / signed: normal addition followed by truncate, same operation on bit level\nUnsigned - addition mode \\(2^{w}\\)\n\nMathematical addition + possible subtraction of \\(2^{w}\\)\n\nSigned - modified addition mode \\(2^{w}\\) (result in proper range)\n\nMathematical addition + possible addition or subtraction of \\(2^{w}\\)\n\n\n\n\n\n\nUnsigned / signed: normal multiplication followed by truncate, same operation on bit level\nUnsigned - multiplication mod \\(2^{w}\\)\nSigned - modified multiplication mod \\(2^{w}\\) (result in proper range)\n\n\n\n\n\n\n\n\nimage-20211027051210460\n\n\n\n\n\nimage-20211027051241526\n\n\n\n\n\nimage-20211027051258484\n\n\n\n\n\n\n\n\n\n\n\nimage-20211027051332281\n\n\nPrograms refer to data by address\n\nConceptually, envision it as a very large array of bytes\nAn address is like an index into that array\n\nand, a pointer variable stores an address\n\n\nSystem provides private address spaces to each ‘process’\n\nThink of a process as a program being executed\nSo, a program can clobber its own data, but not that of others\n\n\n\n\nAny given computer has a ‘word size’\n\nNominal size of integer-valued data and of addresses\nUntil recently, most machines used 32 bits (4 bytes) as word size\n\nLimits addresses to 4GB\n\nIncreasingly, machines have 64-bit word size\n\nPotentially, could have 18EB (exabytes) of addressable memory\n\nMachines still support multiple data formats\n\nFractions or multiples of word size\nAlways integral number of bytes\n\n\n\n\n\n\n\n\nimage-20211027065818406\n\n\nAddresses specify byte locations\n\nAddress of first byte in word\nAddresses of successive words differ by 4 (32-bit) or 8 (64-bit)\n\n\n\n\nimage-20211027065857259\n\n\n\n\n\nConventions\n\n\n\nimage-20211027065913492\n\n\n\n\n\nimage-20211027065921688\n\n\n\n\n\nimage-20211027065953283\n\n\n\n\n\nimage-20211027070043111\n\n\n\n\n\nimage-20211027070050642\n\n\n\n\n\nimage-20211027070102449\n\n\n\n\n\nStrings in C\n\nRepresented by array of characters\nEach character encoded in ASCII format\n\nStandard 7-bit encoding of character set\n\nString should be null-terminated\n\nFinal character = 0\n\n\nCompatibility\n\nByte ordering not an issue\n\n\n\n\nimage-20211027070247413\n\n\n\n\n\n\n\n\n\n\n\nimage-20211027070611400\n\n\n\n\n\nimage-20211027070656701\n\n\n\n\n\n\n\n\nimage-20211027070714526\n\n\n\n\n\nIEEE Standard 754\nDriven by numerical concerns\n\nNice standards for rounding, overflow, underflow\nHard to make fast in hardware\n\nNumerical analysts predominated over hardware designers in defining standard\n\n\n\n\n\n\n\n\nimage-20211027070845517\n\n\n\n\n\nimage-20211027070927661\n\n\n\n\n\nimage-20211027070937032\n\n\n\n\n\n\n\n\nimage-20211027071101884\n\n\n\n\n\nimage-20211027072122233\n\n\nbias = 127\n127을 기준으로 0과 255만 제외하고, 소수점 1칸을 움직이고 싶으면 Exp = 128, E = 128 - 172 = 1로 계산\n\n\n\n\n\n\nimage-20211027072608278\n\n\n\n\n\n\n\n\nimage-20211027072733402\n\n\n\n\n\nimage-20211027072953531\n\n\n\n\n\n\n\n\nimage-20211027073027716\n\n\n\n\n\nimage-20211027073048133\n\n\n\n\n\nFP zero same as Integer zero\n\nall bits = 0\n\nCan (almost) use unsigned integer comparison\n\nMust first compare sign bits\nMust consider -0 = 0\nNaNs problematic\n\nWill be greater than any other values\nWhat should comparison yield?\n\nOtherwise OK\n\nDenorm vs. normalized\nNormalized vs. infinity\n\n\n\n\n\n\n\n\nimage-20211027073309010\n\n\nBasic idea\n\nFirst compute exact result\nMake it fit into desired precision\n\nPossibly overflow if exponent too large\nPossibly round to fit into frac\n\n\n\n\n\n\n\n\nimage-20211027073355639\n\n\n\n\n\nDefault rounding mode\n\nhard to get any other kind without dropping into assembly\nAll other are statistically biased\n\nSum of set of positive numbers will consistently be over- or under-estimated\n\n\n\n\n\nimage-20211027073540219\n\n\n\n\n\n\n\n\nimage-20211027073643032\n\n\n\n\n\n\n\n\nimage-20211027073655642\n\n\nFixing\n\nIf \\(M \\geq 2\\), shift M right, increment E\nIf E out of range, overflow\nRound M to fit frac precision\n\nImplementation\n\nBiggest chore is multiplying significands\n\n\n\n\n\n\n\nimage-20211027073824957\n\n\n\n\n\nimage-20211027073911945\n\n\n\n\n\n\n\n\nimage-20211027073929110\n\n\n\n\n\n\n\n\nimage-20211027074040391\n\n\n\n\n\n\n\n\nimage-20211027074127882\n\n\n\n\n\nfloat - single precision\ndouble - double precision\nConversions / casting\n\nCasting between int, float, and double changes bit representation.\ndouble / float -> int\n\nTruncates fractional part\nlike rounding toward zero\nnot defined when out of range or NaN\n\nint -> double\n\nexact conversion, as long as int has \\(\\leq 53\\) bit word size\n\nint -> float\n\nwill round according to rounding mode\n\n\n\n\n\n\n\n\nimage-20211027074403219\n\n\n\n\n\n\n\n\nimage-20211027074710910\n\n\n\n\n\n\nComplex instruction set computer (CISC)\n\nMany different instructions with many different formats\n\nbut, only small subset encountered with Linux programs\n\nHard to match performance of Reduced Instruction Set Computers (RISC)\nIn terms of speed. Less so for low power.\n\nAdded features\n\nInstructions to support multimedia operations\nInstructions to enable more efficient conditional operations\nTransition from 32 bits to 64 bits\nMore cores\n\n\n\n\n\na little bit slow, a lot cheaper\nDeveloped x86-64, their own extension to 64 bits\n\n\n\n\nArchitecture - (also ISA: Instruction Set Architecture) The parts of a processor design that one needs to understand or write assembly / machine code.\nMicroarchitecture - Implementation of the architecture\nCode forms\n\nMachine code - The byte-level programs that a processor executes\nAssembly code - A text representation of machine code\n\n\n\n\n\n\n\nimage-20211027075257754\n\n\nProgrammer-visible state\n\nPC - Program counter\n\naddress of next instruction\ncalled ‘RIP’\n\nRegister file\n\nheavily used program data\n\nCondition codes\n\nstore status information about most recent arithmetic or logical operation\nused for conditional branching\n\n\nMemory\n\nByte addressable array\nCode and user data\nStack to support procedures\n\n\n\n\n\n\n\nimage-20211027075420009\n\n\n\n\n\n\n\nimage-20211027075451716\n\n\n\n\n\n\n‘Integer’ data of 1, 2, 4, or 8 bytes\n\nData values\nAddresses (untyped pointers)\n\nFloating point data of 4, 8, or 10 bytes\nCode - byte sequences encoding series of instructions\nNo aggregate types such as arrays or structures\n\njust contiguously allocated bytes in memory\n\n\n\n\nperform arithmetic function on register or memory data\nTransfer data between memory and register\n\nLoad data from memory into register\nStore register data into memory\n\nTransfer control\n\nUnconditional jumps to / from procedures\nconditional branches\n\n\n\n\n\n\n\nimage-20211027075819838\n\n\n\n\n\n\n\n\nimage-20211027075907340\n\n\n\n\n\n\n\n\nimage-20211027075952304\n\n\nDisassembler\nobjdump -d sum\n\nuseful tool for examining object code\nAnalyzes bit pattern of series of instructions\nProduces approximate rendition of assembly code\nCan be run on either a.out (complete executable) or .o file\n\n\n\n\n\n\nimage-20211027080055315\n\n\nAnything that can be interpreted as executable code\nDisassembler examines bytes and reconstructs assembly source\n\n\n\n\n\n\n\nimage-20211027080134896\n\n\n\n\n\n\n\nimage-20211027080157590\n\n\n\n\n\n\nmovq Source, Dest\n\n\n\nimage-20211027080222603\n\n\n\n\n\n\n\nimage-20211027080309631\n\n\n\n\n\n\n\n\n\nimage-20211027080353954\n\n\n\n\n\nimage-20211027080408919\n\n\n\n\n\nimage-20211027080425212\n\n\n\n\n\nimage-20211027080602119\n\n\n\n\n\nimage-20211027080610058\n\n\n\n\n\nimage-20211027080617652\n\n\n\n\n\nimage-20211027080625400\n\n\n\n\n\nimage-20211027080630845\n\n\n\n\n\n\n\n\nimage-20211027080653122\n\n\n\n\n\nimage-20211027094428952\n\n\n\n\n\nleaq Src, Dst\n\nSrc is address mode expression\nSet Dst to address denoted by expression\n\nUse\n\nComputing addresses without a memory reference\nComputing arithmetic expressions of the from \\(x + k * y\\)\n\n\n\n\nimage-20211027094541160\n\n\n\n\n\n\n\n\nimage-20211027094629212\n\n\n\n\n\nimage-20211027094634269\n\n\n\n\n\nimage-20211027094704377\n\n\n\n\n\nimage-20211027094735037\n\n\n\n\n\n\n\n\nimage-20211027094916448\n\n\n\n\n\n\n\n\nimage-20211027094954382\n\n\n\n\n\nimage-20211027095002281\n\n\nNot set by leaq instruction\n\n\n\n\n\nimage-20211027095056556\n\n\n\n\n\n\n\n\nimage-20211027095130320\n\n\n\n\n\n\nSetX Instructions\n\nSet low-order bytes of destination to 0 or 1 based on combinations of condition codes\nDoes not alter remaining 7 bytes\n\n\n\n\nimage-20211027095226154\n\n\n\n\n\nimage-20211027095236908\n\n\nSetX Instructions - set single byte based on combination of condition codes\nOne of addressable byte registers\n\nDoes not alter remaining bytes\nTypically use movzbl to finish job\n\n32-bit instructions also set upper 32 bits to 0\n\n\n\n\n\nimage-20211027095339037\n\n\n\n\n\njX instructions - Jump to different part of code depending on condition codes\n\n\n\nimage-20211027095421739\n\n\n\n\n\n\n\n\nimage-20211027095444228\n\n\n\n\n\n\n\n\nimage-20211027100123443\n\n\n\n\n\n\n\n\nimage-20211027100213983\n\n\n\n\n\n\n\n\nimage-20211027100234307\n\n\nConditional move instructions\n\nInstruction supports: if (Test) Dest <- Src\nSupported in post-1995 x86 processors\nGCC tries to use them\n\nBut, only when known to be safe\n\n\nWhy?\n\nBranches are very disruptive instruction flow through pipelines\nConditional moves do not require control transfer\n\n\n\n\n\n\n\nimage-20211027100428875\n\n\n\n\n\n\n\nval  = Test(x) ? Hardl(x) : Hard2(x);\n\nboth values get computed\nOnly makes sense when computations are very simple\n\n\n\n\nval = p ? *p : 0;\n\nBoth values get computed\nMay have undesirable effects\n\n\n\n\nval = x > 0 ? x *= 7 : x += 3\n\nBoth values get computed\nMust be side-effect free\n\n\n\n\n\n\n\n\nimage-20211027100719400\n\n\n\n\n\n\n\n\nimage-20211027100753177\n\n\n\n\n\n\n\n\nimage-20211027100829385\n\n\n\n\n\n\n\nimage-20211027100846135\n\n\n\n\n\n\n\n\n\nimage-20211027100917193\n\n\n\n\n\nimage-20211027100933135\n\n\n\n\n\nimage-20211027100942738\n\n\n\n\n\n\n\n\nimage-20211027101053616\n\n\n\n\n\n\n\n\nimage-20211027101110082\n\n\n\n\n\n\n\n\nimage-20211027101123580\n\n\n\n\n\n\n\n\nimage-20211027101136420\n\n\n\n\n\n\n\n\nimage-20211027101209574\n\n\n\n\n\n\n\n\nimage-20211027101222818\n\n\n\n\n\n\n\n\nimage-20211027101241191\n\n\n\n\n\nimage-20211027101324338\n\n\n\n\n\nTable structure\n\neach target requires 8 bytes\nbase address at .L4\n\nJumping\n\nDirect: jmp .L8\nJump target is denoted by label .L8\nIndirect: jmp *.L4(, %rdi, 8)\nStart of jump table: .L4\nMust scale by factor of 8 (addresses are 8 bytes)\nFetch target from effective address .L4 + x * 8\n\nonly for \\(0 \\leq x \\leq 6\\)\n\n\n\n\n\n\n\n\nimage-20211027101557124\n\n\n\n\n\n\n\nimage-20211027101656175\n\n\n\n\n\n\n\n\nimage-20211027101728790\n\n\n\n\n\n\n\n\nimage-20211027101744257\n\n\n\n\n\n\n\n\nimage-20211027101831338\n\n\n\n\n\n\npassing control\n\nTo beginning of procedure code\nback to return point\n\nPassing data\n\nProcedure arguments\nreturn value\n\nMemory management\n\nallocate during procedure execution\ndeallocate upon return\n\nMechanisms all implemented with machine instructions\nx86-64 implementation of a procedure uses only those mechanisms required\n\n\n\n\n\n\nimage-20211027103417450\n\n\nregion of memory managed with stack discipline\ngrows toward lower addresses\nregister %rsp constains lowest stack address\n\naddress of ‘top’ element\n\n\n\npushq src\n\nFetch operand at src\nDecrement %rsp by 8\nWrite operand address given by %rsp\n\n\n\n\nimage-20211027103538821\n\n\n\n\n\npopq dest\n\nRead value at address given by %rsp\nincrement %rsp by 8\nstore value at dest (must be register)\n\n\n\n\nimage-20211027103627936\n\n\n\n\n\n\n\n\n\nimage-20211027103644808\n\n\n\n\n\nuse stack to support procedure call and return\nprocedure call: call label\n\npush return address on stack\njump to label\n\nreturn address:\n\naddress of the next instruction right after call\nexample from disassembly\n\nProcedure return: ret\n\npop address from stack\njump to address\n\n\n\n\nimage-20211027103755978\n\n\n\n\n\nimage-20211027103818672\n\n\n\n\n\nimage-20211027103905789\n\n\n\n\n\nimage-20211027103916325\n\n\n\n\n\n\n\n\nimage-20211027103941719\n\n\n\n\n\n\n\nimage-20211027104001697\n\n\n\n\n\n\nlanguages that support recursion\n\ncode must be ‘reentrant’\n\nmultiple simultaneous instantiations of single procedure\n\nneed some place to store state of each instantiation\n\narguments\nlocal variables\nreturn pointer\n\n\nstack discipline\n\nstate for given procedure needed for limited time\n\nfrom when called to when return\n\ncallee returns before caller does\n\nstack allocated in frames\n\nstate for single procedure instantiation\n\n\n\n\n\n\n\nimage-20211027104207381\n\n\n\n\n\ncontents\n\nreturn information\nlocal storage (if needed)\ntemporary space (if needed)\n\nmanagement\n\nspace allocated when enter procedure\n\nset-up code\nincludes push by call instruction\n\ndeallocated when return\n\nfinish code\nincludes pop by ret instruction\n\n\n\n\n\nimage-20211027104310318\n\n\n\n\n\n\n\nimage-20211027104326446\n\n\n\n\n\nimage-20211027104338914\n\n\n\n\n\nimage-20211027104347201\n\n\n\n\n\nimage-20211027104353955\n\n\n\n\n\nimage-20211027104402833\n\n\n\n\n\nimage-20211027104411997\n\n\n\n\n\nimage-20211027104418160\n\n\n\n\n\nimage-20211027104426472\n\n\n\n\n\nimage-20211027104433206\n\n\n\n\n\nimage-20211027104439712\n\n\n\n\n\nimage-20211027104445710\n\n\n\n\n\n\n\n\n\nimage-20211027104501895\n\n\nCurrent stack frame (‘top’ to bottom)\n\n‘argument build’: parameters for function about to call\nlocal variables if can’t keep in registers\nsaved register context\nold frame pointer (optional)\n\nCaller stack frame\n\nreturn address\n\npushed by call instruction\n\narguments for this call\n\n\n\n\n\n\n\nimage-20211027104623434\n\n\n\n\n\nimage-20211027110105442\n\n\n\n\n\nimage-20211027110122509\n\n\n\n\n\nimage-20211027110132835\n\n\n\n\n\nimage-20211027110152468\n\n\n\n\n\nimage-20211027110204358\n\n\n\n\n\nwhen procedure yoo calls who:\n\nyoo is the caller\nwho is the callee\n\nCan register be used for temporary storage?\n\n\n\nimage-20211027110246294\n\n\nConventions\n\n‘caller saved’\n\ncaller saves temporary values in its frame before the call\n\n‘callee saved’\n\ncallee saved temporary values in its frame before using\ncallee restores them before returning to caller\n\n\n\n\n\n\n\n\nimage-20211027110355861\n\n\n\n\n\nimage-20211027110528349\n\n\n\n\n\n\n\n\nimage-20211027110556071\n\n\n\n\n\nimage-20211027110607046\n\n\n\n\n\n\n\n\nimage-20211027110622761\n\n\n\n\n\nimage-20211027110734013\n\n\n\n\n\nimage-20211027110745359\n\n\n\n\n\nimage-20211027110755966\n\n\n\n\n\nimage-20211027110804783\n\n\n\n\n\nimage-20211027110812682\n\n\n\n\n\nimage-20211027110825578\n\n\n\n\n\nHandled without special consideration\n\nstack frames mean that each function call has private storage\n\nsaved registers & local variables\nsaved return pointer\n\nRegister saving conventions prevent one function call from corrupting another’s data\n\nUnless the C code explicitly does so - buffer overflow\n\nStack discipline follows call / return pattern\n\nIf P calls Q, then Q returns before P\nLast-In, First-Out\n\nAlso works for mutual recursion\n\nP calls Q; Q calls P\n\n\n\n\n\n\n\n\nimage-20211027111057784\n\n\n\n\n\nBasic principle\nT A[L];\n\narray of data type T and length L\ncontiguously allocated region of L * sizeof(T) bytes in memory\n\n\n\n\nimage-20211027111236807\n\n\n\n\n\n\n\n\nimage-20211027111258042\n\n\n\n\n\n\n\n\nimage-20211027111335888\n\n\n\n\n\nimage-20211027111354692\n\n\n\n\n\nimage-20211027111434013\n\n\n\n\n\n\n\n\nimage-20211027111455485\n\n\n\n\n\n\n\nimage-20211027111514213\n\n\n\n\n\nimage-20211027111536834\n\n\n\n\n\nimage-20211027111549363\n\n\n\n\n\nimage-20211027111731205\n\n\n\n\n\nimage-20211027111806092\n\n\n\n\n\nimage-20211027111924809\n\n\n\n\n\nimage-20211027111945470\n\n\n\n\n\nimage-20211027112047282\n\n\n\n\n\n\n\n\n\nimage-20211027112137346\n\n\n\n\n\n\n\nimage-20211027113556133\n\n\n\n\n\n\n\n\nimage-20211027113609441\n\n\n\n\n\n\n\n\n\nimage-20211027113624871\n\n\nStructure represented as block of memory\n\nbig enough to hold all of the fields\n\nFields ordered according to declaration\n\nEven if another ordering could yield a more compact representation\n\nCompiler determines over size + positions of fields\n\nMachine-level program has no understanding of the structures in the source code\n\n\n\n\n\n\n\nimage-20211027113744195\n\n\n\n\n\n\n\n\nimage-20211027113801438\n\n\n\n\n\n\n\n\nimage-20211027113814816\n\n\n\n\n\nAligned data\n\nprimitive data type requires K bytes\nAddres must be multiple of k\nrequired on some machines; advised on x86-64\n\nMotivtaion for aligning data\n\nmemory accessed by (alinged) chunks of 4 or 8 bytes (system dependent)\n\ninefficient to load or store datum that spans quad word boundaries\nvirtual memory trickier when datum spans 2 pages\n\n\nCompiler\n\ninserts gaps in structure to ensure correct alignment of fields\n\n\n\n\n\n\n\nimage-20211027114001119\n\n\n\n\n\n\n\n\nimage-20211027114045668\n\n\n\n\n\n\n\n\nimage-20211027114106542\n\n\n\n\n\n\n\n\nimage-20211027114123050\n\n\n\n\n\n\n\n\nimage-20211027114141994\n\n\n\n\n\n\n\n\nimage-20211027114318223\n\n\n\n\n\n\n\n\nimage-20211027114335708\n\n\n\n\n\n\n\nimage-20211027114406947\n\n\n\n\n\nimage-20211027114414171\n\n\n\n\n\n\n\n\n\nimage-20211027114458228\n\n\n\n\n\nimage-20211027114508272\n\n\n\n\n\ngenerally called a ‘buffer overflow’\n\nWhen exceeding the memory size allocated for an array\n\nWhy a big deal?\n\nIt’s the #1 technical cause of security vulnerabilities\n\n#1 overall cause is social enginerring / user ignorance\n\n\nMost common form\n\nUnchecked lengths on string inputs\nParticularly for bounded character arrays on the stack\n\nsometimes referred to as stack smashing\n\n\n\n\n\n\n\n\nimage-20211027114642824\n\n\n\n\n\n\n\n\nimage-20211027114659698\n\n\n\n\n\n\n\n\nimage-20211027114714186\n\n\n\n\n\n\n\n\nimage-20211027114727477\n\n\n\n\n\nimage-20211027114738072\n\n\n\n\n\nimage-20211027114749245\n\n\n\n\n\nimage-20211027114800681\n\n\n\n\n\nimage-20211027114815004\n\n\n\n\n\nimage-20211027115817640\n\n\n\n\n\n\n\n\nimage-20211027115849482\n\n\n\n\n\nBuffer overflow bug can allow remote machines to execute arbitrary code on victim machines\nDistressingly common in real programs\n\n\n\n\navoid overflow vulnerabilities\nemploy system-level protections\nhave compiler use ‘stack canaries’\n\n\n\n\n\n\nimage-20211027120039759\n\n\n\n\n\nimage-20211027120046526\n\n\n\n\n\n\n\n\nimage-20211027120103535\n\n\n\n\n\nimage-20211027120129353\n\n\n\n\n\n\n\n\n\nimage-20211027120251913\n\n\n\n\n\n\n\n\nimage-20211027120327882\n\n\n\n\n\n\n\n\nimage-20211027120342746\n\n\n\n\n\n\n\n\nimage-20211027120357226\n\n\n\n\n\nChallenge (for hackers)\n\nstack randomization makes it hard to predict buffer location\nmarking stack nonexecutable makes it hard to insert binary code\n\nAlternative strategy\n\nuse existing code\nstring together fragments to achieve oeverall desired outcome\ndoes not overcome stack canaries\n\nConstruct program from gadgets\n\nsequence of instructions ending in ret\n\nEncoded by single byte 0xc3\n\ncode positions fixed from run to run\ncode is executable\n\n\n\n\n\n\nimage-20211027120609710\n\n\n\n\n\nimage-20211027120623663\n\n\n\n\n\n\n\n\n\nimage-20211027120726700"
  },
  {
    "objectID": "posts/2021-12-09-algorithms-final-summary/2021-12-09-algorithms-final-summary.html",
    "href": "posts/2021-12-09-algorithms-final-summary/2021-12-09-algorithms-final-summary.html",
    "title": "Algorithms Final",
    "section": "",
    "text": "image-20211209000613505\n\n\n\n\n\nimage-20211209000621212\n\n\n\n\n\n\n\n\nimage-20211209000632595\n\n\n\n\n\n\n\n\n\n\nimage-20211209000717087\n\n\n\n\n\n\n\n\n\nimage-20211209000831977\n\n\n\n\n\n\n\nimage-20211209000759261\n\n\n\n\n\nimage-20211209000811117\n\n\ndirected graph with no directed cycles\n\n\n\n\n\n\nimage-20211209000845819\n\n\n\n\n\nimage-20211209000857699\n\n\n\n\n\nimage-20211209000925934\n\n\n\n\n\nimage-20211209000931435\n\n\n\n\n\nimage-20211209000948588\n\n\n\n\n\n\n\n\n\nimage-20211209001004783\n\n\n\n\n\n\n\nimage-20211209001215828\n\n\n\n\n\n\n\n\nimage-20211209001229430\n\n\n\n\n\n\n\n\n\nimage-20211209001309197\n\n\n\n\n\n\n\n\nimage-20211209000434398\n\n\n\n\n\nimage-20211209001335284\n\n\n\n\n\n\n\nimage-20211209002512642\n\n\n\n\n\nimage-20211209002004336\n\n\n\n\n\nimage-20211209002130087\n\n\n\n\n\nimage-20211209002249578\n\n\n\n\n\nimage-20211209002331356\n\n\n\n\n\nimage-20211209002352183\n\n\n\n\n\nimage-20211209002447072\n\n\n\n\n\nimage-20211209002536085\n\n\n\n\n\n\n\n\nimage-20211209002549675\n\n\n\n\n\nimage-20211209002803483\n\n\n\n\n\nimage-20211209002809127\n\n\n\n\n\n\n\n\nimage-20211209002821345\n\n\n\n\n\n\n\n\nimage-20211209005014090\n\n\n\n\n\n\n\n\n\nimage-20211209005335064\n\n\n\n\n\nimage-20211209005359661\n\n\n\n\n\nimage-20211209005413194\n\n\n\n\n\nimage-20211209005549151\n\n\n\n\n\n\n\nimage-20211209005614527\n\n\n\n\n\n\n\n\nimage-20211209005635278\n\n\n\n\n\nimage-20211209005849648\n\n\n\n\n\nimage-20211209005857800\n\n\n\n\n\nimage-20211209013121078\n\n\n\n\n\n\n\n\n\nimage-20211209013207753\n\n\n\n\n\n\n\n\nimage-20211209013510782\n\n\n\n\n\n\n\nimage-20211209013541321\n\n\n\n\n\nimage-20211209013616339\n\n\n\n\n\nimage-20211209013628300\n\n\n\n\n\n\n\n\n\nimage-20211209013652718\n\n\n\n\n\nimage-20211209013707973\n\n\n\n\n\n\n\n\nimage-20211209013719208\n\n\n\n\n\n\n\n\nimage-20211209013837247\n\n\n\n\n\nimage-20211209013845254\n\n\n\n\n\n\n\nimage-20211209013902576\n\n\n\n\n\n\n\n\nimage-20211209014219712\n\n\n\n\n\nimage-20211209014227969\n\n\n\n\n\nimage-20211209014233898\n\n\n\n\n\nimage-20211209014322974\n\n\n\n\n\nimage-20211209014330876\n\n\n\n\n\n\n\n\n\nimage-20211209014349302\n\n\n\n\n\nimage-20211209014401446\n\n\n\n\n\n\n\n\nimage-20211209014439149\n\n\n\n\n\n\n\n\nimage-20211209014543078\n\n\n\n\n\n\n\n\nimage-20211209014616466\n\n\n\n\n\nimage-20211209014623288\n\n\n\n\n\nimage-20211209014633642\n\n\n\n\n\n\n\n\nimage-20211209014654548\n\n\n\n\n\n\n\n\nimage-20211209014724644\n\n\n\n\n\n\n\n\nimage-20211209014817181\n\n\n\n\n\nimage-20211209014843751\n\n\n\n\n\nimage-20211209014900506\n\n\n\n\n\n\n\n\nimage-20211209015015942\n\n\n\n\n\nimage-20211209015039116\n\n\n\n\n\n\n\n\nimage-20211209015112353\n\n\n\n\n\nimage-20211209015120089\n\n\n\n\n\n\n\n\nimage-20211209015202864\n\n\n\n\n\n\n\n\nimage-20211209015228375\n\n\n\n\n\n\n\n\nimage-20211209015846410\n\n\n\n\n\nimage-20211209015856760\n\n\n\n\n\n\n\n\nimage-20211209015937256\n\n\n\n\n\n\n\n\nimage-20211209020001333\n\n\n\n\n\n\n\n\nimage-20211209020034357\n\n\n\n\n\nimage-20211209020044601\n\n\n\n\n\nimage-20211209020056804\n\n\n\n\n\nimage-20211209020124989\n\n\n\n\n\n\n\n\nimage-20211209020149430\n\n\n\n\n\n\n\n\nimage-20211209020204533\n\n\n\n\n\nimage-20211209020213417\n\n\n\n\n\nimage-20211209020228603\n\n\n\n\n\nimage-20211209020241679\n\n\n\n\n\nimage-20211209020249164\n\n\n\n\n\nimage-20211209020259097\n\n\n\n\n\nimage-20211209020655701\n\n\n\n\n\n\n\n\nimage-20211209020722384\n\n\n\n\n\n\n\n\nimage-20211209020752779\n\n\n\n\n\nimage-20211209020817580\n\n\n\n\n\n\n\n\nimage-20211209020849652\n\n\n\n\n\n\n\n\nimage-20211209020920665\n\n\n\n\n\nso we are unlikely to find an efficient algorithm.\n\n\n\n\n\n\nimage-20211209021025874\n\n\n\n\n\n\n\nimage-20211209021047954\n\n\n\n\n\n\n\n\nimage-20211209021103251\n\n\n\n\n\n\n\n\n\nimage-20211209021123134\n\n\n\n\n\nimage-20211209021139075\n\n\n\n\n\n\n\n\nimage-20211209021201306\n\n\n\n\n\n\n\n\nimage-20211209021324373\n\n\n\n\n\n\n\nimage-20211209021345116\n\n\n\n\n\n\n\n\n\nimage-20211209021408630\n\n\n\n\n\nimage-20211209021414684\n\n\n\n\n\nimage-20211209021421917\n\n\n\n\n\n\n\nimage-20211209021433300\n\n\n\n\n\n\n\n\n\nimage-20211209021444945\n\n\n\n\n\n\n\n\nimage-20211209021457797\n\n\n\n\n\nimage-20211209021521186\n\n\n\n\n\n\n\n\nimage-20211209021656538\n\n\n\n\n\nimage-20211209021702523\n\n\n\n\n\nimage-20211209021707642\n\n\n\n\n\nimage-20211209021715120\n\n\n\n\n\n\n\n\nimage-20211209021950587\n\n\n\n\n\nimage-20211209022023177\n\n\n\n\n\n\n\n\nimage-20211209022115678\n\n\n\n\n\n\n\n\nimage-20211209022141925\n\n\n\n\n\n\n\n\nimage-20211209022219290\n\n\n\n\n\nimage-20211209022230484\n\n\n\n\n\nimage-20211209022237293\n\n\n\n\n\nimage-20211209022256998\n\n\n\n\n\n\n\n\nimage-20211209022318333\n\n\n\n\n\n\n\n\nimage-20211209022503811\n\n\n\n\n\nimage-20211209022514576\n\n\n\n\n\n\n\n\nimage-20211209022530970\n\n\n\n\n\n\n\n\nimage-20211209022658129\n\n\n\n\n\n\n\n\nimage-20211209022712564\n\n\n\n\n\nimage-20211209022725181\n\n\n\n\n\n\n\n\nimage-20211209022736552\n\n\n\n\n\n\n\n\nimage-20211209022753608\n\n\n\n\n\n\n\n\nimage-20211209022808803\n\n\n\n\n\n\n\n\nimage-20211209022821913\n\n\n\n\n\nimage-20211209022836427\n\n\n\n\n\n\n\n\nimage-20211209022913587\n\n\n\n\n\nimage-20211209022932710\n\n\n\n\n\n\n\n\nimage-20211209022959233\n\n\n\n\n\n\n\n\nimage-20211209023036923\n\n\n\n\n\n\n\n\nimage-20211209023052857\n\n\n\n\n\nimage-20211209023101335\n\n\n\n\n\n\n\n\nimage-20211209023119201\n\n\n\n\n\nimage-20211209023131816\n\n\n\n\n\n\n\n\nimage-20211209023211015\n\n\n\n\n\nimage-20211209023219814\n\n\n\n\n\n\n\n\nimage-20211209023254097\n\n\n\n\n\nimage-20211209023301368\n\n\n\n\n\n\n\n\nimage-20211209023325167\n\n\n\n\n\n\n\n\nimage-20211209023337145\n\n\n\n\n\n\n\n\nimage-20211209023353343\n\n\n\n\n\nimage-20211209023400463\n\n\n\n\n\n\n\n\n\n\nimage-20211209023425272\n\n\n\n\n\nimage-20211209023432422\n\n\n\n\n\nimage-20211209023440986\n\n\n\n\n\n\n\n\n\nimage-20211209023501263"
  },
  {
    "objectID": "posts/2021-11-06-digital-system-circuits-week-10/2021-11-06-digital-system-circuits-week-10.html",
    "href": "posts/2021-11-06-digital-system-circuits-week-10/2021-11-06-digital-system-circuits-week-10.html",
    "title": "Digital System Circuits Week 10",
    "section": "",
    "text": "image-20211106054228740\n\n\n메모리는 왜 필요한가? - 메모리 없이는 구현 할 수 없는 시스템이 있다.\n\n\n\n\n\n\nimage-20211106055656215\n\n\nRace condition (S, R이 둘 다 1일 때) condition이 latch로 바뀌면 latch에 모순이 생기고 오실레이터가 됨\nt, q가 둘 다 타이밍을 맞춰서 바뀌면서 진동하다가, delay 차이로 인해 서로가 다른 값을 갖게 되는 순간 안정화로 돌입한다.\n\n\n\nimage-20211106060923949\n\n\n\n\n\nimage-20211106060931716\n\n\n항상 reset이 set보다 늦게 결정되는 문제(glitch)가 발생한다.\n같은 신호인데 다른 delay를 가지면 많은 문제가 발생한다.\n\n\n\n\n\n\nimage-20211106060951190\n\n\nclock signal을 도입\n기존에 NOT 회로를 추가해서 구현하는 방식은 논리회로로 방법을 찾은 것이었음. 현재 피하고 싶은 것은 race condition. 그런데 이 방식에서는 delay로 인해 glitch가 발생한다.\nc가 1이면 1, 0이면 0이 되는 구조 -> clock을 설정하고 SR의 값을 바꾸면 glitch가 예방됨\nstabilized된 다음에 신호를 통과시키면 된다.\n\n\n\n\n\n\nimage-20211106062458526\n\n\n\n\n\n\n\n\nimage-20211106091616438\n\n\n\n\n\n\n\n\nimage-20211106091956789\n\n\n\n\n\n\n\n\nimage-20211106092935868\n\n\n\n\n\n\n\n\nimage-20211106093019250"
  },
  {
    "objectID": "posts/2021-11-05-algorithms-week-9/2021-11-05-algorithms-week-9.html",
    "href": "posts/2021-11-05-algorithms-week-9/2021-11-05-algorithms-week-9.html",
    "title": "Algorithms Week 9",
    "section": "",
    "text": "위상정렬\n\n\n\n\n\nimage-20211105053608856\n\n\n\n\n\nimage-20211105053831906\n\n\ndependencies를 고려해서 올바른 순서를 찾아주는 sorting -> topological sorting\n\n\n\n\n\n\n\nimage-20211105053951010\n\n\n\n\n\n\n\n\nimage-20211105054105226\n\n\n\n\n\n\n\n\nimage-20211105054235979\n\n\n\n\n\n\n\n\nimage-20211105054348814\n\n\n\n\n\n\n\n\nimage-20211105054541891\n\n\n\n\n\nimage-20211105054754679\n\n\n\n\n\nimage-20211105054803920\n\n\n\n\n\n\n\n\nimage-20211105054817242\n\n\n\n\n\n\n\n\nimage-20211105055052827\n\n\n\n\n\n\n\n\nimage-20211105055104896\n\n\n\n\n\nimage-20211105055112251\n\n\n\n\n\n\n\nimage-20211105055134402\n\n\n\n\n\nimage-20211105055127357\n\n\n\n\n\nimage-20211105060958201\n\n\n\n\n\nimage-20211105061003923\n\n\n\n\n\nimage-20211105061010571\n\n\n\n\n\nimage-20211105061018009\n\n\n\n\n\nimage-20211105061025420\n\n\n\n\n\nimage-20211105061032723\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211105150655153\n\n\n\n\n\n\n\n\nimage-20211105150708822\n\n\n\n\n\nimage-20211105150716159\n\n\n\n\n\nimage-20211105150723432\n\n\n\n\n\nimage-20211105150730916\n\n\n\n\n\nimage-20211105150738747\n\n\n\n\n\nimage-20211105151041469\n\n\n\n\n\nimage-20211105151047735\n\n\n\n\n\nimage-20211105151052997\n\n\n\n\n\n\n\n\nimage-20211105151108917\n\n\n\n\n\n\n\n\nimage-20211105152247090\n\n\nn = number of nodes\nm = number of edges\n\n\n\n\n\n\nimage-20211105152303847\n\n\n\n\n\n\n\n\nimage-20211105152506769\n\n\n\n\n\nimage-20211105152513773\n\n\n\n\n\n\n\n\nimage-20211105152530953\n\n\n\n\n\n\n\n\nimage-20211105152716299\n\n\n\n\n\n\n\n\nimage-20211105153154271\n\n\n\n\n\n\n\n\nimage-20211105153212068\n\n\n\n\n\nimage-20211105153221946\n\n\n\n\n\nimage-20211105153232021\n\n\n\n\n\n\n\n\nimage-20211105153244399\n\n\n\n\n\nimage-20211105153256183\n\n\n\n\n\n\n\n\nimage-20211105153822565\n\n\n\n\n\n\n\n\nimage-20211105154306535\n\n\n\n\n\n\n\n\nimage-20211105154332820\n\n\n\n\n\nimage-20211105154523140"
  },
  {
    "objectID": "posts/2021-11-06-probability-and-inferential-statistics-week-10/2021-11-06-probability-and-inferential-statistics-week-10.html",
    "href": "posts/2021-11-06-probability-and-inferential-statistics-week-10/2021-11-06-probability-and-inferential-statistics-week-10.html",
    "title": "Probability and Inferential Statistics Week 10",
    "section": "",
    "text": "probability_and_inferential_statistics_week_10_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_10_3\n\n\n\n\n\nprobability_and_inferential_statistics_week_10_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_10_1"
  },
  {
    "objectID": "posts/2021-10-06-kaggle-Data-Visualization/2021-10-06-kaggle-Data-Visualization.html",
    "href": "posts/2021-10-06-kaggle-Data-Visualization/2021-10-06-kaggle-Data-Visualization.html",
    "title": "Kaggle - Data Visualization",
    "section": "",
    "text": "image-20211002220017090\n\n\n\n\n\ntut0_read_csv\n\n\n\n\n\nimage-20211006173350639\n\n\n\n\n\n\n\n\nimage-20211006173601495\n\n\nsns.lineplot tells the notebook that we want to create a line chart.\n\n\n\nimage-20211006173635883\n\n\nThe first line of code sets the size of the figure to 14 inches (in width) by 6 inches (in height). To set the size of any figure, you need only copy the same line of code as it appears. Then, if you’d like to use a custom size, change the provided values of 14 and 6 to the desired width and height.\n\n\n\n\n\nimage-20211006173716415\n\n\n\n\n\n\n\n\n\nimage-20211006174434647\n\n\n\n\n\nimage-20211006174458977\n\n\nannot=True - This ensures that the values for each cell appear on the chart.\n\n\n\n\n\n\nimage-20211006175118494\n\n\nTo double-check the strength of this relationship, you might like to add a regression line, or the line that best fits the data. We do this by changing the command to sns.regplot.\n\n\n\nimage-20211006175125869\n\n\n\n\n\n\n\nimage-20211006175155413\n\n\n\n\n\nimage-20211006175219474\n\n\nWe’ll refer to this plot type as a categorical scatter plot, and we build it with the sns.swarmplot command.\n\n\n\nimage-20211006175244044\n\n\n\n\n\n\n\n\n\n\n\nimage-20211006180210150\n\n\nWe customize the behavior of the command with two additional pieces of information:\n\na= chooses the column we’d like to plot (in this case, we chose 'Petal Length (cm)').\nkde=False is something we’ll always provide when creating a histogram, as leaving it out will create a slightly different plot.\n\n\n\n\n\n\n\nimage-20211006180243443\n\n\n\n\n\n\n\n\nimage-20211006180306571\n\n\n\n\n\n\n\n\nimage-20211006180343444\n\n\nIn this case, the legend does not automatically appear on the plot. To force it to show (for any plot type), we can always use plt.legend().\n\n\n\nimage-20211006180401868\n\n\n\n\n\n\n\n\n\nimg\n\n\nSince it’s not always easy to decide how to best tell the story behind your data, we’ve broken the chart types into three broad categories to help with this.\n\nTrends\n- A trend is defined as a pattern of change.\n\nsns.lineplot - Line charts are best to show trends over a period of time, and multiple lines can be used to show trends in more than one group.\n\nRelationship\n- There are many different chart types that you can use to understand relationships between variables in your data.\n\nsns.barplot - Bar charts are useful for comparing quantities corresponding to different groups.\nsns.heatmap - Heatmaps can be used to find color-coded patterns in tables of numbers.\nsns.scatterplot - Scatter plots show the relationship between two continuous variables; if color-coded, we can also show the relationship with a third categorical variable.\nsns.regplot - Including a regression line in the scatter plot makes it easier to see any linear relationship between two variables.\nsns.lmplot - This command is useful for drawing multiple regression lines, if the scatter plot contains multiple, color-coded groups.\nsns.swarmplot - Categorical scatter plots show the relationship between a continuous variable and a categorical variable.\n\nDistribution\n- We visualize distributions to show the possible values that we can expect to see in a variable, along with how likely they are.\n\nsns.distplot - Histograms show the distribution of a single numerical variable.\nsns.kdeplot - KDE plots (or 2D KDE plots) show an estimated, smooth distribution of a single numerical variable (or two numerical variables).\nsns.jointplot - This command is useful for simultaneously displaying a 2D KDE plot with the corresponding KDE plots for each individual variable.\n\n\n\n\n\nimage-20211006181059400\n\n\n\n\n\nimage-20211006181110176\n\n\nSeaborn has five different themes: (1)\"darkgrid\", (2)\"whitegrid\", (3)\"dark\", (4)\"white\", and (5)\"ticks\", and you need only use a command similar to the one in the code cell above (with the chosen theme filled in) to change it.\n\n\n\n\n\nYou can access Kaggle Datasets by visiting the link below:\n\nhttps://www.kaggle.com/datasets\n\nThe link will bring you to a webpage with a long list of datasets that you can use in your own projects."
  },
  {
    "objectID": "posts/2021-12-15-system-programming-final/2021-12-15-system-programming-final.html",
    "href": "posts/2021-12-15-system-programming-final/2021-12-15-system-programming-final.html",
    "title": "System Programming final",
    "section": "",
    "text": "image-20211215144546753\n\n\n\n\n\n\n\n\nimage-20211215144725841\n\n\n\n\n\n\n\n\nimage-20211215144905925\n\n\n\n\n\nimage-20211215144849316\n\n\n\n\n\n\n\n\nimage-20211215145343566\n\n\n\n\n\nimage-20211215145419970\n\n\n\n\n\nimage-20211215145428740\n\n\n\n\n\n\n\n\nimage-20211215145508439\n\n\n\n\n\nimage-20211215145725188\n\n\n\n\n\nimage-20211215145752636\n\n\n\n\n\nimage-20211215145800308\n\n\n\n\n\n\n\n\nimage-20211215145928096\n\n\n\n\n\n\n\n\nimage-20211215150002279\n\n\n\n\n\n\n\n\nimage-20211215150346399\n\n\n\n\n\n\n\n\n\n\n\nimage-20211215150432950\n\n\n\n\n\nimage-20211215150509856\n\n\n\n\n\n\n\n\nimage-20211215150522743\n\n\n\n\n\n\n\n\nimage-20211215150642886\n\n\n\n\n\n\n\n\nimage-20211215150703490\n\n\n\n\n\nimage-20211215150711785\n\n\n\n\n\nimage-20211215150722569\n\n\n\n\n\nimage-20211215150728435\n\n\n\n\n\nimage-20211215150744275\n\n\n\n\n\nimage-20211215150800301\n\n\n\n\n\n\n\n\nimage-20211215150824606\n\n\n\n\n\n\n\n\nimage-20211215150844651\n\n\n\n\n\nimage-20211215150906394\n\n\n\n\n\n\n\n\nimage-20211215150917218\n\n\n\n\n\n\n\n\nimage-20211215151054065\n\n\n\n\n\n\n\n\nimage-20211215151122100\n\n\n\n\n\n\n\n\nimage-20211215151150203\n\n\n\n\n\nimage-20211215151200424\n\n\n\n\n\n\n\n\nimage-20211215151225591\n\n\n\n\n\n\n\n\nimage-20211215151236801\n\n\n\n\n\nimage-20211215151244304\n\n\n\n\n\nimage-20211215151251903\n\n\n\n\n\nimage-20211215151259432\n\n\n\n\n\nimage-20211215151306641\n\n\n\n\n\nimage-20211215151311831\n\n\n\n\n\nimage-20211215151318227\n\n\n\n\n\nimage-20211215151326242\n\n\n\n\n\nimage-20211215151332263\n\n\n\n\n\n\n\n\nimage-20211215151349882\n\n\n\n\n\nimage-20211215151400304\n\n\n\n\n\n\n\n\nimage-20211215151431642\n\n\n\n\n\n\n\n\nimage-20211215151530662\n\n\n\n\n\n\n\n\nimage-20211215151613808\n\n\n\n\n\nimage-20211215151624855\n\n\n\n\n\nimage-20211215151638756\n\n\n\n\n\n\n\n\nimage-20211215151655459\n\n\n\n\n\n\n\n\nimage-20211215151749406\n\n\n\n\n\n\n\n\nimage-20211215151818028\n\n\n\n\n\n\n\n\nimage-20211215151840596\n\n\n\n\n\n\n\n\nimage-20211215151901313\n\n\nTemporal locality - 썼던 거 또 쓸 가능성이 높다.\nSpatial locality - 썼던거에서 가까운 것을 쓸 가능성이 높다.\n\n\n\nimage-20211215151944282\n\n\n\n\n\n\n\n\nimage-20211215152006223\n\n\n\n\n\nimage-20211215152033194\n\n\n\n\n\n\n\n\nimage-20211215152130353\n\n\n\n\n\nimage-20211215152226080\n\n\n\n\n\nimage-20211215152628327\n\n\n\n\n\n\n\n\nimage-20211215152309457\n\n\n\n\n\n\n\n\nimage-20211215152416373\n\n\n\n\n\n\n\nimage-20211215152431250\n\n\n\n\n\n\n\n\nimage-20211215152458178\n\n\n\n\n\n\n\n\n\nimage-20211215152535039\n\n\n\n\n\n\n\n\nimage-20211215152656484\n\n\n\n\n\n\n\n\n\n\n\nimage-20211215152720272\n\n\n\n\n\n\n\n\nimage-20211215152745875\n\n\n\n\n\nimage-20211215152756740\n\n\n\n\n\n\n\n\nimage-20211215152835897\n\n\n\n\n\nimage-20211215152924412\n\n\n\n\n\n\n\n\nimage-20211215153018895\n\n\n\n\n\n\n\n\nimage-20211215153104282\n\n\n\n\n\n\n\n\nimage-20211215153122082\n\n\n\n\n\nimage-20211215153200856\n\n\n\n\n\n\n\n\nimage-20211215153255564\n\n\n\n\n\n\n\n\nimage-20211215153348934\n\n\n\n\n\n\n\n\nimage-20211215153417302\n\n\n\n\n\n\n\n\nimage-20211215153738202\n\n\n\n\n\n\n\n\nimage-20211215153800893\n\n\n\n\n\n\n\n\nimage-20211215153830231\n\n\n\n\n\n\n\n\nimage-20211215153922052\n\n\n\n\n\nimage-20211215153931683\n\n\n\n\n\nimage-20211215153950579\n\n\n\n\n\nimage-20211215154031403\n\n\n\n\n\nimage-20211215154110354\n\n\n\n\n\n\n\n\nimage-20211215154120087\n\n\n\n\n\n\n\n\nimage-20211215154136732\n\n\n\n\n\nimage-20211215154146292\n\n\n\n\n\nimage-20211215154155429\n\n\n\n\n\n\n\n\nimage-20211215154243945\n\n\n\n\n\n\n\n\nimage-20211215154339943\n\n\n\n\n\n\n\n\nimage-20211215154418757\n\n\n\n\n\n\n\n\nimage-20211215154434218\n\n\n\n\n\n\n\n\nimage-20211215154514939\n\n\n\n\n\n\n\n\nimage-20211215154529866\n\n\n\n\n\nimage-20211215154538381\n\n\n\n\n\n\n\n\nimage-20211215154557361\n\n\n\n\n\nimage-20211215154604985\n\n\n\n\n\n\n\n\nimage-20211215154719147\n\n\n\n\n\nimage-20211215154808325\n\n\n\n\n\n\n\n\nimage-20211215154833477\n\n\n\n\n\n\n\n\nimage-20211215154858202\n\n\n\n\n\nimage-20211215154907835\n\n\n\n\n\nimage-20211215154915330\n\n\n\n\n\n\n\n\nimage-20211215154940892\n\n\n\n\n\n\n\n\n\nimage-20211215155013329\n\n\n\n\n\n\n\nimage-20211215155034473\n\n\n\n\n\nimage-20211215155125396\n\n\n\n\n\n\n\n\nimage-20211215155138897\n\n\n\n\n\n\n\n\nimage-20211215155209787\n\n\n\n\n\nimage-20211215155229263\n\n\n\n\n\n\n\n\nimage-20211215155305908\n\n\n\n\n\nimage-20211215155318738\n\n\n\n\n\n\n\n\nimage-20211215155358628\n\n\n\n\n\nimage-20211215155422889\n\n\n\n\n\nimage-20211215155431503\n\n\n\n\n\n\n\n\nimage-20211215155445470\n\n\n\n\n\n\n\n\n\n\n\nimage-20211215155543089\n\n\n\n\n\n\n\n\nimage-20211215161111328\n\n\n\n\n\n\n\n\nimage-20211215161142036\n\n\n\n\n\n\n\n\nimage-20211215161250379\n\n\n\n\n\n\n\n\nimage-20211215161310796\n\n\n\n\n\n\n\n\nimage-20211215161355576\n\n\n\n\n\n\n\n\nimage-20211215161416413\n\n\nTrap - 의도적\nfaults - 의도하지 않았으나 복구 가능\naborts - 의도하지 않았고 복구 불능\n\n\n\n\n\n\nimage-20211215161442267\n\n\n\n\n\n\n\n\nimage-20211215161523360\n\n\n\n\n\n\n\n\nimage-20211215161625510\n\n\n\n\n\n\n\n\nimage-20211215161649703\n\n\n\n\n\n\n\n\n\nimage-20211215161709579\n\n\n\n\n\n\n\nimage-20211215161745378\n\n\n\n\n\n\n\n\nimage-20211215161831969\n\n\n\n\n\n\n\n\nimage-20211215161848037\n\n\n\n\n\n\n\n\nimage-20211215161912373\n\n\n\n\n\nimage-20211215161926846\n\n\n\n\n\nimage-20211215161946170\n\n\n\n\n\nimage-20211215162011407\n\n\n\n\n\nimage-20211215162028567\n\n\n\n\n\nimage-20211215162054274\n\n\n\n\n\nimage-20211215162103703\n\n\n\n\n\n\n\n\nimage-20211215162124672\n\n\n\n\n\n\n\n\nimage-20211215162138337\n\n\n\n\n\nimage-20211215162226222\n\n\n\n\n\nimage-20211215162235277\n\n\n\n\n\n\n\n\nimage-20211215162306110\n\n\n\n\n\nimage-20211215162514724\n\n\n\n\n\nimage-20211215162525382\n\n\n\n\n\n\n\n\nimage-20211215162607207\n\n\n\n\n\n\n\n\nimage-20211215162638420\n\n\n\n\n\nimage-20211215162717200\n\n\n\n\n\n\n\n\nimage-20211215162757271\n\n\n\n\n\nimage-20211215162806878\n\n\n\n\n\n\n\n\n\n\n\nimage-20211215162915346\n\n\n\n\n\n\n\n\nimage-20211215163033847\n\n\n\n\n\n\n\n\nimage-20211215163108444\n\n\n\n\n\n\n\n\nimage-20211215163119120\n\n\n\n\n\n\n\n\nimage-20211215163145993\n\n\n\n\n\n\n\n\nimage-20211215163159317\n\n\n\n\n\n\n\n\nimage-20211215163213761\n\n\n\n\n\n\n\n\nimage-20211215163500474\n\n\n\n\n\n\n\n\n\nimage-20211215163537428\n\n\n\n\n\n\n\nimage-20211215163615154\n\n\n\n\n\n\n\n\nimage-20211215163701451\n\n\n\n\n\n\n\n\nimage-20211215163738766\n\n\n\n\n\n\n\n\nimage-20211215164230478\n\n\n\n\n\n\n\n\nimage-20211215164258124\n\n\n\n\n\n\n\n\nimage-20211215164328235\n\n\n\n\n\n\n\n\nimage-20211215164354351\n\n\n\n\n\n\n\n\nimage-20211215164433765\n\n\n\n\n\n\n\n\nimage-20211215164511607\n\n\n\n\n\n\n\n\nimage-20211215164524338"
  },
  {
    "objectID": "posts/2021-10-08-probability-and-inferential-statistics-week-6/2021-10-08-probability-and-inferential-statistics-week-6.html",
    "href": "posts/2021-10-08-probability-and-inferential-statistics-week-6/2021-10-08-probability-and-inferential-statistics-week-6.html",
    "title": "Probability and Inferenctial Statistics Week 6",
    "section": "",
    "text": "probability_and_inferential_statistics_week_5_Page_04\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_05\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_06\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_07\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_08\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_09\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_10\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_11\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_12\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_13\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_14\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_15\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_1\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_3\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_5\n\n\n\n\n\nprobability_and_inferential_statistics_week_6_Page_6\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_01\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_02\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_03\n\n\n\n\n\n\n\n\n20170361_5주차 숙제_Page_10\n\n\n\n\n\n20170361_5주차 숙제_Page_11\n\n\n\n\n\n20170361_5주차 숙제_Page_12\n\n\n\n\n\n20170361_5주차 숙제_Page_13\n\n\n\n\n\n20170361_5주차 숙제_Page_14\n\n\n\n\n\n20170361_5주차 숙제_Page_15\n\n\n\n\n\n20170361_5주차 숙제_Page_01\n\n\n\n\n\n20170361_5주차 숙제_Page_02\n\n\n\n\n\n20170361_5주차 숙제_Page_03\n\n\n\n\n\n20170361_5주차 숙제_Page_04\n\n\n\n\n\n20170361_5주차 숙제_Page_05\n\n\n\n\n\n20170361_5주차 숙제_Page_06\n\n\n\n\n\n20170361_5주차 숙제_Page_07\n\n\n\n\n\n20170361_5주차 숙제_Page_08\n\n\n\n\n\n20170361_5주차 숙제_Page_09"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html",
    "title": "CFA Level 2 Derivatives",
    "section": "",
    "text": "Derivatives"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#forward-contract",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#forward-contract",
    "title": "CFA Level 2 Derivatives",
    "section": "Forward contract",
    "text": "Forward contract\nLong forward position – agrees to buy the financial or physical asset\nShort forward position – agrees to sell/deliver the asset\nTypically, no money changes hands at the inception of the contract, unlike futures contracts.\nAt any point in time, forward contract is a zero-sum game.\n\nThe no-arbitrage principle\nForward price refers to the forward price of the underlying.\nNo-arbitrage principle - The price that we wish to determine is the forward price that makes the values of both the long and the short positions zero at contract initiation. There should be no riskless profit to be gained by a combination of a forward contract position with positions in other assets.\nThis principle assumes that (1) transactions costs are zero, (2) there are no restrictions on short sales or the use of short sale proceeds, and (3) both borrowing and lending can be done in unlimited amounts at the risk-free rate of interest.\nForward price = price that prevents profitable riskless arbitrage in frictionless markets"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#a-simple-version-of-the-cost-of-carry-model",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#a-simple-version-of-the-cost-of-carry-model",
    "title": "CFA Level 2 Derivatives",
    "section": "A simple version of the cost-of-carry model",
    "text": "A simple version of the cost-of-carry model\n\n\n\nimg"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#cash-and-carry-arbitrage-when-the-forward-contract-is-overpriced-compared-to-no-arbitrage-price",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#cash-and-carry-arbitrage-when-the-forward-contract-is-overpriced-compared-to-no-arbitrage-price",
    "title": "CFA Level 2 Derivatives",
    "section": "Cash and carry arbitrage when the forward contract is overpriced compared to no-arbitrage price",
    "text": "Cash and carry arbitrage when the forward contract is overpriced compared to no-arbitrage price\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#reverse-cash-and-carry-arbitrage-when-the-forward-contract-is-underprice-compared-to-no-arbitrage-price",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#reverse-cash-and-carry-arbitrage-when-the-forward-contract-is-underprice-compared-to-no-arbitrage-price",
    "title": "CFA Level 2 Derivatives",
    "section": "Reverse cash and carry arbitrage when the forward contract is underprice compared to no-arbitrage price",
    "text": "Reverse cash and carry arbitrage when the forward contract is underprice compared to no-arbitrage price\nShort spot asset, long forward"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#day-count-convention",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#day-count-convention",
    "title": "CFA Level 2 Derivatives",
    "section": "Day count convention",
    "text": "Day count convention"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#a-picture-containing-diagram-description-automatically-generated",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#a-picture-containing-diagram-description-automatically-generated",
    "title": "CFA Level 2 Derivatives",
    "section": "",
    "text": "All LIBOR-based contracts such as FRAs, swaps, caps, floors, etc\n360 days per year and simple interest\n\n\nEquities, bonds, currencies, and stock options\n365 days per year and periodic compound interest\n\n\nEquity indexes\n365 days per year and continuous compounding"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-equity-forwards",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-equity-forwards",
    "title": "CFA Level 2 Derivatives",
    "section": "Pricing and valuation of equity forwards",
    "text": "Pricing and valuation of equity forwards\n\nEquity forward contracts with discrete dividends\n\n\n\nSchematic Description automatically generated with low confidence\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n진정한 기초자산이 뭔지 확인하고, 그 기초자산을 무위험 이자율로 굴려나가면 forward price\n\n\n\nA blackboard with writing on it Description automatically generated with medium confidence\n\n\n\n\n\nDiagram Description automatically generated with low confidence\n\n\n\n\nEquity forward contracts with continuous dividends\nUnderlying asset is an equity index.\nContinuous dividend는 dividend yield로 표시됨\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-fixed-income-forwards",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-fixed-income-forwards",
    "title": "CFA Level 2 Derivatives",
    "section": "Pricing and valuation of fixed income forwards",
    "text": "Pricing and valuation of fixed income forwards\n\n\n\nText Description automatically generated\n\n\n\n\n\nimg\n\n\nClean price는 해당 주기에서의 bond 가격\nFull price는 실제 bond market에서 거래되는 가격, 해당 bond를 사려면 지불해야 하는 실제 가격 – accrued interest 포함\nUST – semiannual\nBond futures contracts often allow the short an option to deliver any of several bonds, which will satisfy the delivery terms of the contract. This is called a delivery option and is valuable to the short. Each bond is given a conversion factor that is used to adjust the long’s payment at delivery so the more valuable bonds receive a lager payment. These factors are multipliers for the futures price at settlement. The long pays the futures price at expiration, multiplied by the conversion factor (CF).\n\n\n\nimg\n\n\n가격표시는 clean price로 되지만, 실제 futures price를 거래할 때는 accrued interest의 현재가치를 빼준 full price를 사용해야 한다.\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated with low confidence\n\n\n\n\n\nA picture containing diagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-forward-rate-agreements",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-forward-rate-agreements",
    "title": "CFA Level 2 Derivatives",
    "section": "Pricing forward rate agreements",
    "text": "Pricing forward rate agreements\nLIBOR – London Interbank Offered Rate\nIt is quoted as an annualized rate based on a 360-day year.\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nThe long position in a forward rate agreement (FRA) is the party that is effectively borrowing money (long the loan, with the contract price being the interest rate on the loan).\n\nPricing FRAs\n\\1. LIBOR rates in the Eurodollar market are add-on rates and are always quoted on a 30/360 day basis in annual terms.\n\\2. The long position in an FRA is, in effect, long the rate and benefits when the rate increases.\n\\3. Although the interest on the underlying loan won’t be paid until the end of the loan. Therefore, the payoff on the FRA is the present value of the interest savings on the loan.\nThe forward price in an FRA is actually a forward interest rate.\n\n\nValuation of forward rate agreements\n\n\n\nA blackboard with white writing Description automatically generated with low confidence\n\n\n\n\n\nA picture containing diagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-currency-contracts",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#pricing-and-valuation-of-currency-contracts",
    "title": "CFA Level 2 Derivatives",
    "section": "Pricing and valuation of currency contracts",
    "text": "Pricing and valuation of currency contracts\nInterest rate parity is based on a no-arbitrage condition: a higher interest rate currency will trade at a forward discount to offset the extra interest income.\n\n\n\nText Description automatically generated\n\n\n\n\n\nA picture containing text Description automatically generated\n\n\n항상 가격 표시는 price currency (foreign currency)로\n\n\n\nimg\n\n\n\n\n\nimg\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#futures-contract",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#futures-contract",
    "title": "CFA Level 2 Derivatives",
    "section": "Futures contract",
    "text": "Futures contract\nFutures contract are very much like the forward contracts except that they trade on organized exchanges.\nTo safeguard the clearinghouse, the exchange requires both sides of the trade to post margin and settle their accounts on a daily basis. Thus, the margin in the futures markets is a performance guarantee.\nMarking to market is the process of adjusting the margin balance in a futures account each day for the change in the value of the contract from the previous trading day, based on the settlement price.\nLike forward contracts, futures contracts have no value at contract initiation. Unlike forward contracts, futures contracts do not accumulate value changes over the term of the contract. Since futures accounts are marked to market daily, the value after the margin deposit has been adjusted for the day’s gains and losses in contract value is always zero."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#computing-the-swap-fixed-rate",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#computing-the-swap-fixed-rate",
    "title": "CFA Level 2 Derivatives",
    "section": "Computing the swap fixed rate",
    "text": "Computing the swap fixed rate\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nText Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#calculating-the-market-value-of-an-interest-rate-swap",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#calculating-the-market-value-of-an-interest-rate-swap",
    "title": "CFA Level 2 Derivatives",
    "section": "Calculating the market value of an interest rate swap",
    "text": "Calculating the market value of an interest rate swap\n\n\n\nA blackboard with white writing Description automatically generated with low confidence\n\n\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#currency-swaps",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#currency-swaps",
    "title": "CFA Level 2 Derivatives",
    "section": "Currency swaps",
    "text": "Currency swaps\nThe interest rates in a currency swap are simply the swap rates calculated from each country’s yield curve in the relevant country’s currency.\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nA blackboard with writing on it Description automatically generated with medium confidence\n\n\n\n\n\nChart Description automatically generated\n\n\n\n\n\nTimeline Description automatically generated with low confidence"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equity-swaps",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equity-swaps",
    "title": "CFA Level 2 Derivatives",
    "section": "Equity swaps",
    "text": "Equity swaps\nA swap of returns on two different stocks can be viewed as buying one stock (receiving the returns) and shorting an equal value of a different stock (paying the returns). There is on pricing at swap initiation.\n\n\n\nA blackboard with writing on it Description automatically generated with medium confidence\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nText Description automatically generated\n\n\n\n\n\nDiagram, schematic Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#two-period-binomial-model-and-put-call-parity",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#two-period-binomial-model-and-put-call-parity",
    "title": "CFA Level 2 Derivatives",
    "section": "Two period binomial model and put-call parity",
    "text": "Two period binomial model and put-call parity\n\n\n\nText Description automatically generated\n\n\nPut call parity can be used to create a synthetic instrument that replicates the desired instrument.\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nGraphical user interface, application Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated with low confidence"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#american-option",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#american-option",
    "title": "CFA Level 2 Derivatives",
    "section": "American option",
    "text": "American option\nWhile the early exercise feature is not valuable for call options on non-dividend paying stocks, deep-in-the-money put options could benefit from early exercise. When an investor exercises an option early, she captures only the intrinsic value of the option and forgoes the time value. While the intrinsic value can be invested at the risk-free rate, the interest so earned is less than the time value in most cases. For deep-in-the-money put option, the upside is limited. In such cases, the interest on intrinsic value can exceed the option’s time value.\nAmerican-style call options on dividend-paying stocks can be evaluated similarly. For dividend-paying stocks the stock price falls when the stock goes ex-dividend, and it may take sense to exercise the call option before such a decline in price.\n\n\n\nDiagram Description automatically generated with medium confidence\n\n\n\n\n\nText Description automatically generated\n\n\n\n\n\nA picture containing diagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equivalencies-in-interest-rate-derivatives-contracts",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equivalencies-in-interest-rate-derivatives-contracts",
    "title": "CFA Level 2 Derivatives",
    "section": "Equivalencies in interest rate derivatives contracts",
    "text": "Equivalencies in interest rate derivatives contracts\n\\1. A long interest rate call and a short interest rate put (with exercise rate = current FRA rate) can be used to replicate a long FRA.\n\\2. If the exercise rate = the current FRA rate, a short interest rate call and long interest rate put can be combined to replicate a short FRA position.\n\\3. A series of interest rate call options with different maturities and the same exercise price can be combined to form an interest rate cap. Each of the call options in an interest rate cap is known as a caplet. A floating rate loan can be hedged using a long interest rate bond.\n\\4. An interest rate floor is a portfolio of interest rate put options, and each of these puts is known as a floorlet. Floors can be used to hedge a long position in a floating rate bond.\n\\5. If the exercise rate on a cap and floor is same, a long cap and short floor can be used to replicate a payer swap.\n\\6. A short cap and long floor can replicate a receiver swap.\n\\7. If the exercise rate on a floor and a cap are set equal to a market swap fixed rate, the value of the cap will be equal to the value of the floor.\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nA screenshot of a computer Description automatically generated with low confidence\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#swaptions",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#swaptions",
    "title": "CFA Level 2 Derivatives",
    "section": "Swaptions",
    "text": "Swaptions\nA swaption is an option that gives the holder the right to enter into an interest rate swap.\n\n\n\nA blackboard with writing on it Description automatically generated with low confidence\n\n\n\n\n\nA picture containing text, clipart Description automatically generated\n\n\nA swaption is equivalent to an option on a series of cash flows (annuity), one for each settlement date of the underlying swap, equal to the difference between the exercise rate on the swaption and the market swap fixed rate.\nIf PVA represents the present value of such an annuity, the value of a payer swaption using the Black model can be calculated."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equivalencies",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#equivalencies",
    "title": "CFA Level 2 Derivatives",
    "section": "Equivalencies",
    "text": "Equivalencies\nA long callable bond can be replicated using a long position option-free bond plus a short receiver swaption.\n\n\n\nA blackboard with white writing Description automatically generated with low confidence\n\n\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#delta",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#delta",
    "title": "CFA Level 2 Derivatives",
    "section": "Delta",
    "text": "Delta\nDelta describes the relationship between changes in asset prices and changes in option prices. Delta is also the hedge ratio. Call option deltas are positive because as the underlying asset price increases, call option value also increases. Conversely, the delta of a put option is negative because put value falls as the asset price increases.\n\nInterpreting Delta\nThe bottom line is that a call option’s delta will increase from 0 to 1 as non-dividend paying stock price increase. For a non-dividend paying stock, the put delta increases from -1 to 0 as the stock price increases.\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nText Description automatically generated with medium confidence\n\n\n\n\n\nA picture containing diagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#gamma",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#gamma",
    "title": "CFA Level 2 Derivatives",
    "section": "Gamma",
    "text": "Gamma\nGamma measures the rate of changes in delta as the underlying stock price changes. Gamma captures the curvature of the option-value-versus-stock-price relationship. Long positions in calls and puts have positive gamma.\nGamma is highest for at-the-money options. Deep-in-the-money or deep-out-of-the-money options have low gamma. Gamma changes with stock price and with time to expiration. To lower (increase) the overall gamma of a portfolio, one should short (go long) options.\nCall and put options on the same underlying asset with the same exercise price and time to expiration will have equal gammas.\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#vega",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#vega",
    "title": "CFA Level 2 Derivatives",
    "section": "Vega",
    "text": "Vega\nVega measures the sensitivity of the option price to changes in the volatility of returns on the underlying asset. Both call and put options are more valuable, all else equal, the higher the volatility, so vega is positive for both calls and puts."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#theta",
    "href": "posts/2021-09-12-CFA-Level-2-Derivatives/2021-09-12-CFA-Level-2-Derivatives.html#theta",
    "title": "CFA Level 2 Derivatives",
    "section": "Theta",
    "text": "Theta\nTheta measures the sensitivity of option price to the passage of time. As time passes and a call option approaches maturity, its speculative value declines, all else equal. This is called time decay. That behavior also applied for most put options (though deep in-the-money put options closed to maturity may actually increases in value as time passes). Because it is a measure of time decay, theta is less than zero: as time passes and the option approaches the maturity date, the option value decreases (holding other factors constant).\n\n\n\nText Description automatically generated\n\n\n\n\n\nDiagram, schematic Description automatically generated\n\n\n\n\n\n\n\n\n\n\n\nSensitivity factor (Greeks)\nInput\nLong call\nLong put\n\n\n\n\nDelta\nAsset price\nPositively related\nNegatively related\n\n\nGamma\nDelta\nPositive\nPositive\n\n\nVega\nVolatility\nPositive\nPositive\n\n\nRho\nRisk-free rate\nPositive\nNegative\n\n\nTheta\nTime to expiration\nNegative\nNegative\n\n\n\nExercise price\nNegatively related\nPositively related"
  },
  {
    "objectID": "posts/2021-11-04-system-programming-week-9/2021-11-04-system-programming-week-9.html",
    "href": "posts/2021-11-04-system-programming-week-9/2021-11-04-system-programming-week-9.html",
    "title": "System Programming Week 9",
    "section": "",
    "text": "There’s more to performance than asymptotic complexity.\nConstant factors matter too!\n\nEasily see 10:1 performance range depending on how code is written.\nMust optimize at multiple levels:\n\nalgorithm, data representations, procedures, and loops\n\n\n\n\n\nimage-20211104124636896\n\n\n\n\n\nProvide efficient mapping of program to machine\n\nregister allocation\ncode selection and ordering (scheduling)\ndead code elimination\neliminating minor inefficiencies\n\nDon’t (usually) improve asymptotic efficiency\n\nup to programmer to select best overall algorithm\nbig-O savings are (often) more important than constant factors\n\nbut constant factors also matter\n\n\nHave difficulty overcoming “optimization blockers”\n\npotential memory aliasing\npotential procedure side-effects\n\n\n\n\noperate under fundamental constraint\n\nMust not cause any change in program behavior\n\nexcept, possibly when program making use of nonstandard language features\n\noften prevents it from making optimizations that would only affect behavior under pathological conditions.\n\nBehavior that may be obvious to the programmer can be obfuscated by language and coding styles\nMost analysis is performed only within procedures\n\nwhole-program analysis is too expensive in most cases\nNewer versions of GCC do interprocedural analysis within individual files.\n\nbut, not between code in different files\n\n\nmost analysis is based only on static information\n\ncompiler has difficult anticipating run-time inputs.\n\nWhen in doubt, the compiler must be conservative.\n\n\n\noptimizations that you or the compiler should do regardless of processor / compiler\n\n\n\nimage-20211104125409736\n\n\n\n\n\nimage-20211104125507441\n\n\n\n\n\nReplace costly operation with simpler one\nShift, add instead of multiply or divide\n\n\n\nimage-20211104125538581\n\n\n\n\n\n\nReuse portions of expressions\nGCC will do this with -O1\n\n\n\nimage-20211104125955469\n\n\n\n\n\n\n\n\nimage-20211104130102398\n\n\n\n\n\nimage-20211104130109095\n\n\nAliasing\n\nTwo different memory references specify single location\nEasy to have happen in C\n\nSince allowed to address arithmetic\nDirect access to storage structures\n\nGet in habit of introducing local variables\n\nAccumulating within loops\nYour way of telling compiler not to check for aliasing\n\n\n\n\n\n\n\n\nimage-20211104130434564\n\n\n만약 function이 global state를 바꾸면 기대한 return이 아닐 수 있다.\n\n\n\nimage-20211104130441996\n\n\n\n\n\n\n\n\nimage-20211104130458892\n\n\n\n\n\n\n\n\nimage-20211105040355474\n\n\n\n\n\n\n\n\nimage-20211105040446251\n\n\nstrlen은 굉장히 비싼 연산임. null로 끝날 때까지 찾아서 length를 계산하기 때문.\n\n\n\n\n\n\nimage-20211105040452997\n\n\n\n\n\n\n\n\nimage-20211105040509578\n\n\n\n\n\n\n\n\nimage-20211105040532095\n\n\n\n\n\n\n\n\nimage-20211105040705243\n\n\n\n\n\n\n\n\nimage-20211105040859420\n\n\n\n\n\nimage-20211105040934355\n\n\n\n\n\n\n\n\nimage-20211105041021277\n\n\n\n\n\nimage-20211105042331170\n\n\n\n\n\n\n\n\nimage-20211105042339362\n\n\n\n\n\n\n\n\nimage-20211105042421627\n\n\n\n\n\n\n\n\nimage-20211105042705329\n\n\n\n\n\n\n\n\nimage-20211105042819389\n\n\n\n\n\n\n\n\nimage-20211105043032830\n\n\n\n\n\n\n\n\nimage-20211105043057963\n\n\nsuperscalar processor의 특징을 활용한 optimization\n\n\n\n\n\n\nimage-20211105043230620\n\n\n\n\n\n\n\n\nimage-20211105043335484\n\n\n\n\n\n\n\n\nimage-20211105043400630\n\n\n\n\n\n\n\n\nimage-20211105043604627\n\n\n\n\n\n\n\n\nimage-20211105043610158\n\n\n\n\n\n\n\n\nimage-20211105043624140"
  },
  {
    "objectID": "posts/2021-12-05-system-programming-week-13/2021-12-05-system-programming-week-13.html",
    "href": "posts/2021-12-05-system-programming-week-13/2021-12-05-system-programming-week-13.html",
    "title": "System Programming Week 13",
    "section": "",
    "text": "image-20211205040917940\n\n\n\n\n\n\n\n\nimage-20211205040932629\n\n\n\n\n\n\n\n\nimage-20211205041056453\n\n\n\n\n\n\n\n\nimage-20211205041306667\n\n\n\n\n\n\n\n\nimage-20211205041338103\n\n\n\n\n\n\n\n\nimage-20211205041450219\n\n\n\n\n\n\n\n\nimage-20211205041741766\n\n\n\n\n\n\n\n\nimage-20211205041943017\n\n\n\n\n\n\n\n\nimage-20211205042100676\n\n\n\n\n\n\n\n\nimage-20211205044234172\n\n\n\n\n\n\n\n\nimage-20211205044406371\n\n\n\n\n\n\n\n\nimage-20211205044503809\n\n\n\n\n\n\n\n\nimage-20211205044541688\n\n\n\n\n\n\n\n\nimage-20211205044714817\n\n\n\n\n\n\n\n\nimage-20211205044757732\n\n\n\n\n\n\n\n\nimage-20211205044933059\n\n\n\n\n\n\n\n\nimage-20211205045030843\n\n\n\n\n\n\n\n\nimage-20211205045112507\n\n\n\n\n\n\n\n\nimage-20211205045307051\n\n\n\n\n\n\n\n\nimage-20211205045322580\n\n\n\n\n\n\n\n\nimage-20211205045513798\n\n\n\n\n\n\n\n\nimage-20211205045529025\n\n\n\n\n\n\n\n\nimage-20211205045650531\n\n\n\n\n\nimage-20211205045657298"
  },
  {
    "objectID": "posts/2021-12-05-system-programming-week-14/2021-12-05-system-programming-week-14.html",
    "href": "posts/2021-12-05-system-programming-week-14/2021-12-05-system-programming-week-14.html",
    "title": "System Programming Week 14",
    "section": "",
    "text": "image-20211205051808716\n\n\n\n\n\n\n\n\nimage-20211205051823778\n\n\n\n\n\n\n\n\nimage-20211205051943933\n\n\n\n\n\n\n\n\nimage-20211205052219269\n\n\n\n\n\n\n\n\nimage-20211205052503073\n\n\n\n\n\n\n\n\nimage-20211205052643250\n\n\n\n\n\nimage-20211205053226327\n\n\n\n\n\nimage-20211205053236560\n\n\n\n\n\n\n\n\nimage-20211205053250583\n\n\n\n\n\nimage-20211205053300874\n\n\n\n\n\nimage-20211205053307398\n\n\n\n\n\nimage-20211205053316177\n\n\n\n\n\n\n\n\nimage-20211205053329741\n\n\n\n\n\n\n\n\nimage-20211205053358705\n\n\n\n\n\n\n\n\nimage-20211205053410276\n\n\n\n\n\nimage-20211205053418875\n\n\n\n\n\n\n\n\nimage-20211205053438559\n\n\n\n\n\nimage-20211205053445847\n\n\n\n\n\n\n\n\nimage-20211205053454810\n\n\n\n\n\n\n\n\nimage-20211205053510405\n\n\n\n\n\nimage-20211205053516735"
  },
  {
    "objectID": "posts/2021-12-07-probability-and-statistics-week-13/2021-12-07-probability-and-statistics-week-13.html",
    "href": "posts/2021-12-07-probability-and-statistics-week-13/2021-12-07-probability-and-statistics-week-13.html",
    "title": "System Programming Week 13",
    "section": "",
    "text": "image-20211208033528394\n\n\n\n\n\n\n\n\nimage-20211208033539132\n\n\n\n\n\nimage-20211208033544399\n\n\n\n\n\nimage-20211208033550157\n\n\n\n\n\n\n\n\nimage-20211208033610678\n\n\n\n\n\n\n\n\nimage-20211208033628186\n\n\n\n\n\n\n\n\nimage-20211208033708087\n\n\n\n\n\n\n\n\nimage-20211208033722196\n\n\n\n\n\n\n\n\nimage-20211208033733611\n\n\n\n\n\nimage-20211208034100561\n\n\n\n\n\n\n\n\nimage-20211208034113888\n\n\n\n\n\n\n\n\nimage-20211208034125490\n\n\n\n\n\n\n\n\nimage-20211208035734196\n\n\n\n\n\n\n\n\nimage-20211208035744852\n\n\n\n\n\n\n\n\nimage-20211208040142007\n\n\n\n\n\nimage-20211208040147654\n\n\n\n\n\nimage-20211208040153656\n\n\n\n\n\n\n\n\nimage-20211208040206472\n\n\n\n\n\n\n\n\nimage-20211208040306626\n\n\n\n\n\nimage-20211208040327590\n\n\n\n\n\n\n\n\nimage-20211208040343520\n\n\n\n\n\nimage-20211208040348615\n\n\n\n\n\nimage-20211208040354684\n\n\n\n\n\nimage-20211208040400162\n\n\n\n\n\n\n\n\nimage-20211208040418533\n\n\n\n\n\nimage-20211208040425191\n\n\n\n\n\nimage-20211208040431515\n\n\n\n\n\nimage-20211208040438123\n\n\n\n\n\nimage-20211208040445742\n\n\n\n\n\n\n\n\nimage-20211208040507219\n\n\n\n\n\n\n\n\nimage-20211208040518044"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html",
    "title": "CFA Level 2 Fixed Income",
    "section": "",
    "text": "Text Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#spot-rates",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#spot-rates",
    "title": "CFA Level 2 Fixed Income",
    "section": "Spot rates",
    "text": "Spot rates\n\n\n\nDiagram Description automatically generated\n\n\nSpot rates are the annualized market interest rates for a single payment to be received in the future. Generally, we use spot rates for government securities (risk-free) to generate the spot rate curve.\nThe spot interest rate is the yield to maturity of a zero-coupon bond.\nThe term structure of spot rates is known as the spot curve. The shape and level of the spot curve changes continuously with the market prices of bonds."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#forward-rates",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#forward-rates",
    "title": "CFA Level 2 Fixed Income",
    "section": "Forward rates",
    "text": "Forward rates\nThe annualized interest rate on a loan to be initiated at a future period is called the forward rate for that period.\nThe term structure of forward rates is called the forward curve.\nForward curves and spot curves are mathematically related."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#yield-to-maturity",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#yield-to-maturity",
    "title": "CFA Level 2 Fixed Income",
    "section": "Yield to maturity",
    "text": "Yield to maturity\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nYTM of a zero-coupon bond with maturity T is the spot interest rate for a maturity of T.\nFor a coupon bond, if the spot rate curve is not flat, the YTM will not be the same as the spot rate.\nThe yield on a coupon bond is a weighted average of three spot rates."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#expected-and-realized-returns-on-bonds",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#expected-and-realized-returns-on-bonds",
    "title": "CFA Level 2 Fixed Income",
    "section": "Expected and realized returns on bonds",
    "text": "Expected and realized returns on bonds\n\n\n\nText Description automatically generated with medium confidence\n\n\nExpected return is the ex-ante holding period return that a bond investor expects to earn. The expected return will be equal to the bond’s yield only when all three of the following are true:\n- The bond is held to maturity\n- All payments (coupon and principal) are made on time and in full\no option-free, no default risk, no prepayment risk\n- All coupons are reinvested at the original YTM\no If the yield curve is not flat, the coupon payments will not be reinvested at the YTM and the expected return will differ from the yield.\n\n\n\nText Description automatically generated\n\n\nRealized return on a bond refers to the actual return that the investor experiences over the investment’s holding period. Realized return is based on actual reinvestment rates."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-forward-pricing-model",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-forward-pricing-model",
    "title": "CFA Level 2 Fixed Income",
    "section": "The forward pricing model",
    "text": "The forward pricing model\n\n\n\nDiagram Description automatically generated\n\n\nForward price는 현재 spot price에 투자하는 대신 무위험 수익률을 노렸을 경우의 가치와 동일하다. 그래서 forward price를 risk-free rate으로 할인하면 현재 시점의 spot price와 같은 값이 나오는 것. 이게 arbitrage-free pricing\n현재 시점에서 Forward contract value는 0"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-forward-rate-model",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-forward-rate-model",
    "title": "CFA Level 2 Fixed Income",
    "section": "The forward rate model",
    "text": "The forward rate model\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\nThis model is useful because it illustrates how forward rates and spot rates are interrelated.\nSpot rate과 forward rate은 geometric average 관계\nSpot curve가 upward sloping이면 forward rate은 항상 spot rate보다 크고, 그러므로 forward curve는 spot curve 위에 위치한다. Spot curve가 downward sloping이면, forward rate은 spot rate보다 항상 작고, 그러므로 forward curve는 spot curve 밑에 위치한다.\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#relationships-between-spot-and-forward-rates",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#relationships-between-spot-and-forward-rates",
    "title": "CFA Level 2 Fixed Income",
    "section": "Relationships between spot and forward rates",
    "text": "Relationships between spot and forward rates\nFor an upward-sloping spot curve the forward rate rises as increases. For downward-sloping spot curve, the forward rate declines as increases. For an upward-sloping spot curve, the forward curve will be above the spot curve. Conversely, when the spot curve is downward sloping, the forward curve will be below it.\nThe spot rate for a long-maturity security will equal the geometric mean of a series of one-year forward rates."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#forward-price-evolution",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#forward-price-evolution",
    "title": "CFA Level 2 Fixed Income",
    "section": "Forward price evolution",
    "text": "Forward price evolution\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nForward rate는 현재 상황에서 관찰된 미래 시점의 단위기간 금리일 뿐, 시간이 지나면 변화한다. 그러므로 forward contract들도 expiration이 되면 해당 시점의 contract value와 forward price가 다를 수 있다. 만기 시점의 contract value = forward price이려면, 계약 시점에 관찰된 forward rate가 미래인 만기 시점에 spot rate로 정확히 실현되어야 한다.\nIf the future spot rates actually evolve as forecasted by the forward curve, the forward price will remain unchanged. Therefore, a change in the forward price indicates that the future spot rate(s) did not conform to the forward curve. When spot rates turn out to be lower (higher) than implied by the forward curve, the forward price will increase (decrease). A trader expecting lower future spot rates (then implied by the current forward rates) would purchase the forward contract to profit from its appreciation.\nFor a bond investor, the return on a bond over a one-year horizon is always equal to the one-year risk-free rate if the future spot rates evolve as predicted by today’s forward curve. If the spot curve one year from today is not the same as that predicted by today’s forward curve, the return over the one-year period will differ, with the return depending on the bond’s maturity.\nAn active portfolio manager will try to outperform the overall bond market by predicting how the future spot rates will differ from those predicted by the current forward curve.\nIf an investor believes that future spot rates will be lower than corresponding forward rates, then she will purchase bonds (at a presumably attractive price) because the market appears to be discounting future cash flows at “too high” of a discount rate.\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#riding-the-yield-curve-ytm-curve",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#riding-the-yield-curve-ytm-curve",
    "title": "CFA Level 2 Fixed Income",
    "section": "Riding the yield curve (YTM curve)",
    "text": "Riding the yield curve (YTM curve)\n\n\n\nA picture containing diagram Description automatically generated\n\n\nWith an upward-sloping interest rate term structure, investors seeking superior returns may pursue a strategy called riding the yield curve (also known as rolling down the yield curve). Under this strategy, an investor will purchase bonds with maturities longer than his investment horizon. In an upward-sloping yield curve, shorter maturity bonds have lower yields than longer maturity bonds. As the bond approaches maturity, it is valued using successively lower yields and, therefore, at successively higher prices.\nIf the yield curve remains unchanged over the investment horizon, riding the yield curve strategy will produce higher returns than a simple maturity matching strategy, increasing the total return of a bond portfolio. The greater the difference between the forward rate and the spot rate, and the longer the maturity of the bond, the higher the total return. (shoulder effect)"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-swap-rate-curve",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-swap-rate-curve",
    "title": "CFA Level 2 Fixed Income",
    "section": "The swap rate curve",
    "text": "The swap rate curve\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nIn a plain vanilla interest rate swap, one party makes payments based on a fixed rate while the counterparty makes payments based on a floating rate. The fixed rate in an interest rate swap is called the swap fixed rate or swap rate."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#swap-spread",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#swap-spread",
    "title": "CFA Level 2 Fixed Income",
    "section": "Swap spread",
    "text": "Swap spread\nSwap spread refers to the amount by which the swap rate exceeds the yield of a government bond with the same maturity\nSwap spread = SFR – treasury yield\nSwap spread는 banking industry의 credit risk를 반영\nSwap spreads are almost always positive, reflecting the lower credit risk of governments compared to the credit risk of surveyed banks that determines the swap rate."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#i-spread",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#i-spread",
    "title": "CFA Level 2 Fixed Income",
    "section": "I-spread",
    "text": "I-spread\n\n\n\nDiagram Description automatically generated\n\n\nThe I-spread for a credit-risky bond is the amount by which the yield on the risky bond exceeds the swap rate for the same maturity. The missing swap rate can be estimated from the swap rate curve using linear interpolation.\nI-spread = credit-risky bond yield – SFR\nWhile a bond’s yield reflects time value as well as compensation for credit and liquidity risk, I-spread only reflects compensation for credit and liquidity risks. The higher the I-spread, the higher the compensation for liquidity and credit risk."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-z-spread",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#the-z-spread",
    "title": "CFA Level 2 Fixed Income",
    "section": "The Z-spread",
    "text": "The Z-spread\n\n\n\nA picture containing schematic Description automatically generated\n\n\nThe Z-spread is the spread that, when added to each spot rate on the default-free spot curve, makes the present value of a bond’s cash flows equal to the risky bond’s market price. Therefore, the Z-spread is a spread over the entire spot rate curve.\nThe term zero volatility in the Z-spread refers to the assumption of zero interest rate volatility. Z-spread is not appropriate to use to value bonds with embedded options."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#ted-spread",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#ted-spread",
    "title": "CFA Level 2 Fixed Income",
    "section": "TED spread",
    "text": "TED spread\nThe TED in TED spread is an acronym that combines the T in T-bill with ED, the ticker symbol for the Eurodollar futures contract.\nTED spread = 3-month LIBOR rate – 3-month T-bill rate\nBecause T-bill are considered to risk free while LIBOR reflects the risk of lending to commercial banks, the TED spread is seen as an indication of the risk of interbank loans. A rising TED spread indicates that market participants believe banks are increasingly likely to default on loans and that risk-free T-bill are becoming more valuable in comparison. The TED spread captures the risk in the banking system more accurately than does the 10-year swap spread.\nSFR은 long-term rate"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#ois-spread",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#ois-spread",
    "title": "CFA Level 2 Fixed Income",
    "section": "OIS spread",
    "text": "OIS spread\nOIS stands for overnight indexed swap. The OIS rate roughly reflects the federal funds rate and includes minimal counterparty risk. 거의 credit risk 없음\nThe OIS spread is the amount by which the LIBOR rate which includes credit risk exceeds the OIS rate which includes only minimal credit risk. This makes the OIS spread a useful measure of credit risk and an indication of the overall wellbeing of the banking system. Credit risk도 포함되어 있지만, OIS spread는 liquidity를 더 잘 반영함\nA low OIS spread is a sign of high market liquidity while a high OIS spread is a sign of high market liquidity while a high OIS spread is a sign that banks are unwilling to lend due to concerns about creditworthiness.\n낮을수록 High liquidity, 높을수록 low liquidity"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#unbiased-expectation-theory",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#unbiased-expectation-theory",
    "title": "CFA Level 2 Fixed Income",
    "section": "Unbiased expectation theory",
    "text": "Unbiased expectation theory\nPure expectation theory\nWe hypothesize that it is investors’ expectations that determine the shape of the interest rate term structure.\nSpecifically, this theory suggests that forward rates are solely a function of expected future spot rates, and the every maturity strategy has the same expectation return over a given investment horizon. In other words, long-term interest rates equal the mean of future expected short-term rates. This implies that an investor should earn the same return by investing in a five-year bond or by investing in a three-year bond and then a two-year bond after the three-year bond matures. Similarly, an investor with a three-year investment horizon would be indifferent between investing in a three-year bond or in a five-year bond that will be sold two years prior to maturity. The underlying principle behind the pure expectations theory is risk neutrality.\nThe implications for the shape of the yield curve under the pure expectations theory are:\n- if the yield curve is upward sloping, short-term rates are expected to rise.\n- if the curve is downward sloping, short-term rates are expected to fall.\n- A flat yield curve implies that the market expects short-term rates to remain constant.\n\nLocal expectations theory\nThe local expectations theory preserves the risk-neutrality assumption only for short-term holding periods."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#liquidity-preference-theory",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#liquidity-preference-theory",
    "title": "CFA Level 2 Fixed Income",
    "section": "Liquidity preference theory",
    "text": "Liquidity preference theory\nThe liquidity preference theory of the term structure addresses the shortcomings of the pure expectations theory by proposing that forward rates reflects investors’ expectations of future spot rates, plus a liquidity premium to compensate investors for exposure to interest rate risk. The theory suggests that this liquidity premium is positively related to maturity.\nThe liquidity preference theory states that forward rates are upward-biased estimates of the market’s expectation of future rates because they include a liquidity premium. Therefore, a positive-sloping yield curve may indicate that either: (1) the market expects future interest rates to rise or (2) rates are expected to remain constant (or even fall), but the addition of the liquidity premium results in a positive slope.\n기대가 어떻든, liquidity premium이 있으니까 positive-sloping"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#segmented-markets-theory",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#segmented-markets-theory",
    "title": "CFA Level 2 Fixed Income",
    "section": "Segmented markets theory",
    "text": "Segmented markets theory\nThe shape of the yield curve is determined by the preferences of borrowers and lenders, which drives the balance between supply of and demand for loans of different maturities. This is called the segmented markets theory because the theory suggests that the yield at each maturity is determined independently of the yields at other maturities; we can think of each maturity to be essentially unrelated to other maturities.\nThe segmented markets theory supposes that various market participants only deal in securities of a particular maturity because they are prevented from operating at different maturities."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#preferred-habitat-theory",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#preferred-habitat-theory",
    "title": "CFA Level 2 Fixed Income",
    "section": "Preferred habitat theory",
    "text": "Preferred habitat theory\nThe preferred habitat theory also proposes that forward rates represent expected future spot rates plus a premium, but it does not support the view that this premium is directly related to maturity.\nPremiums are related to supply and demand for funds at various maturities."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#modern-term-structure-models",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#modern-term-structure-models",
    "title": "CFA Level 2 Fixed Income",
    "section": "Modern term structure models",
    "text": "Modern term structure models\nModern interest rate term structure models attempt to capture the statistical properties of interest rates movements and provide us with quantitatively precise descriptions of how interest rates will change.\n\nEquilibrium term structure models.\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nTo-be model\nEquilibrium term structure models attempt to describe changes in the term structure through the use of fundamental economic variables that drive interest rates.\nCIR model, Vasicek model – single factor model\nCIR model, Vasicek model 둘 다 drift term, random component로 이루어져있고, Vasicek model은 random component에서 interest rate의 차수가 0, CIR model은 차수가 0.5이다.\nCIR model은 negative interest rate을 방지하고, 현재의 금리수준이 금리의 변동성에 영향을 주는 반면, Vasicek model은 negative interest rate이 가능하고, 현재의 금리수준이 금리의 변동성에 영향을 주지 않는다.\nDrift term에서 a는 회귀상수, 장기평균으로 회귀하는 속도를 의미\nb는 장기평균, r은 연재 금리수준을 의미.\n\n\nArbitrage-free models\nArbitrage-free models of the term structure of interest rates begin with the assumption that bonds trading in the market are correctly priced, and the model is calibrated to value such bonds consistent with their market price. These models do not try to justify the current yield curve; rather, they take this curve as given.\nAs-is model\nCalibration – parameter를 추정하는 과정\nBenchmark bond를 대상으로 설명하고 있음"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#managing-yield-curve-risks",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#managing-yield-curve-risks",
    "title": "CFA Level 2 Fixed Income",
    "section": "Managing yield curve risks",
    "text": "Managing yield curve risks\nYield curve risk refers to risk to the value of a bond portfolio due to unexpected changes in the yield curve."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#effective-duration",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#effective-duration",
    "title": "CFA Level 2 Fixed Income",
    "section": "Effective duration",
    "text": "Effective duration\n\n\n\nMap Description automatically generated\n\n\n\n\n\nText Description automatically generated\n\n\nEffective duration은 yield curve가 parallel shift 할 때만 의미있음\nNon-parallel shift 하는 경우를 shaping risk라고 부름"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#key-rate-duration",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#key-rate-duration",
    "title": "CFA Level 2 Fixed Income",
    "section": "Key rate duration",
    "text": "Key rate duration\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nKey rate duration is the sensitivity of the value of a security (or a bond portfolio) to changes in a single pare rate, holding all other spot rates constant. In other words, key rate duration isolate price sensitivity to a change in the yield at a particular maturity only.\nEvery security or portfolio has a set of key rate durations-one for each key rate.\nKey rate duration은 key rate으로 설정한 해당 금리의 변화에 따라 가격이 얼마나 변하는지 나타내는 민감도이다. 만약 해당 금리의 변화가 모두 같으면, Key rate duration들의 합은 effective duration과 같다."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#sensitivity-to-parallel-steepness-and-curvature-movements",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#sensitivity-to-parallel-steepness-and-curvature-movements",
    "title": "CFA Level 2 Fixed Income",
    "section": "Sensitivity to parallel, steepness, and curvature movements",
    "text": "Sensitivity to parallel, steepness, and curvature movements\n\n\n\nShape, polygon Description automatically generated with medium confidence\n\n\nLevel – a parallel increase or decrease of interest rates\nSteepness – long-term interest rates increase while short-term rates decrease\nCurvature – increasing curvature means short- and long-term interest rates increase while intermediate rates do not change."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#binomial-tree",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#binomial-tree",
    "title": "CFA Level 2 Fixed Income",
    "section": "Binomial tree",
    "text": "Binomial tree\n\n\n\nDiagram, schematic Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#three-rules",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#three-rules",
    "title": "CFA Level 2 Fixed Income",
    "section": "Three rules",
    "text": "Three rules\n\\1. The interest rate tree should generate arbitrage-free values for the benchmark security. (fair pricing assumption) This means that the value of bonds produced by the interest rate tree must be equal to their market price, which excludes arbitrage opportunities.\n\\2. Adjacent forward rates (for the same period) are two standard deviations apart.\n\\3. The middle forward rate (or mid-point in case of even number of rates) in a period is approximately equal to the implied (from the benchmark spot rate curve) one-period forward rate for that period."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#path-dependency",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#path-dependency",
    "title": "CFA Level 2 Fixed Income",
    "section": "Path dependency",
    "text": "Path dependency\nUnlike call risk, prepayment risk is affected not only by the level of interest rate at a particular point in time, but also by the path rates took to get there.\nAn important assumption of the binomial valuation process is that the value of the cash flows at a given point in time is independent of the path that interest rates followed up to that point. In other words, cash flows are not path dependent; cash flows at any node do not depend on the path rates took to get to that node. Because of path dependency of cash flows of mortgage-backed securities, the binomial tree backward induction process cannot be used to value such securities. We instead use the Monte Carlo simulation method to value mortgage-backed securities.\nA monte Carlo forward-rate simulation involves randomly generating a large number of interest rate paths, using a model that incorporates a volatility assumption and assumed probability distribution. A key feature of the Monte Carlo method is that the underlying cash flows can be path dependent.\nAs with pathwise valuation, the value of the bond is the average of values from the various paths. The simulated paths should be calibrated so benchmark interest rate paths value benchmark securities at their market price (i.e. arbitrage-free valuation). The calibration process entails adding (subtracting) a constant to all rates when the value obtained from the simulated paths is too high (too low) relative to market prices. This calibration process results in a drift adjusted model.\nA Monte Carlo simulation may impose upper and lower bounds on interest rates as part of the model generating the simulated paths. These bounds are based on the notion of mean reversion; rates tend to rise when they are too low and fall when they are too high.\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#style",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#style",
    "title": "CFA Level 2 Fixed Income",
    "section": "Style",
    "text": "Style\nEuropean style – whereby the option can only be exercised on a single day immediately after the lockout period\nAmerican style – whereby the option can be exercised at any time after the lockout period\nBermudan-style – whereby the option can be exercised at fixed dates after the lockout period"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#complex-options",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#complex-options",
    "title": "CFA Level 2 Fixed Income",
    "section": "Complex options",
    "text": "Complex options\n- Estate put\n- Sinking fund bonds\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#level-of-interest-rates",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#level-of-interest-rates",
    "title": "CFA Level 2 Fixed Income",
    "section": "Level of interest rates",
    "text": "Level of interest rates\nAs interest rates decline, the value of a callable bond rises less rapidly than the value of an otherwise-equivalent straight bond. As interest rates increase, the value of a putable bond falls less rapidly than the value of an otherwise-equivalent straight bond.\nCall option on a bond value is inversely related to the level of interest rates, while put option on a bond value varies directly with the level of interest rates."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#shape-of-the-yield-curve",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#shape-of-the-yield-curve",
    "title": "CFA Level 2 Fixed Income",
    "section": "Shape of the yield curve",
    "text": "Shape of the yield curve\nAs an upward-sloping yield curve becomes flatter, the call option value increases. Put option value therefore declines as an upward-sloping yield curve flattens."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#one-sided-durations",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#one-sided-durations",
    "title": "CFA Level 2 Fixed Income",
    "section": "One-sided durations",
    "text": "One-sided durations\nFor bonds with embedded options, one-sided durations-durations that apply only when interest rates rise (or, alternatively, only when rates fall)-are better at capturing interest rate sensitivity than simple effective duration. When the underlying option is at-the-money (or near-the-money), callable bonds will have lower one-sided down-duration than one-sided up-duration: the price change of a callable when rates fall is smaller than the price change for an equal increase in rates. Conversely, a near-the-money putable bond will have larger one-sided down-duration than one-sided up-duration."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#key-rate-duration-1",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#key-rate-duration-1",
    "title": "CFA Level 2 Fixed Income",
    "section": "Key rate duration",
    "text": "Key rate duration\nKey rate duration or partial durations capture the interest rate sensitivity of a bond to changes in yields (par rates) of specific benchmark maturities. Key rate duration is used to identify the interest rate risk from changes in the shape of the yield curve (shaping risk).\n\nGeneralization\n\\1. If an option-free bond is trading at par, the bond’s maturity-matched rate is the only rate that affects the bond’s value. Its maturity key rate duration is the same as its effective duration, and all other rate durations are zero.\n\\2. For an option-free bond not trading at par, the maturity-matched rate is still the most important rate.\n\\3. A bond with low (or zero) coupon rate may have negative key rate durations for horizons other than the bond’s maturity.\n\\4. A callable bond with a low coupon rate is unlikely to be called; hence, the bond’s maturity-matched rate is the most critical rate.\n\\5. All else equal, higher coupon bonds are more likely to be called, and therefore the time-to-exercise rate will tend to dominate the time-to-maturity rate.\n\\6. Putable bonds with high coupon rates are unlikely to be put, and thus are most sensitive to their maturity-matched rates.\n\\7. All else equal, lower coupon bonds are more likely to be put, and therefore the time-to-exercise rate will tend to dominate the time-to-maturity rate.\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#bond-analytics",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#bond-analytics",
    "title": "CFA Level 2 Fixed Income",
    "section": "Bond analytics",
    "text": "Bond analytics\n\\1. Put-call parity\no C – P = PV(forward price of the bond on exercise date) – PV(exercise price)\n\\2. Option-free bond pricing\no The valuation of option-free bonds should be independent of the assumed level of volatility used to generate the interest rate tree."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#single-name-cds",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#single-name-cds",
    "title": "CFA Level 2 Fixed Income",
    "section": "Single-name CDS",
    "text": "Single-name CDS\nIn the case of a single-name CDS, the reference obligation is the fixed-income security on which the swap is written, usually a senior unsecured obligation (in the case of a senior CDS). The issuer of the reference obligation is called the reference entity. The CDS pays off not only when the reference entity defaults on any other issue that is ranked pari passu or higher. The CDS payoff is based on the market value of the cheapest-to-deliver (CTD) bond that has the same seniority as the reference obligation.\nContract에 따라 다르지만, credit event의 reference entity\nCoverage reference obligation 기준은 same or higher, payoff는 CTD bond (same rank)"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#index-cds",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#index-cds",
    "title": "CFA Level 2 Fixed Income",
    "section": "Index CDS",
    "text": "Index CDS\nAn index CDS covers multiple issuers.\nThe pricing of an index CDS is dependent on the correlation of default (credit correlation) among the entities in the index. The higher the correlation of default among index constituents, the higher the spread on the index CDS."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#credit-event",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#credit-event",
    "title": "CFA Level 2 Fixed Income",
    "section": "Credit event",
    "text": "Credit event\n\\1. Bankruptcy on the refence entity\n\\2. Failure to pay all the outstanding debts\n\\3. Restructuring\n\\4. Cross default\n\\5. Obligation acceleration\n\\6. Moratorium\n\\7. Repudiation\nA 15-member group of the ISDA called the Determinations Committee (DC) declares when a credit event has occurred.\nWhen there is a credit event, the swap will be settled in cash or by physical delivery. With physical delivery, the protection seller receives the reference obligation and pays the protection buyer the notional amount.\nIn the case of a cash settlement, the payout amount is the payout ratio times the notional principal. The payout ratio depends on the recovery rate.\nPayout amount = payout ratio * notional principal\nPayout ratio = 1 – recovery rate (%)"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#valuation-after-inception-of-cds",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#valuation-after-inception-of-cds",
    "title": "CFA Level 2 Fixed Income",
    "section": "Valuation after inception of CDS",
    "text": "Valuation after inception of CDS\nAt inception of a CDS, the CDS spread (and the upfront premium) is computed based on the credit quality of the reference entity. After inception, the credit quality of the reference entity (or the credit risk premium in the overall market) may change. This will lead to the underlying CDS having a nonzero value.\nThe change in value of a CDS after inception can be approximated by the change in spread multiplied by the duration of the CDS.\nProfit for protection buyer = change in spread * duration * notional principal\nProfit for protection buyer (%) = change in spread (%) * duration\nThe protection buyer (or seller) can unwind an existing CDS exposure (prior to expiration or default) by entering into an offsetting transaction. This process of capturing value from an in-the-money CDS exposure is called monetizing the gain.\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#credit-curve",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#credit-curve",
    "title": "CFA Level 2 Fixed Income",
    "section": "Credit curve",
    "text": "Credit curve\nThe credit curve is the relationship between credit spreads for different bonds issued by an entity, and the bonds’ maturities. The credit curve is similar to the term structure of interest rates. If the longer maturity bonds have a higher credit spread compared to shorter maturity bonds, the credit curve will be upward sloping."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#naked-cds",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#naked-cds",
    "title": "CFA Level 2 Fixed Income",
    "section": "Naked CDS",
    "text": "Naked CDS\nCDS can be used to manage credit exposures of a bond portfolio. In a naked CDS, an investor with no underlying exposure purchases protection in the CDS market."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#longshort-trade",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#longshort-trade",
    "title": "CFA Level 2 Fixed Income",
    "section": "Long/Short trade",
    "text": "Long/Short trade\nIn a long / short trade, an investor purchases protection on one reference entity while simultaneously selling protection on another (often related) reference entity. The investor is betting that the difference in credit spreads between the two reference entities will change to the investor’s advantage. This is similar to going long (protection seller exposure) in one reference entity bond and simultaneously going short (protection buyer exposure) in the other reference entity bond.\n\n\n\nDiagram Description automatically generated with medium confidence"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#curve-trade",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#curve-trade",
    "title": "CFA Level 2 Fixed Income",
    "section": "Curve trade",
    "text": "Curve trade\nA curve trade is a type of long/short trade where the investor is buying and selling protection on the same reference entity but with a different maturity.\nAn investor concerned about the credit risk of an issuer in the near term while being more confident of the long-term prospects of the issuer might buy protection in the short-term CDS and offset the premium cost by selling protection in the long-term CDS. Conversely, an investor who is bearish about the reference entity’s prospects in the short term will enter into a curve-flattening trade.\n\n\n\nDiagram Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#basis-trade",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#basis-trade",
    "title": "CFA Level 2 Fixed Income",
    "section": "Basis trade",
    "text": "Basis trade\nA basis trade is an attempt to exploit the difference in credit spreads between bond markets and the CDS market.\nAnother arbitrage transaction involves buying and selling debt instruments issued by the same entity based on which instruments the CDS market suggests to be undervalued or overvalued.\n\n\n\nDiagram Description automatically generated with medium confidence"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#lbo",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#lbo",
    "title": "CFA Level 2 Fixed Income",
    "section": "LBO",
    "text": "LBO\nIn a leveraged buyout (LBO), the firm will issue a great amount of debt in order to repurchase all of the company’s publicly traded equity. This additional debt will increase the CDS spread because default is now more likely. An investor who anticipates an LBO might purchase both the stock and CDS protection both of which will increase in value when the LBO eventually occurs."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#index-cds-1",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#index-cds-1",
    "title": "CFA Level 2 Fixed Income",
    "section": "Index CDS",
    "text": "Index CDS\nIn the case of an index CDS, the value of the index should be equal to the sum of the values of the index components. An arbitrage transaction is possible if the credit risk of the index constituents is priced differently than the index CDS spread."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#cdo",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#cdo",
    "title": "CFA Level 2 Fixed Income",
    "section": "CDO",
    "text": "CDO\nCollateralized debt obligation (CDO) are claims against a portfolio of debt securities. A synthetic CDO has similar credit risk exposure to that of a cash CDO but is assembled using CDS rather than debt securities. If the synthetic CDO can be created at a cost lower than that of the cash CDO, investors can buy the synthetic CDO and sell the cash CDO, engaging in a profitable arbitrage.\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#risk-neutral-probability-of-default",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#risk-neutral-probability-of-default",
    "title": "CFA Level 2 Fixed Income",
    "section": "Risk neutral probability of default",
    "text": "Risk neutral probability of default\nRisk neutral probability of default, which is the probability of default implied in the current market price.\nWe can also calculate the implied recovery rate in the market price given the probability of default. In general, given the market price (and hence the credit spread), the estimated risk neutral probabilities of default and recovery rates are positively correlated."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#relative-credit-risk-analysis",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#relative-credit-risk-analysis",
    "title": "CFA Level 2 Fixed Income",
    "section": "Relative credit risk analysis",
    "text": "Relative credit risk analysis\nWhile comparing the credit risk of several bonds, the metric that combines the probability of default as well as loss severity is the expected loss. Everything else constant, for a given period, the higher the expected loss, the higher the credit risk."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#structural-models",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#structural-models",
    "title": "CFA Level 2 Fixed Income",
    "section": "Structural models",
    "text": "Structural models\nStructural models of corporate credit risk are based on the structure of a company’s balance sheet and rely on insights provided by option pricing theory.\n\nOption analogy\nConsider a hypothetical company with assets that are financed by equity and a single issue of zero-coupon debt.\nDue to the limited liability nature of corporate equity, the shareholders effectively have a call option on the company’s assets with a strike price equal to the face value of debt.\nAn alternate, but related interpretation considers equity investors as long the net assets of the company and long a put option, allowing them to seel the assets at an exercise price of X. default is then synonymous to exercising the put opition.\nUnder the put option analogy, the investors in risky debt can be constructed to have a long position in risk-free debt and a short position in that put option.\nValue of risky debt = value of risk-free debt – value of put option\nValue of risky debt = value of risk-free debt – CVA\nThe value of the put option = CVA\nIf the value of assets falls below the default barrier X, the company defaults. The probability of default is indicated by the region in the left tail below the default barrier of X.\nAdvantages of structural models\n- Structural models provide an economic rationale for default and explain why default occurs.\n- Structural models utilize option pricing models to value risky debt.\nDisadvantages of structural models\n- Because structural models assume a simple balance sheet structure, complex balance sheets cannot be modeled. Additionally, when companies have off-balance sheet debt, the default barrier under structural models (X) would be inaccurate and hence the estimate outputs of the model will be inaccurate.\n- One of the key assumptions of the structural model is that the assets of the company are traded in the market. This restrictive assumption makes the structural model impractical.\n\n\n\nChart Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#reduced-form-models",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#reduced-form-models",
    "title": "CFA Level 2 Fixed Income",
    "section": "Reduced form models",
    "text": "Reduced form models\nReduced form (RF) models do not rely on the structure of a company’s balance sheet and therefore do not assume that the assets of the company trade. Unlike the structural model, reduce form models do not explain why default occurs. Instead, they statistically model when default occurs. Default under the RF model is a randomly occurring exogenous variable.\nA key input into the RF model is the default intensity, which is the probability of default over the next (small) time period. Default intensity can be estimated using regression models.\nAdvantages of reduced form models\n- RF models do not assume that the assets of a company trade\n- Default intensity is allowed to vary as company fundamentals changes, as well as when the state of the economy changes.\nDisadvantages of reduced form models\n- RF models do not explain why default occurs.\n- Under RF models, default is treated as a random event, but in reality, default is rarely a surprise."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#determinants-of-term-structure-of-credit-spreads",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#determinants-of-term-structure-of-credit-spreads",
    "title": "CFA Level 2 Fixed Income",
    "section": "Determinants of term structure of credit spreads",
    "text": "Determinants of term structure of credit spreads\n\\1. Credit quality – lower-rated sectors tend to have steeper spread curves, reflecting greater uncertainty as well as greater sensitivity to the business cycle.\n\\2. Financial conditions – spreads narrow during economic expansions and widen during cyclical downturns. During boom times, benchmark yields tend to be higher while credit spreads tend to be narrower.\n\\3. Market demand and supply – recall that a credit spread includes a premium for lack of liquidity. Hence, less liquid maturities would show higher spreads (even if the expectations for that time period are stable). Due to low liquidity in most corporate issues, the credit curves are most heavily influenced by more heavily traded bonds.\n\\4. Equity market volatility – increases in equity volatility therefore tend to widen spreads and influence the shape of the credit spread curve."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#components-of-credit-analysis-of-secured-debt",
    "href": "posts/2021-09-12-CFA-Level-2-Fixed-Income/2021-09-12-CFA-Level-2-Fixed-Income.html#components-of-credit-analysis-of-secured-debt",
    "title": "CFA Level 2 Fixed Income",
    "section": "Components of credit analysis of secured debt",
    "text": "Components of credit analysis of secured debt\n\nCollateral pool\nCredit analysis of structured, securitized debt begins with the collateral pool. Homogeneity of a pool refers to the similarity of the assets within the collateral pool. Granularity refers to the transparency of assets within the pool. A highly granular pool would have hundreds of clearly defined loans, allowing for use of summary statistics as opposed to investigating each borrower.\nShort-term granular and homogeneous structured finance vehicles are evaluated using a statistical-based approach. Medium-term granular and homogeneous obligations are evaluated using a portfolio-based approach because the portfolio composition varies over time. (segmentation해서 analysis) discrete and non-granular portfolios have to be evaluated at the individual loan level.\n\n\nServicer quality\nServicer quality is important to evaluate the ability of the servicer to manage the origination and servicing of the collateral pool.\n\n\nStructure\nStructure determines the tranching or other management of credit and other risks in a collateral pool.\nInternal / external credit enhancement\nA special structure is the case of covered bond. – loan을 true sale하지 않고, 담보로 structuring, 해당 loan들은 balance sheet에서 segregated\nIssued by a financial institution, covered bonds are senior, secured bonds backed by a collateral pool as well as by the issuer."
  },
  {
    "objectID": "posts/2021-12-05-system-programming-week-12/2021-12-05-system-programming-week-12.html",
    "href": "posts/2021-12-05-system-programming-week-12/2021-12-05-system-programming-week-12.html",
    "title": "System Programming Week 12",
    "section": "",
    "text": "image-20211205031612545\n\n\nexecutable object file은 disk에 있고, CPU는 단지 sequential하게 instruction을 실행할 뿐\n\n\n\n\n\n\nimage-20211205031623976\n\n\nprogram의 의지에 따라서 flow를 control하는 방법들이 있음 - jumps, branches, call, return\n그러나 이런 방법 만으로는 제어에 한계가 있음 - system의 상태가 변해도 program을 여전히 제어 할 수 있어야 함\n\n\n\n\n\n\nimage-20211205031636463\n\n\n\n\n\nimage-20211205032019933\n\n\n\n\n\n\n\n\nimage-20211205032204555\n\n\n\n\n\nimage-20211205032213282\n\n\n\n\n\n\n\n\nimage-20211205032338677\n\n\n\n\n\n\n\n\nimage-20211205032409230\n\n\nException은 일단 문제가 생긴 것, 일이 생긴거임 - 실행하는 일이랑 별로 상관없는 문제가 asynchronous exception\n\n\n\n\n\n\nimage-20211205032644309\n\n\n\n\n\nimage-20211205032823075\n\n\ntraps - 의도적으로 system에 요청하는 것\nfaults - 뭘 잘못했는지 모르겠는데 system state를 바꾼 이벤트, architecture dependent함\nabort - 누가봐도 심각한 오류\n\n\n\n\n\n\nimage-20211205032837793\n\n\n\n\n\n\n\n\nimage-20211205033030378\n\n\n\n\n\n\n\n\nimage-20211205032659505\n\n\n\n\n\n\n\n\nimage-20211205033220557\n\n\n\n\n\n\n\n\nimage-20211205033248878\n\n\n\n\n\n\n\n\nimage-20211205033406882\n\n\n\n\n\n\n\n\nimage-20211205033416309\n\n\n\n\n\nimage-20211205033430148\n\n\n\n\n\n\n\n\nimage-20211205033632959\n\n\n\n\n\n\n\n\nimage-20211205033700330\n\n\n\n\n\n\n\n\nimage-20211205033729557\n\n\n\n\n\nimage-20211205033940070\n\n\n\n\n\nimage-20211205033945862\n\n\n\n\n\nimage-20211205033953376\n\n\n\n\n\n\n\n\nimage-20211205034038132\n\n\n\n\n\nimage-20211205034044931\n\n\n\n\n\n\n\n\nimage-20211205034150488\n\n\n\n\n\n\n\n\nimage-20211205034207252\n\n\n\n\n\nimage-20211205034220350\n\n\n\n\n\n\n\n\nimage-20211205034316526\n\n\n\n\n\n\n\n\nimage-20211205034451144\n\n\n\n\n\n\n\n\nimage-20211205034509632\n\n\n\n\n\n\n\n\nimage-20211205034525676\n\n\n\n\n\n\n\n\nimage-20211205034555870\n\n\n\n\n\n\n\n\nimage-20211205034648743\n\n\n\n\n\n\n\n\nimage-20211205034845599\n\n\nshell도 하나의 program\n\n\n\n\n\n\nimage-20211205035111223\n\n\n\n안 봐도 되지만\n\n\n\n\n\n\nimage-20211205035332355\n\n\n\n\n\n\n\n\nimage-20211205035344789\n\n\n\n\n\n\n\n\nimage-20211205035405638\n\n\n\n\n\n\n\n\nimage-20211205035428141\n\n\n\n\n\n\n\n\n\nimage-20211205035444942\n\n\n\n\n\n\n\n\nimage-20211205035702885\n\n\n\n\n\n\n\n\nimage-20211205035815615\n\n\n\n\n\n\n\n\nimage-20211205035835451\n\n\n\n\n\n\n\n\nimage-20211205035930309\n\n\n\n\n\n\n\n\nimage-20211205040012770\n\n\n\n\n\n\n\n\nimage-20211205040101087\n\n\n\n\n\n\n\n\nimage-20211205040238974\n\n\n\n\n\n\n\n\nimage-20211205040623546\n\n\n\n\n\n\n\n\nimage-20211205040608857\n\n\n\n\n\nimage-20211205040824049"
  },
  {
    "objectID": "posts/2021-10-23-system-programming-week-7/2021-10-23-system-programming-week-7.html",
    "href": "posts/2021-10-23-system-programming-week-7/2021-10-23-system-programming-week-7.html",
    "title": "System Programming Week 7",
    "section": "",
    "text": "image-20211023220301091\n\n\n이런 주소들은 사실 가상 메모리 주소\n\n\n\nimage-20211023220633873\n\n\n\n\n\nimage-20211023220710523\n\n\n\n\n\n\n\n\nimage-20211023220819305\n\n\n\n\n\nimage-20211023221014528\n\n\n\n\n\n\n\nimage-20211023221327278\n\n\n\n\n\n\n\n\nimage-20211023221442145\n\n\n\n\n\n\n\n\nimage-20211023221721448\n\n\n\n\n\n\n\n\nimage-20211023221812079\n\n\n\n\n\n\n\n\nimage-20211023221849264\n\n\n\n\n\nimage-20211023222129261\n\n\n\n\n\nimage-20211023222138610\n\n\n00은 문자열의 끝을 표시\n\n\n\nimage-20211023222145796\n\n\n\n\n\nimage-20211023222201006\n\n\n\n\n\nimage-20211023222326923\n\n\n\n\n\n\n\n\nimage-20211023222345731\n\n\n\n\n\n\n\n\nimage-20211023222531305\n\n\n\n\n\n\n\n\nimage-20211023222647027\n\n\n\n\n\n\n\n\nimage-20211023222748812\n\n\n\n\n\nimage-20211023222905522\n\n\n\n\n\n\n\n\nimage-20211023223020111\n\n\n\n\n\n\n\n\n\n\n\nimage-20211023223131350\n\n\n\n\n\n\n\n\nimage-20211023223159861\n\n\n\n\n\nimage-20211023223329811\n\n\n\n\n\n\n\n\nimage-20211023223446845\n\n\n\n\n\nimage-20211023223621936\n\n\n\n\n\n\n\nimage-20211023223636524\n\n\n\n\n\nimage-20211023223817021\n\n\n\n\n\n\n\n\n\nimage-20211023223827063\n\n\n\n\n\n\n\n\n\nimage-20211023224310021\n\n\n\n\n\n1. 1단계 문제\n\n문제 기술\n\n함수가 실행될 때 메모리는 정해진 역할에 따라 구획을 나눈다. 이 때 return address를 담을 공간을 할당하고 padding을 두고 local data를 쌓을 stack의 시작점을 지정한다. 함수가 실행되면서 local data는 쌓이고, stack은 return address를 향해 자란다. (작은 주소에서 큰 주소로 자란다) 그러나 이 padding보다 큰 길이의 local data가 쌓일 경우 다른 용도로 할당된 메모리 공간(return address가 담길 자리)을 침범하면서 segmentation fault, buffer overflow가 발생한다.\n이번 문제는 stack에 할당된 padding보다 긴 데이터를 받아 buffer overflow를 발생시키고, 그 데이터에 실행하고자 하는 함수(touch 1)의 주소를 담아 return address를 담기로 할당했던 메모리에 해당 함수의 memory address를 담아 실행하는 문제이다.\n\n해답\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80002.bmp 원본 그림의 크기: 가로 679pixel, 세로 183pixel\n\n\n\n분석\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80003.bmp 원본 그림의 크기: 가로 580pixel, 세로 132pixel\n\n\nobjdump를 통해 assembly 코드를 확인해본 결과 %rsp에 24(0x18)bytes만큼의 padding을 설정하고(sub $0x18, %rsp), %rdi를 %rsp에 할당해주고, Gets 함수를 호출한다. 따라서 %rdi에 24bytes 만큼의 stack이 할당됐고, buffer overflow를 발생시키기 위해서는 24bytes 초과의 dummy 데이터를 Gets를 통해 전달하면 된다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80011.bmp 원본 그림의 크기: 가로 734pixel, 세로 165pixel\n\n\ntouch1 함수를 실행하는 것이 목적이므로 return address 담는 %rsp에 touch1 함수의 memory address(00000000004017e6)를 주면 된다. 따라서 24bytes 크기의 dummy data로 padding을 가득 채우고, 이후 touch 1의 memory address를 삽입하여 데이터를 구성했다.\nGets 함수는 해당 데이터를 읽으면서 padding을 dummy data로 가득 채우게 되고, %rsp(return address가 담길 공간)을 침범하여 해당 공간에 touch1 함수의 memory address를 담게 된다. 따라서 getbuf 함수가 값을 return할 때 %rsp를 참조하게 되면서 touch1 함수가 실행된다.\ndummy data는 00 24개로 구성했고, little endian을 감안하여 dummy data 이후 줄 바꿈을 하고 memory address를 역순으로 삽입했다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80002.bmp 원본 그림의 크기: 가로 679pixel, 세로 183pixel\n\n\n\n실행결과\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80002.bmp 원본 그림의 크기: 가로 679pixel, 세로 183pixel\n\n\n2. 2단계 문제\n\n문제 기술\n\n1단계와 마찬가지로 buffer overflow를 통해 원하는 코드를 실행하여 결과를 얻는 문제다. 차이는 바로 원하는 코드를 바로 실행하는 것이 아니라 중간 작업을 하고 난 뒤에 함수를 실행하는 것이다. buffer overflow를 통해 중간 작업(comparison 통과를 위한 값을 cookie로 할당)을 수행하고, touch2를 실행하여 comparison 연산을 통과하는 것이다.\n\n해답\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000c.bmp 원본 그림의 크기: 가로 571pixel, 세로 49pixel\n\n\n\n분석\n\n1단계는 return 받는 값을 내가 원하는 함수의 주소로 설정해서 해당 함수를 실행하는 것이라면, 2단계는 return 받는 값을 내가 원하는 함수를 실행하기 전에 해야 작업을 하는 주소로 설정해서 해당 작업을 끝내면 내가 원하는 함수가 실행하도록 하는 문제다.\n1단계: buffer overflow -> touch1 실행\n2단계: buffer overflow -> 작업 -> touch2 실행\n해야 하는 작업은 touch2에서 진행되는 input argument와 cookie 값의 비교연산을 통과하기 위해 input argument를 cookie 값으로 바꿔주는 것이다.\n먼저 cookie의 값을 확인했다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80012.bmp 원본 그림의 크기: 가로 155pixel, 세로 32pixel\n\n\nbuffer overflow를 통해 return address 자리에는 해당 작업을 수행하는 함수의 주소를 넣어줘야 한다. 그러나 그 작업이 함수를 호출하는 형태로 구현되지 않는다. buffer overflow를 일으키기 위해 stack에 해당 작업을 수행하는 코드를 담아서, buffer overflow가 일어난 getbuf 함수가 반환하는 return의 값이 stack의 맨 처음으로 돌아가도록 구현한다. 이 때 맨 처음으로 돌아가면 우리가 원하는 중간작업을 수행하게 되고, 그 작업이 끝나면 touch2 함수를 실행하는 것이다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80006.bmp 원본 그림의 크기: 가로 898pixel, 세로 410pixel\n\n\n2단계를 pass하기 위해서는 중간에 있는 비교연산을 통과해야 한다. (cmp %edi, 0x202cfc(%rip)) edi값을 cookie 값으로 바꿔줘야 한다. 이는 attck lab instruction(pdf)을 따라 수행했으며 다음과 같이 작성했다. (2-1의 코드와 섞여있다.)\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80007.bmp 원본 그림의 크기: 가로 456pixel, 세로 100pixel\n\n\n%edi에 쿠키 값을 할당해야 했으므로 쿠키 값인 0x754e7ddd를 %edi에 할당하는 코드를 작성했다.\n중간작업이 끝나면 touch2 함수를 실행해야 한다. touch2 함수의 memory address를 return 해야 하므로 pushq $0x401812와 retq를 작성했다. 이 해결방법은 구글링과 수업자료, attack lab instruction에서 해답을 찾았다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80008.bmp 원본 그림의 크기: 가로 650pixel, 세로 319pixel\n\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80005.bmp 원본 그림의 크기: 가로 586pixel, 세로 209pixel\n\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80013.bmp 원본 그림의 크기: 가로 1546pixel, 세로 491pixel\n\n\npushq를 통해 %rbx를 stack의 최상단에 넣고, ret 전에 popq로 %rbx를 빼낸다. return 값이 따로 없으므로 ret 명령어가 실행되도 stack에서 별도의 return address를 받지 않고 다음으로 넘어간다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80007.bmp 원본 그림의 크기: 가로 456pixel, 세로 100pixel\n\n\npushq를 이용해서 return하고자 하는 값(touch2의 address, 0x401812)을 stack의 최상단에 넣고, retq는 해당 값을 pop하고 바로 그 address로 이동한다.\n이렇게 작성한 assembly 코드를 컴파일 한 뒤 objdump로 disassemble하여 실행코드를 생성했다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000a.bmp 원본 그림의 크기: 가로 507pixel, 세로 182pixel\n\n\n그리고 이 코드를 ans2.txt에 삽입했다. 1단계와 마찬가지로 24개의 dummy와 return address 영역에 buffer overflow로 들어갈 코드를 입력해야 했다. return address 영역에 넣어줄 코드는 다시 stack의 처음(%rdi)으로 돌아와야 하기 때문에 해당 주소를 넣어야 했는데, 실행 때마다 메모리가 동적으로 할당되어 %rdi에 있는 값과 주소가 계속 바뀌므로 Gets 함수 실행 직전의 %rdi 주소를 알아내야 했다.\ngdb를 통해 debug를 진행했고, 아래와 같이 알아냈다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000b.bmp 원본 그림의 크기: 가로 624pixel, 세로 497pixel\n\n\ngetbuf 함수에 breakpoint를 걸고, ni로 함수 내부까지 한 줄 한 줄 실행해서 내려가면서 Type string:을 힌트로 이 때 Gets 함수가 호출되었음을 확인했다. 그리고 info reg rdi 명령어를 통해 이 때의 rdi address를 알아냈다. 이 주소를 buffer overflow를 통해 segmentation fault를 낼 return address 자리에 넣어줬다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000c.bmp 원본 그림의 크기: 가로 571pixel, 세로 49pixel\n\n\n그렇게 실행하여 테스트에 통과했다.\n\n실행결과\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000d.bmp 원본 그림의 크기: 가로 688pixel, 세로 185pixel\n\n\n2. 2_1단계 문제\n\n문제 기술\n\n2단계 문제와 모든 것이 같으나 차이점은 cookie와 비교하는 것이 아니라 내가 두 값을 할당해서, 두 값을 비교하는 것이다.\n\n해답\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000e.bmp 원본 그림의 크기: 가로 570pixel, 세로 49pixel\n\n\n\n분석\n\n2단계에서는 cookie와 비교연산을 했고, 그 비교를 위한 argument를 내가 할당하는 방식이었다. 이번 2_1단계에서는 내가 두 값을 모두 할당해야 하고, 비교연산이 일어나는 레지스터가 무엇인지 알아내 두 값을 똑같은 값으로 할당하도록 코드만 변경하면 된다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000f.bmp 원본 그림의 크기: 가로 803pixel, 세로 385pixel\n\n\ncomp %esi, %edi 연산을 확인하고, 2단계를 해결한 코드 (shell.s)에서 %esi에 값을 할당하는 연산을 추가하여 disassemble 과정을 통해 코드를 추출하고, 해당 코드를 똑같이 getbuf의 stack에 쌓아주어 실행하였다.\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80007.bmp 원본 그림의 크기: 가로 456pixel, 세로 100pixel\n\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000a.bmp 원본 그림의 크기: 가로 507pixel, 세로 182pixel\n\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c8000e.bmp 원본 그림의 크기: 가로 570pixel, 세로 49pixel\n\n\n\n실행결과\n\n\n\n\n그림입니다. 원본 그림의 이름: CLP000066c80010.bmp 원본 그림의 크기: 가로 699pixel, 세로 182pixel"
  },
  {
    "objectID": "posts/2021-10-26-probability-and-inferential-statistics-midterm/2021-10-26-probability-and-inferential-statistics-midterm.html",
    "href": "posts/2021-10-26-probability-and-inferential-statistics-midterm/2021-10-26-probability-and-inferential-statistics-midterm.html",
    "title": "Probability and Inferential Statistics midterm",
    "section": "",
    "text": "probability_and_inferential_statistics_midterm_Page_13\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_14\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_15\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_16\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_17\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_18\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_19\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_20\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_21\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_22\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_01\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_02\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_03\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_04\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_05\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_06\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_07\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_08\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_09\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_10\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_11\n\n\n\n\n\nprobability_and_inferential_statistics_midterm_Page_12"
  },
  {
    "objectID": "posts/2021-10-02-kaggle-Feature-Engineering/2021-10-02-kaggle-Feature-Engineering.html",
    "href": "posts/2021-10-02-kaggle-Feature-Engineering/2021-10-02-kaggle-Feature-Engineering.html",
    "title": "Kaggle - Feature Engineering",
    "section": "",
    "text": "determine which features are the most important with mutual information\ninvent new features in several real-world problem domains\nencode high-cardinality categoricals with a target encoding\ncreate segmentation features with k-means clustering\ndecompose a dataset’s variation into features with principal component analysis\n\n\n\nThe goal of feature engineering is simply to make your data better suited to the problem at hand.\nYou might perform feature engineering to:\n\nimprove a model’s predictive performance\nreduce computational or data needs\nimprove interpretability of the results\n\n\n\n\nFor a feature to be useful, it must have a relationship to the target that your model is able to learn.\nThe key idea here is that a transformation you apply to a feature becomes in essence a part of the model itself. Say you were trying to predict the Price of square plots of land from the Length of one side. Fitting a linear model directly to Length gives poor results: the relationship is not linear.\nWhatever relationships your model can’t learn, you can provide yourself through transformations. As you develop your feature set, think about what information your model could use to achieve its best performance.\n\n\n\n\nA great first step is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target. Then you can choose a smaller set of the most useful features to develop initially and have more confidence that your time will be well spent.\nThe metric we’ll use is called “mutual information”. Mutual information is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.\nMutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you’d like to use yet. It is:\n\neasy to use and interpret,\ncomputationally efficient,\ntheoretically well-founded,\nresistant to overfitting, and,\nable to detect any kind of relationship\n\n\n\nMutual information describes relationships in terms of uncertainty. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other.\nTechnical note: What we’re calling uncertainty is measured using a quantity from information theory known as “entropy”. The entropy of a variable means roughly: “how many yes-or-no questions you would need to describe an occurance of that variable, on average.” The more questions you have to ask, the more uncertain you must be about the variable. Mutual information is how many questions you expect the feature to answer about the target.\n\n\n\nThe least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there’s no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a logarithmic quantity, so it increases very slowly.)\nHere are some things to remember when applying mutual information:\n\nMI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\nIt’s possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can’t detect interactions between features. It is a univariate metric.\nThe actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn’t mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.\n\nScikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets (mutual_info_regression) and one for categorical targets (mutual_info_classif).\n\n\n\nimage-20211002165106048\n\n\n\n\n\nimage-20211002165210148\n\n\n\n\n\n\nTips on Discovering New Features\n\nUnderstand the features. Refer to your dataset’s data documentation, if available.\nResearch the problem domain to acquire domain knowledge. If your problem is predicting house prices, do some research on real-estate for instance. Wikipedia can be a good starting point, but books and journal articles will often have the best information.\nStudy previous work. Solution write-ups from past Kaggle competitions are a great resource.\nUse data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified. Be sure to visualize your dataset as you work through the feature engineering process.\n\n\n\nRelationships among numerical features are often expressed through mathematical formulas, which you’ll frequently come across as part of your domain research. In Pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers.\n\n\n\nimage-20211002172808194\n\n\nData visualization can suggest transformations, often a “reshaping” of a feature through powers or logarithms.\n\n\n\nimage-20211002172830886\n\n\n\n\n\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count.\nYou could also use a dataframe’s built-in methods to create boolean values. In the Concrete dataset are the amounts of components in a concrete formulation. Many formulations lack one or more components (that is, the component has a value of 0). This will count how many components are in a formulation with the dataframe’s built-in greater-than gt method:\n\n\n\n\n\n\nimage-20211002173033385\n\n\n\n\n\nimage-20211002173043206\n\n\n\n\n\nFinally we have Group transforms, which aggregate information across multiple rows grouped by some category. If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\n\n\n\nimage-20211002173131527\n\n\nIf you’re using training and validation splits, to preserve their independence, it’s best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set’s merge method after creating a unique set of values with drop_duplicates on the training set:\n\n\n\nimage-20211002173241055\n\n\nTips on Creating Features It’s good to keep in mind your model’s own strengths and weaknesses when creating features. Here are some guidelines:\n\nLinear models learn sums and differences naturally, but can’t learn anything more complex.\nRatios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\nLinear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\nTree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\nCounts are especially helpful for tree models, since these models don’t have a natural way of aggregating information across many features at once.\n\n\n\n\n\nUnsupervised algorithms don’t make use of a target; instead, their purpose is to learn some property of the data, to represent the structure of the features in a certain way. In the context of feature engineering for prediction, you could think of an unsupervised algorithm as a “feature discovery” technique.\nClustering simply means the assigning of data points to groups based upon how similar the points are to each other.\nAdding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n\nApplied to a single real-valued feature, clustering acts like a traditional “binning” or “discretization” transform. On multiple features, it’s like “multi-dimensional binning” (sometimes called vector quantization).\n\n\n\nimg\n\n\nIt’s important to remember that this Cluster feature is categorical.Here, it’s shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate.\nThe motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It’s a “divide and conquer” strategy.\n\n\n\nimg\n\n\n\n\n\nThere are a great many clustering algorithms. They differ primarily in how they measure “similarity” or “proximity” and in what kinds of features they work with. The algorithm we’ll use, k-means, is intuitive and easy to apply in a feature engineering context. Depending on your application another algorithm might be more appropriate.\nK-means clustering measures similarity using ordinary straight-line distance (Euclidean distance, in other words). It creates clusters by placing a number of points, called centroids, inside the feature-space. Each point in the dataset is assigned to the cluster of whichever centroid it’s closest to. The “k” in “k-means” is how many centroids (that is, clusters) it creates. You define the k yourself.\nYou could imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what’s called a Voronoi tessallation. The tessallation shows you to what clusters future data will be assigned; the tessallation is essentially what k-means learns from its training data.\n\n\n\nimg\n\n\nWe’ll focus on three parameters from scikit-learn’s implementation: n_clusters, max_iter, and n_init.\nIt’s a simple two-step process. The algorithm starts by randomly initializing some predefined number (n_clusters) of centroids. It then iterates over these two operations:\n\nassign points to the nearest cluster centroid\nmove each centroid to minimize the distance to its points\n\nIt iterates over these two steps until the centroids aren’t moving anymore, or until some maximum number of iterations has passed (max_iter).\nIt often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times (n_init) and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.\n\n\n\nimg\n\n\nYou may need to increase the max_iter for a large number of clusters or n_init for a complex dataset. Ordinarily though the only parameter you’ll need to choose yourself is n_clusters (k, that is). The best partitioning for a set of features depends on the model you’re using and what you’re trying to predict, so it’s best to tune it like any hyperparameter (through cross-validation, say).\nSince k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values.\n\n\n\n\n\n\nimage-20211002180704464\n\n\nX = df.copy()\ny = X.pop(\"SalePrice\")\n\n\n# YOUR CODE HERE: Define a list of the features to be used for the clustering\nfeatures = ['LotArea', 'TotalBsmtSF', 'FirstFlrSF', 'SecondFlrSF', 'GrLivArea']\n\n\n# Standardize\nX_scaled = X.loc[:, features]\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n\n\n# YOUR CODE HERE: Fit the KMeans model to X_scaled and create the cluster labels\nkmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\nX[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n\n\n# Check your answer\nq_2.check()\n\n\n\n\nJust like clustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n(Technical note: PCA is typically applied to standardized data. With standardized data “variation” means “correlation”. With unstandardized data “variation” means “covariance”. All data in this course will be standardized before applying PCA.)\n\n\n\n\n\nimg\n\n\nNotice that instead of describing abalones by their 'Height' and 'Diameter', we could just as well describe them by their 'Size' and 'Shape'. This, in fact, is the whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\n\n\n\nimg\n\n\nThese new features are called the principal components of the data. The weights themselves are called loadings. There will be as many principal components as there are features in the original dataset: if we had used ten features instead of two, we would have ended up with ten components.\nPCA also tells us the amount of variation in each component. We can see from the figures that there is more variation in the data along the Size component than along the Shape component. PCA makes this precise through each component’s percent of explained variance.\n\n\n\nimg\n\n\nThe Size component captures the majority of the variation between Height and Diameter. It’s important to remember, however, that the amount of variance in a component doesn’t necessarily correspond to how good it is as a predictor: it depends on what you’re trying to predict.\n\n\n\nThe first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create – a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components.\nThe second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n\nDimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\nAnomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\nNoise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\nDecorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\nPCA basically gives you direct access to the correlational structure of your data.\nPCA Best Practices There are a few things to keep in mind when applying PCA:\n\nPCA only works with numeric features, like continuous quantities or counts.\nPCA is sensitive to scale. It’s good practice to standardize your data before applying PCA, unless you know you have good reason not to.\nConsider removing or constraining outliers, since they can an have an undue influence on the results.\n\n\n\n\nimage-20211002182102193\n\n\n\n\n\nimage-20211002182134006\n\n\n\n\n\nimage-20211002182150973\n\n\n\n\n\nimage-20211002182202310\n\n\n\n\n\n\ntarget encoding, is instead meant for categorical features. It’s a method of encoding categories as numbers, like one-hot or label encoding, with the difference that it also uses the target to create the encoding. This makes it what we call a supervised feature engineering technique.\nA target encoding is any kind of encoding that replaces a feature’s categories with some number derived from the target.\nThis kind of target encoding is sometimes called a mean encoding. Applied to a binary target, it’s also called bin counting. (Other names you might come across include: likelihood encoding, impact encoding, and leave-one-out encoding.)\n\n\nAn encoding like this presents a couple of problems, however. First are unknown categories. Target encodings create a special risk of overfitting, which means they need to be trained on an independent “encoding” split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.\nSecond are rare categories. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate.\nA solution to these problems is to add smoothing. The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.\n\n\n\nimage-20211002183225844\n\n\nWhen choosing a value for m, consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for m; if the average price for each make were relatively stable, a smaller value could be okay.\nUse Cases for Target Encoding Target encoding is great for:\n\nHigh-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature’s most important property: its relationship with the target.\nDomain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature’s true informativeness."
  },
  {
    "objectID": "posts/2021-10-20-algorithms-midterm/2021-10-20-algorithms-midterm.html",
    "href": "posts/2021-10-20-algorithms-midterm/2021-10-20-algorithms-midterm.html",
    "title": "Algorithms midterm",
    "section": "",
    "text": "Computer science is abstraction.\nAbstraction is to create a new model that allows to ignore irrelevant details.\n\n\nMathematical abstraction of computer program\n\nAt the heart of programs lie algorithms\n\nWell-specified procedure for solving a computational problem.\nComputational problem = mapping from inputs to outputs\n\n\n\nDoes an algorithm actually work? - correctness\nIs it fast? - Time/space efficiency, lower bounds, optimality\n\n\n\n\n\n\nimage-20211021045928551\n\n\n\n\n\nIntuition - maintain a growing sorted list. For each element, put it into the ‘right place’ in this growing list.\n\n\n\nimage-20211021050146237\n\n\nKey insight - to reason about the behavior of algorithms, it often helps to look for things that don’t change.\n\n\nThere’s name for a condition that is true before and after each iteration of a loop: a loop invariant\n\n\n\nInductive hypothesis - The loop invariant holds after the ith iteration.\nBase case - The loop invariant holds before the first iteration.\nInductive step - If the loop invariant holds after the ith iteration, then it holds after the (i + 1)st iteration.\nConclusion - If the loop invariant holds after the last iteration, then the algorithm is correct.\n\n\n\n\n\n\n\n\n\nimage-20211021051227989\n\n\n\n\n\n\nWorst-case analysis - What is the runtime of the algorithm on the worst possible input?\n\nAlgorithm performs at least this fast for every input.\n\nBest-case analysis - What is the runtime of the algorithm on the best possible input?\nAverage-case analysis - What is the runtime of the algorithm on the average input?\n\n\n\n\nWhat does it mean to measure ‘runtime’ of an algorithm?\n\nEngineers probably care most about the ‘real-world time’ - how long does the algorithm take in seconds, etc?\nThis heavily depends on computer hardware, programming language, etc.\nInstead, we want to use a universal measure of runtime that’s independent of these considerations.\nTime-complexity\n\n\n\n\nimage-20211021064234706\n\n\n\n\n\nimage-20211021064244895\n\n\n\n\n\nimage-20211021064308655\n\n\n\n\n\n\n\n\nimage-20211021064326937\n\n\n\n\n\nimage-20211021064339047\n\n\n\n\n\nimage-20211021064404835\n\n\n\n\n\nimage-20211021064434353\n\n\n\n\n\nimage-20211021064442732\n\n\n\n\n\nimage-20211021064450186\n\n\n\n\n\nimage-20211021064459872\n\n\n\n\n\nimage-20211021065532173\n\n\n\n\n\n\n\n\nimage-20211021065624103\n\n\n\n\n\nimage-20211021065632773\n\n\n\n\n\nimage-20211021065713427\n\n\n\n\n\n\n\n\nimage-20211021065743151\n\n\n\n\n\nWe call this asymptotic runtime as time-complexity\n\n\n\nimage-20211021113646297\n\n\n\n\n\nimage-20211021113654535\n\n\n\n\n\nimage-20211021113707205\n\n\n\n\n\n\nInsertion sort - It finds a proper (sorted) position, and insert an element into there\nSelection sort - It picks a min/max element from the unsorted list, and put the element into the front/end of the sorted list.\nBubble sort - It pushes higher values to the end of the list. The highest element in the list will float toward the end of the list.\n\n\n\n\n\n\n\nimage-20211021114006059\n\n\n\n\n\n\n\n\nimage-20211021114017977\n\n\n\n\n\nMerge sort uses divide-and-conquer.\nIntuition - Divide the list into halves, recursively sort them, merge the sorted halves into a whole sorted list, and return this list.\n\n\nDivide - Break the current problem into smaller (easier) sub-problems.\nConquer - Solve the smaller problems and collect the results to solve the current problem.\n\n\n\nimage-20211021114147750\n\n\n\n\n\nimage-20211021114156442\n\n\n\n\n\nimage-20211021114323616\n\n\n\n\n\nimage-20211021114206080\n\n\n\n\n\nimage-20211021114213527\n\n\n\n\n\nimage-20211021114342564\n\n\n\n\n\n\n\n\nimage-20211021114713745\n\n\n\n\n\nFind the loop invariant\nDefine the inductive hypothesis (internal state at iteration i)\nProve the base case\nProve the inductive step\nProve the conclusion\n\n\n\n\n\nDefine the inductive hypothesis (correct for inputs of sizes 1 to i)\nProve the base case (i < small constant)\nProve the inductive step (i >= i + 1 or {1, 2, …, i} => i + 1)\nProve the conclusion (i = n => correct)\n\n\n\n\n\n\n\n\nimage-20211021115113577\n\n\n\n\n\nimage-20211021115128435\n\n\n\n\n\nimage-20211021115139728\n\n\n\n\n\nimage-20211021115153888\n\n\nA recurrence relation is a function or sequence whose values are defined in terms of earlier or smaller values.\nOur recurrence relation for the runtime of merge sort isn’t very useful unless we can define the runtime as closed-form expression.\n\n\n\nimage-20211021115320101\n\n\n\n\n\nimage-20211021115325460\n\n\n\n\n\nimage-20211021115332666\n\n\n\n\n\nimage-20211021115402489\n\n\nThere are a few different methods to translate a recurrence relation for T(n) to a closed form expression for T(n).\n\nRecursion tree method\nIteration method\nMaster method\nSubstitution method\n\n\n\n\n\n\nimage-20211021115511422\n\n\n\n\n\nimage-20211021115549864\n\n\n\n\n\n\n\n\nimage-20211021115644792\n\n\n\n\n\n\n\n\nimage-20211021115749458\n\n\n\n\n\nimage-20211021120117774\n\n\n\n\n\nimage-20211021120338604\n\n\n\n\n\nimage-20211021120353984\n\n\n\n\n\n\n\n\n\nimage-20211021120419695\n\n\n\n\n\nimage-20211021120855580\n\n\n\n\n\nimage-20211021120938894\n\n\n\n\n\nimage-20211021121021910\n\n\n\n\n\nimage-20211021121033424\n\n\n\n\n\n\n\n\n\nimage-20211021121111444\n\n\n\n\n\nimage-20211021121435241\n\n\n\n\n\nimage-20211021123527621\n\n\n\n\n\nimage-20211021123604626\n\n\n\n\n\nimage-20211021123614720\n\n\n\n\n\nimage-20211021123705893\n\n\nIntuition - Partition the list about a pivot selected at random, either return the pivot itself or recurse on the left or right sublists.\n\n\n\nimage-20211021123841793\n\n\n\n\n\nimage-20211021123859299\n\n\n\n\n\nimage-20211021124013222\n\n\n\n\n\n\n\nimage-20211021124053956\n\n\n\n\n\nimage-20211021124117908\n\n\n\n\n\nimage-20211021124206062\n\n\n\n\n\nimage-20211021124221045\n\n\n\n\n\nimage-20211021124237358\n\n\n\n\n\nimage-20211021124302121\n\n\n\n\n\nimage-20211021124334872\n\n\n\n\n\n\n\n\nimage-20211021124543973\n\n\n\n\n\nimage-20211021124607439\n\n\n\n\n\nimage-20211021124642884\n\n\n\n\n\nimage-20211021124756000\n\n\n\n\n\nimage-20211021124807814\n\n\n\n\n\nimage-20211021124921869\n\n\n\n\n\nimage-20211021125019839\n\n\n\n\n\nimage-20211021125052810\n\n\n\n\n\nimage-20211021125120172\n\n\n\n\n\nimage-20211021125140424\n\n\n\n\n\nimage-20211021125152001\n\n\n\n\n\nimage-20211021125306215\n\n\n\n\n\nimage-20211021125319020\n\n\n\n\n\nimage-20211021125346128\n\n\n\n\n\nimage-20211021125402720\n\n\n\n\n\nimage-20211021125413114\n\n\n\n\n\n\n\n\n\nimage-20211021125455038\n\n\n\n\n\nimage-20211021125508556\n\n\n\n\n\nimage-20211021125515292\n\n\n\n\n\nimage-20211021125522704\n\n\n\n\n\nimage-20211021125640746\n\n\n\n\n\nimage-20211021125710320\n\n\n\n\n\nimage-20211021125720635\n\n\n\n\n\n\n\n\nimage-20211021125835592\n\n\n\n\n\nimage-20211021125859358\n\n\n\n\n\nimage-20211021125908463\n\n\n\n\n\n\n\nimage-20211021125927533\n\n\n\n\n\nimage-20211021125938128\n\n\n\n\n\nimage-20211021130018106\n\n\n\n\n\nimage-20211021130030014\n\n\n\n\n\nimage-20211021130043823\n\n\n\n\n\n\n\n\n\nimage-20211021130236653\n\n\n\n\n\n\n\n\n\n\nimage-20211021130330928\n\n\n\n\n\n\n\n\nimage-20211021130456924\n\n\n\n\n\nimage-20211021130514580\n\n\n\n\n\n\n\n\nimage-20211021130527622\n\n\n\n\n\nimage-20211021130538816\n\n\n\n\n\nimage-20211021130545390\n\n\n\n\n\nimage-20211021130623552\n\n\n\n\n\nimage-20211021130645631\n\n\n\n\n\n\n\n\nimage-20211021130700769\n\n\n\n\n\nimage-20211021130723160\n\n\n\n\n\nimage-20211021130735725\n\n\n\n\n\nimage-20211021130753189\n\n\n\n\n\nimage-20211021130809763\n\n\n\n\n\nimage-20211021130815905\n\n\n\n\n\n\n\n\nimage-20211021130834178\n\n\n\n\n\n\n\n\nimage-20211021130911653\n\n\n\n\n\n\n\n\n\nimage-20211021131022047\n\n\n\n\n\n\n\n\nimage-20211021135954380\n\n\n\n\n\nimage-20211021140007618\n\n\n\n\n\nimage-20211021140014875\n\n\n\n\n\nimage-20211021140038533\n\n\n\n\n\nimage-20211021140048914\n\n\n\n\n\nimage-20211021140107324\n\n\n\n\n\nimage-20211021140117131\n\n\n\n\n\nimage-20211021140127261\n\n\n\n\n\nimage-20211021140147163\n\n\n\n\n\nimage-20211021140210438\n\n\n\n\n\nimage-20211021140331091\n\n\n\n\n\n\n\nimage-20211021140345342\n\n\n\n\n\n\n\n\nimage-20211021140401201\n\n\n\n\n\n\n\n\nimage-20211021140436508\n\n\n\n\n\nimage-20211021140558531\n\n\n\n\n\n\n\n\nimage-20211021140628307\n\n\n\n\n\nimage-20211021140646044\n\n\n\n\n\nimage-20211021140657468\n\n\n\n\n\nimage-20211021140712112\n\n\n\n\n\nimage-20211021140724599\n\n\n\n\n\n\n\n\n\nimage-20211021140802968\n\n\n\n\n\nimage-20211021140819066\n\n\n\n\n\nimage-20211021140841480\n\n\n\n\n\nimage-20211021140851270\n\n\n\n\n\nimage-20211021140930844\n\n\n\n\n\n\n\nimage-20211021140941021\n\n\n\n\n\n\n\n\n\nimage-20211021141100050\n\n\n\n\n\n\n\nimage-20211021141124175\n\n\n\n\n\n\n\n\nimage-20211021141138896\n\n\n\n\n\n\n\n\nimage-20211021141204027\n\n\n\n\n\n\n\n\n\n\n\nimage-20211021141230971\n\n\n\n\n\nimage-20211021141237742\n\n\n\n\n\nimage-20211021141247448\n\n\n\n\n\nimage-20211021141307041\n\n\n\n\n\n\n\n\nimage-20211021141324893\n\n\n\n\n\n\n\n\n\nimage-20211021141408613\n\n\n\n\n\nimage-20211021141420978\n\n\nkinda like quick sort\n\n\n\nimage-20211021141450278\n\n\n\n\n\n\n\nimage-20211021141516883\n\n\n\n\n\n\n\n\nimage-20211021141546068\n\n\n\n\n\n\n\n\nimage-20211021141622910\n\n\n\n\n\n\n\nimage-20211021141651027\n\n\n\n\n\nimage-20211021141702181\n\n\n\n\n\nimage-20211021141732651\n\n\n\n\n\nimage-20211021141742085\n\n\n\n\n\nimage-20211021141752435\n\n\n\n\n\nimage-20211021141857775\n\n\n\n\n\n\n\n\n\nimage-20211021141912341\n\n\n\n\n\nimage-20211021141923868\n\n\n\n\n\nimage-20211021141941401\n\n\n\n\n\n\n\n\n\n\n\nimage-20211021142015744\n\n\n\n\n\nimage-20211021142037839\n\n\n\n\n\nimage-20211021142049696\n\n\n\n\n\nimage-20211021142117873\n\n\n\n\n\n\n\n\nimage-20211021142134160\n\n\n\n\n\n\n\n\n\nimage-20211021142149756\n\n\n\n\n\n\n\nimage-20211021142216503\n\n\n\n\n\nimage-20211021142247516\n\n\n\n\n\nimage-20211021142653695\n\n\n\n\n\n\n\n\nimage-20211021142724709\n\n\n\n\n\n\n\n\nimage-20211021142821101\n\n\n\n\n\nimage-20211021142849669\n\n\n\n\n\nimage-20211021142900482\n\n\n\n\n\nimage-20211021142914061\n\n\n\n\n\nimage-20211021142931249\n\n\n\n\n\nimage-20211021142942187\n\n\n\n\n\nimage-20211021142956735\n\n\n\n\n\nimage-20211021143016512\n\n\n\n\n\nimage-20211021143054322\n\n\n\n\n\nimage-20211021143108138\n\n\n\n\n\nimage-20211021143130638\n\n\n\n\n\nimage-20211021143141392\n\n\n\n\n\n\n\n\nimage-20211021143157008\n\n\n\n\n\nimage-20211021143208450\n\n\n\n\n\nimage-20211021143233556"
  },
  {
    "objectID": "posts/2021-11-29-digital-system-circuits-week-11/2021-11-30-digital-system-circuits-week-11.html",
    "href": "posts/2021-11-29-digital-system-circuits-week-11/2021-11-30-digital-system-circuits-week-11.html",
    "title": "Digital System Circuits Week 11",
    "section": "",
    "text": "image-20211130032813685\n\n\n\n\n\n\n\n\nimage-20211130034059478\n\n\nnet - node와 node를 이어주는 것\n\n\n\n\n\n\nimage-20211130035357091\n\n\n\n\n\nimage-20211130035410050\n\n\n\n\n\n\n\n\nimage-20211130035424572\n\n\n\n\n\nimage-20211130035430154\n\n\n\n\n\n\n\n\nimage-20211130035822500\n\n\n\n\n\n\n\n\nimage-20211130035837076\n\n\n\n\n\nimage-20211130035848204\n\n\n\n\n\n\n\n\nimage-20211130035903793\n\n\n\n\n\nimage-20211130035913396\n\n\n\n\n\nimage-20211130035921650\n\n\n\n\n\nimage-20211130035933231\n\n\n\n\n\nimage-20211130035942965\n\n\n\n\n\n\n\n\nimage-20211130035953861"
  },
  {
    "objectID": "posts/2021-12-07-algorithms-week-12/2021-12-07-algorithms-week-12.html",
    "href": "posts/2021-12-07-algorithms-week-12/2021-12-07-algorithms-week-12.html",
    "title": "Algorithms Week 12",
    "section": "",
    "text": "image-20211207071420794\n\n\n\n\n\nimage-20211207071429748\n\n\n\n\n\n\n\nimage-20211207071611990\n\n\n\n\n\nimage-20211207071623525\n\n\n\n\n\nimage-20211207071632062\n\n\n\n\n\n\n\n\nimage-20211207071646562\n\n\n\n\n\n\n\n\nimage-20211207072048370\n\n\n\n\n\n\n\n\nimage-20211207072105323\n\n\n\n\n\nimage-20211207072114914\n\n\n\n\n\n\n\n\nimage-20211207072350584\n\n\n\n\n\nimage-20211207072358870\n\n\n\n\n\nimage-20211207072408646\n\n\n\n\n\n\n\n\n\n\nimage-20211207072849043\n\n\n\n\n\n\n\n\nimage-20211207072903251\n\n\n\n\n\nimage-20211207072911272\n\n\n\n\n\n\n\n\n\nimage-20211207072937862\n\n\n\n\n\nimage-20211207072945233\n\n\n\n\n\nimage-20211207073257898\n\n\n\n\n\nimage-20211207073305806\n\n\n\n\n\nimage-20211207073312807\n\n\n\n\n\nimage-20211207073320545\n\n\n\n\n\nimage-20211207073327420\n\n\n\n\n\nimage-20211207073334458\n\n\n\n\n\nimage-20211207073341451\n\n\n\n\n\n\n\n\n\n\nimage-20211207073647275\n\n\n\n\n\n\n\n\n\nimage-20211207073658672\n\n\n\n\n\n\n\n\n\n\nimage-20211207073711683\n\n\n\n\n\nimage-20211207073733220\n\n\n\n\n\n\n\n\nimage-20211207073745440\n\n\n\n\n\n\n\n\n\nimage-20211207115907866\n\n\n\n\n\n\n\n\nimage-20211207115919252\n\n\n\n\n\nimage-20211207115937549\n\n\n\n\n\n\n\n\n\n\nimage-20211207120048389\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207120119754\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207120204853\n\n\n\n\n\nimage-20211207120542232\n\n\n\n\n\n\n\n\nimage-20211207120555733\n\n\n\n\n\nimage-20211207120607323\n\n\n\n\n\nimage-20211207120617834\n\n\n\n\n\nimage-20211207120628535\n\n\n\n\n\nimage-20211207120640009\n\n\n\n\n\nimage-20211207120704371\n\n\n\n\n\nimage-20211207120713980\n\n\n\n\n\nimage-20211207120728180\n\n\n\n\n\n\n\n\n\nimage-20211207121150220\n\n\n\n\n\n\n\n\nimage-20211207121228743\n\n\n\n\n\n\n\n\n\n\nimage-20211207121243559\n\n\n\n\n\n\n\n\nimage-20211207121309228\n\n\n\n\n\n\n\n\nimage-20211207121329522\n\n\n\n\n\n\n\n\n\nimage-20211207121345277\n\n\n\n\n\n\n\n\nimage-20211207121453001\n\n\n\n\n\n\n\nimage-20211207121621772\n\n\n\n\n\n\n\n\nimage-20211207121700737\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207121739588\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207121841801\n\n\n\n\n\n\n\n\n\nimage-20211207121905657\n\n\n\n\n\nimage-20211207121914313\n\n\n\n\n\nimage-20211207122020213\n\n\n\n\n\nimage-20211207122029165\n\n\n\n\n\nimage-20211207122037977\n\n\n\n\n\nimage-20211207122046546\n\n\n\n\n\nimage-20211207122056884\n\n\n\n\n\nimage-20211207122107288\n\n\n\n\n\nimage-20211207122117785\n\n\n\n\n\nimage-20211207122129053\n\n\n\n\n\nimage-20211207122218721\n\n\n\n\n\nimage-20211207122229043\n\n\n\n\n\nimage-20211207122249982\n\n\n\n\n\nimage-20211207122259152\n\n\n\n\n\nimage-20211207122309367\n\n\n\n\n\nimage-20211207122319578\n\n\n\n\n\nimage-20211207122328257\n\n\n\n\n\n\n\n\nimage-20211207122621320\n\n\n\n\n\n\n\n\nimage-20211207122636131\n\n\n\n\n\n\n\n\nimage-20211207122912123\n\n\n\n\n\n\n\n\nimage-20211207122939165\n\n\n\n\n\n\n\n\n\n\nimage-20211207123332584\n\n\n\n\n\n\n\n\nimage-20211207123402431\n\n\n\n\n\n\n\n\nimage-20211207123422340\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207123630180\n\n\n\n\n\n\n\n\nimage-20211207123822559\n\n\n\n\n\n\n\n\n\n\n\nimage-20211207123901494\n\n\n\n\n\n\n\n\n\nimage-20211207123920179\n\n\n\n\n\nimage-20211207124315270\n\n\n\n\n\n\n\n\nimage-20211207124327964\n\n\n\n\n\n\n\n\nimage-20211207124337430"
  },
  {
    "objectID": "posts/2021-10-08-probability-and-statistics-week-6/2021-10-08-probability-and-statistics-week-6_notebook.html",
    "href": "posts/2021-10-08-probability-and-statistics-week-6/2021-10-08-probability-and-statistics-week-6_notebook.html",
    "title": "Siyun Min",
    "section": "",
    "text": "Probability and Statistics Week 6\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\npreg = pd.read_csv(\"./LectureCode/2002FemPreg.tsv\", sep='\\t')\nlive = preg[preg['outcome'] == 1]\n\n\nplt.hist(live['birthwgt_lb'], rwidth=0.8, label='birthwgt_lb', bins=range(int(live['birthwgt_lb'].min()), int(live['birthwgt_lb'].max() + 2)))\nplt.xlabel('Birth weight (pounds)')\nplt.ylabel('frequency')\nplt.show()\nplt.close()\n\n\n\n\n\nhist = {key: val for key, val in enumerate(np.bincount(live['birthwgt_lb'].dropna()))}\n\n\nhist\n\n{0: 8,\n 1: 40,\n 2: 53,\n 3: 98,\n 4: 229,\n 5: 697,\n 6: 2223,\n 7: 3049,\n 8: 1889,\n 9: 623,\n 10: 132,\n 11: 26,\n 12: 10,\n 13: 3,\n 14: 3,\n 15: 1}\n\n\n\nn = sum(hist.values())\npmf = hist.copy()\nfor x, freq in hist.items():\n    pmf[x] = freq / n\n\n\npmf\n\n{0: 0.0008806693086745927,\n 1: 0.004403346543372964,\n 2: 0.005834434169969176,\n 3: 0.01078819903126376,\n 4: 0.025209158960810215,\n 5: 0.07672831351827389,\n 6: 0.24471598414795245,\n 7: 0.3356450902686041,\n 8: 0.2079480405107882,\n 9: 0.06858212241303391,\n 10: 0.01453104359313078,\n 11: 0.0028621752531924264,\n 12: 0.001100836635843241,\n 13: 0.00033025099075297226,\n 14: 0.00033025099075297226,\n 15: 0.00011008366358432408}\n\n\n\nplt.bar(pmf.keys(), pmf.values())\nplt.xlabel('value')\nplt.ylabel('pmf')\nplt.show()\nplt.close()\n\n\n\n\n\nprint(sum(pmf.values()))\n\n1.0\n\n\n\nvalues = [1, 2, 2, 3, 5]\npmf = {key : val/len(values) for key, val in enumerate(np.bincount(values)) if val != 0}\n\n\npmf\n\n{1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2}\n\n\n\npmf = {key: val / len(live['prglngth'].dropna()) for key, val in enumerate(np.bincount(live['prglngth'].dropna()))}\n\n\nplt.bar(pmf.keys(), pmf.values())\nplt.xlabel('value')\nplt.ylabel('pmf')\nplt.show()\nplt.close()\n\n\n\n\n\nplt.step(list(pmf.keys()), list(pmf.values()))\nplt.xlabel('value')\nplt.ylabel('pmf')\nplt.show()\nplt.close()\n\n\n\n\n\nfirsts = live[live['birthord'] == 1]\nothers = live[live['birthord'] > 1]\nfirst_pmf = {key: val / len(firsts['prglngth'].dropna()) for key, val in enumerate(np.bincount(firsts['prglngth'].dropna()))}\nothers_pmf = {key: val / len(others['prglngth'].dropna()) for key, val in enumerate(np.bincount(others['prglngth'].dropna()))}\n\n\nplt.bar(first_pmf.keys(), first_pmf.values(), width=0.5, align='edge', label='firsts')\nplt.bar(others_pmf.keys(), others_pmf.values(), width=0.5, align='center', label='others')\nplt.xlabel('value')\nplt.ylabel('pmf')\nplt.legend()\nplt.xlim(26, 45)\nplt.show()\nplt.close()\n\n\n\n\n\nplt.step(list(first_pmf.keys()), list(first_pmf.values()), label='firsts')\nplt.step(list(others_pmf.keys()), list(others_pmf.values()), label='others')\nplt.xlabel('value')\nplt.ylabel('pmf')\nplt.xlim(26, 45)\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nweeks = range(36, 46)\ndiffs = []\nfor week in weeks:\n    p1 = first_pmf[week]\n    p2 = others_pmf[week]\n    diff = 100 * (p1 - p2)\n    diffs.append(diff)\n\nplt.bar(weeks, diffs)\nplt.xlabel('prg length')\nplt.ylabel('diff')\nplt.show()\nplt.close()\n\n\n\n\n\nlive = preg[preg['outcome'] == 1]\nfirsts = live[live['birthord'] == 1]\nothers = live[live['birthord'] > 1]\n\n\nfirst_wgt_dropna = firsts['totalwgt_lb'].dropna()\nother_wgt_dropna = others['totalwgt_lb'].dropna()\n\n\nfirst_hist, first_bins = np.histogram(first_wgt_dropna, bins=np.arange(0, 16, 0.05))\nother_hist, other_bins = np.histogram(other_wgt_dropna, bins=np.arange(0, 16, 0.05))\nfirst_pmf = {key: val / len(first_wgt_dropna) for key, val in zip(first_bins, first_hist)}\nother_pmf = {key: val / len(other_wgt_dropna) for key, val in zip(other_bins, other_hist)}\n\n\nplt.step(first_bins[:-1], first_hist, label='firsts')\nplt.step(other_bins[:-1], other_hist, label='firsts')\nplt.xlabel('value')\nplt.ylabel('hist')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nplt.step(list(first_pmf.keys()), list(first_pmf.values()), label='firsts')\nplt.step(list(other_pmf.keys()), list(other_pmf.values()), label='firsts')\nplt.xlabel('value')\nplt.ylabel('hist')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\ndef PercentileRank(scores, your_score):\n    count = 0\n    for score in scores:\n        if score <= your_score:\n            count += 1\n    \n    percentile_rank = 100 * count / len(scores)\n    return percentile_rank\n\n\nt = [55, 66, 77, 88, 99]\nPercentileRank(t, 88)\n\n80.0\n\n\n\ndef Percentile(scores, percentile_rank):\n    for score in sorted(scores):\n        if PercentileRank(scores, score) >= percentile_rank:\n            return score\n\nPercentile(t, 45)\n\n77\n\n\n\ndef Percentile2(scores, percentile_rank):\n    scores.sort()\n    index = percentile_rank * (len(scores) - 1) // 100\n    return scores[index]\n\nPercentile2(t, 45)\n\n66\n\n\n\ndef evalCdf(sample, x):\n    count = 0.0\n    for value in sorted(sample):\n        if value <= x:\n            count += 1\n    prob = count / len(sample)\n    return prob\n\ndef getValCdf(sample, cdf_value):\n    for score in sorted(sample):\n        if evalCdf(sample, score) >= cdf_value:\n            return score\n\ndef evalCdfCdf(cdf, target_val):\n    for val, score in sorted(cdf.items()):\n        if val >= target_val:\n            return score\n\ndef getValCdfCdf(cdf, cdf_value):\n    for val, score in sorted(cdf.items()):\n        if score >= cdf_value:\n            return val\n\n\nt = [1, 2, 3, 4, 5]\nprint(evalCdf(t, 3))\n\n0.6\n\n\n\ndef pmf2cdf(pmf_dict):\n    cdf_dict = {}\n    now = 0.0\n    for k, v in sorted(pmf_dict.items()):\n        now += v\n        cdf_dict[k] = now\n    return cdf_dict\n\n\nfirst_hist, first_bins = np.histogram(first_wgt_dropna, bins=np.arange(0, 16, 0.05))\nother_hist, other_bins = np.histogram(other_wgt_dropna, bins=np.arange(0, 16, 0.05))\nlive_wgt_dropna = live['totalwgt_lb'].dropna()\nlive_hist, live_bins = np.histogram(live_wgt_dropna, bins=np.arange(0, 16, 0.05))\n\nfirst_pmf = {key: val / len(first_wgt_dropna) for key, val in zip(first_bins, first_hist)}\nother_pmf = {key: val / len(other_wgt_dropna) for key, val in zip(other_bins, other_hist)}\nlive_pmf = {key: val / len(live_wgt_dropna) for key, val in zip(live_bins, live_hist)}\n\nfirst_cdf = pmf2cdf(first_pmf)\nother_cdf = pmf2cdf(other_pmf)\nlive_cdf = pmf2cdf(live_pmf)\n\n\nplt.step(list(first_cdf.keys()), list(first_cdf.values()), label='first')\nplt.step(list(other_cdf.keys()), list(other_cdf.values()), label='others')\nplt.step(list(live_cdf.keys()), list(live_cdf.values()), label='live')\nplt.xlabel('brithwgt')\nplt.ylabel('CDF')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\n\nsample = np.random.choice(live_wgt_dropna, 20, replace=True)\nsample_hist, sample_bins = np.histogram(sample, bins=np.arange(0, 16, 0.05))\nsample_pmf = {key: val for key, val in zip(sample_bins, sample_hist)}\nsample_cdf = pmf2cdf(sample_pmf)\n\nplt.step(list(sample_cdf.keys()), list(sample_cdf.values()))\nplt.show()\nplt.close()"
  },
  {
    "objectID": "posts/2021-10-08-probability-and-statistics-week-6/2021-10-08-probability-and-statistics-week-6.html",
    "href": "posts/2021-10-08-probability-and-statistics-week-6/2021-10-08-probability-and-statistics-week-6.html",
    "title": "Probability and Statistics Week 6",
    "section": "",
    "text": "image-20211008165800978\n\n\n\n\n\n\n\n\nimage-20211008165844897\n\n\n\n\n\nimage-20211008170139246\n\n\n\n\n\n\n\n\nimage-20211008170222276\n\n\n\n\n\n\n\n\nimage-20211008170852099\n\n\n\n\n\nimage-20211008171030366\n\n\n\n\n\n\n\nimage-20211008171341111\n\n\n\n\n\n\n\n\n\nimage-20211008171539758\n\n\n\n\n\nimage-20211008171727547\n\n\n\n\n\n\n\n\nimage-20211008171749161\n\n\n\n\n\nimage-20211008171823883\n\n\n\n\n\nimage-20211008171830883\n\n\n\n\n\n\n\n\nimage-20211008172021195\n\n\n\n\n\nimage-20211008172145094\n\n\n\n\n\nimage-20211008172237024\n\n\n\n\n\n\n\n\nimage-20211008172309089\n\n\n\n\n\n\n\n\nimage-20211008172413492\n\n\n\n\n\nimage-20211008172630790\n\n\n\n\n\nimage-20211008172653951\n\n\n\n\n\nimage-20211008172718353\n\n\n\n\n\nimage-20211008172751275\n\n\n\n\n\n\n\n\nimage-20211008172917907\n\n\n\n\n\n\n\n\nimage-20211008172947902\n\n\n\n\n\nimage-20211008173026392\n\n\n\n\n\n\n\n\nimage-20211008173137523"
  },
  {
    "objectID": "posts/2021-10-17-CFA-Level-2-Corporate-Finance/2021-10-17-CFA-Level-2-Corporate-Finance.html",
    "href": "posts/2021-10-17-CFA-Level-2-Corporate-Finance/2021-10-17-CFA-Level-2-Corporate-Finance.html",
    "title": "CFA Level 2 Corporate Finance",
    "section": "",
    "text": "image-20211017200234167\n\n\n\n\n\nimage-20211017200326244\n\n\n\n\n\nimage-20211017201059307\n\n\n\n\n\n\n\n\nimage-20211017201115435\n\n\n\n\n\nimage-20211017201126107\n\n\n\n\n\nimage-20211017201135147\n\n\n\n\n\nimage-20211017201142808\n\n\nThe relevant cash flows for evaluating a capital project are the incremental after-tax cash flows. How various real options give managers flexibility with capital budgeting projects. Even if you are unsure how to handle the calculation of NPV involving real options, remember that the existence of options will always increase NPV. Finally, familiarize yourself with alternative concepts of calculating income, including economic income, economic profit, residual income, and claims analysis, and pay attention to the proper discount rate under each method.\n\n\n\n\nThe capital budgeting process is the process of identifying and evaluating capital projects; that is, projects where the cash flow to the firm will be received over a period longer than a year. Any corporate decisions with an impact on future earnings can be examined using the framework. Decisions about whether to buy a new machine, expand business in another geographic area, move the corporate headquarters to Cleveland, or replace a delivery truck, to name a few, can be examined using a capital budgeting analysis.\n\n\n\n\nReplacement projects to maintain the business\nReplacement projects for cost reduction\nExpansion projects\nNew product or market\nMandatory projects\nOther projects\n\n\n\n\n\nDecisions are based on cash flows, not accounting income.\nThe relevant cash flows to consider as part of the capital budgeting process are incremental cash flows.\nSunk costs are costs that cannot be avoided, even if the project is not undertaken. (과거 cost는 고려하지 않음) An example of a sunk cost is a consulting fee paid to a marketing research firm to estimate demand for a new product prior to a decision on the project.\nExternalities - cannibalization\n\npre-emptive cannibalization - 시장 선점 목적, 수익률은 떨어지지만 NPV 극대화\n\nCash flows are based on opportunity costs.\nThe timing of cash flows is important.\nCapital budgeting decisions account for the time value of money, which means that cash flows received earlier are worth more than cash flows to be received later.\nCash flows are analyzed on an after-tax basis.\nFinancing costs are reflected in the project’s required rate of return.\nThe required rate of return is a function of its risks.\n\n\n\n\n\ncase by case로 적용되서 보통 table을 제시하는데, 정 안주면 Double-declining method로 depreciation 계싼하면 됨.\nIn the United States, modified accelerated cost recovery system (MACRS) for tax purposes.\n\n\n\nimage-20211017195221848\n\n\nThe depreciable basis is equal to the purchase price plus any shipping or handling and installation costs. The basis is not adjusted for salvage value regardless of whether the accelerated or straight-line method is used.\nSalvage value 고려 않고 0까지 deprecation in tax.\n\n\n\n\nInitial investment outlay is the up-front costs associated with the project. Components are price, which includes shipping and installation (FCInv) and investment in net working capital (NWCInv)\n\n\\[\n\\\\\\operatorname{outlay} = \\operatorname{FCInv} + \\operatorname{NWCInv}\n\\\\\\operatorname{NWCInv} = \\Delta\\operatorname{non-cash\\, current\\, assets} - \\Delta\\operatorname{non-debt\\, current\\, liabilities} = \\Delta\\operatorname{NWC}\n\\]\n\nAfter-tax operating cash flows (CF)\n\n\\[\n\\\\CF = (S - C - D)(1 - T) + D\n\\\\= (S - C)(1 - T) + TD\n\\\\= EBITDA(1 - t) + Dep * t\n\\\\= EBIT(1 - t) + Dep\n\\]\nIn general, a higher depreciation expense will result in greater tax savings and higher cash flows. This means that accelerated depreciation methods will create higher after-tax cash flows for the project earlier in the project’s life as compared to the straight-line method, resulting in a higher net present value (NPV) for the project.\nInterest is not included in operating cash flows for capital budgeting purposes because it is incorporated into the project’s cost of capital.\n\nTerminal value after-tax non-operating cash flows (TNOCF)\n\n\\[\n\\\\TNOCF = Sal_{T} + NWCInv - T(Sal_{T} - B_{T})\n\\\\Sal_{T} = \\text{pre-tax cash procedds from sale of fixed capital}\n\\\\B_{T} = \\text{book value of the fixed capital sold}\n\\]\n\n\n\nimage-20211017200110833\n\n\n\n\n\nimage-20211017200117903\n\n\n\n\n\nimage-20211017200356823\n\n\n\n\n\n증분고려\n\n\n\nimage-20211017200148649\n\n\n\n\n\nimage-20211017200154973\n\n\n\n\n\nimage-20211017200202853\n\n\n\n\n\nimage-20211017200209278\n\n\n\n\n\nimage-20211017200422080\n\n\n\n\n\n\nInflation is a complication.\n\nAnalyzing nominal or real cash flows. - Nominal cash flows should be discounted at a nominal discount rate, while real cash flows should be discounted at a real discount rate.\n\nrate adjusted for inflation -> real rate\n\nChanges in inflation affect project profitability. - If inflation is higher than expected, future project cash flows are worth less, and the value of the project will be lower than expected. The opposite is also true, however. If inflation turns out to be lower than originally expected, future cash flows from the project will be worth more, effectively increasing the project’s value.\nInflation reduces the tax savings from deprecation.\nInflation decreases the value of payments to bondholders.\nInflation may affect revenues and costs differently.\n\n\n\n\n\n\nWhen two projects are mutually exclusive, the firm may choose one project or the other, but not both. It mutually exclusive projects have different lives, and the projects are expected to be replaced indefinitely as they wear out, an adjustment needs to be made in the decision-making process.\n\nLeast common multiple of lives approach - 최소공배수법\nEquivalent annual annuity (EAA) approach\n\n\n\n\nimage-20211017200951691\n\n\n\n\n\nimage-20211017200957730\n\n\n\n\n\nimage-20211017201011848\n\n\n\n\n\nimage-20211017201023378\n\n\n\n\n\nimage-20211017201029806\n\n\n\n\n\nimage-20211017202636161\n\n\n\n\n\nimage-20211017202642823\n\n\n\n\n\nCapital rationing is the allocation of a fixed amount of capital among the set of available projects that will maximize shareholder wealth. Greatest total NPV.\nNote that capital rationing is not the optimal decision from the firm’s perspective. Therefore, capital rationing violates market efficiency because society’s resources are not allocated to their best use (i.e., to generate the highest return).\nHard capital rationing occurs when the funds allocated to managers under the capital budget cannot be increased. Soft capital rationing occurs when managers are allowed to increase their allocated capital budget if they can justify to senior management that the additional funds will create shareholder value.\n\n\n\nimage-20211017202000892\n\n\n\n\n\nimage-20211017202006586\n\n\n\n\n\n\nSensitivity analysis involves changing an input (independent) variable to see how sensitive the dependent variable is to the input variable. For example, by varying sales, we could determine how sensitive a project’s NPV is to changes in sales, assuming that all other factors are held constant. The key to sensitivity analysis is to only change one variable at a time.\n\n\n\nimage-20211017202315045\n\n\nScenario analysis is a risk analysis technique that considers both the sensitivity of some key output variable to changes in a key input variable and the likely probability distribution of these variables. The key difference between scenario analysis and sensitivity analysis is that scenario analysis allows for changes in multiple input variables all at once.\n\n\n\nimage-20211017202309221\n\n\nSimulation analysis (or Monte Carlo simulation) results in a probability distribution of project NPV outcomes, rather than a limited number of outcomes as with sensitivity or scenario analysis.\n\n\n\nimage-20211017202358612\n\n\n\n\n\n\\[\n\\\\R_{project} = R_{F} + \\beta_{project}[E(R_{MKT}) - R_{F}]\n\\]\nUsing a project’s beta to determine discount rates is important when the risk of a project is different from the risk of the overall company. Simply using the company’s weighted average cost of capital (WACC) will overstate the required return for a conservative (low beta) project and will understate the required return for an aggressive (high beta) project.\n\n\n\nReal options allow managers to make future decisions that change the value of capital budgeting decisions made today.\n\nTiming options\nAbandonment options are similar to put options. They allow management to abandon a project if the present value of the incremental cash flows from exiting a project exceeds the present value of the incremental cash flows from continuing a project.\nExpansion options are similar to call options.\nFlexibility options\n\nprice-setting\nproduction-flexibility\n\nFundamental options are projects that are options themselves because the payoffs depend on the price of an underlying asset. For example, the payoff for a copper mine is dependent on the market price for copper.\n\nDifferent approaches for evaluating the profitability of an investment with real options.\n\nDetermine the NPV of the project without the option.\nCalculate the project NPV without the option and add the estimated value of the real option\nUse decision trees\nUse option pricing models\n\n\n\n\nimage-20211017203306791\n\n\n\n\n\nimage-20211017203313306\n\n\n\n\n\nimage-20211017203319965\n\n\n\n\n\nimage-20211017202651779\n\n\n\n\n\nCommon mistakes\n\nFailing to incorporate economic responses into the analysis\nMisusing standardized templates\nPet projects of senior management - 경영자 선호 project\nBasing investment decision on EPS or ROE\nUsing the IRR criterion for project decisions\nPoor cash flow estimation\nMisestimation of overhead costs\nUsing the incorrect discount rate\nPolitics involved with spending the entire capital budget - Many managers try to spend their entire capital budget each year and ask for an increase for the following year.\nFailure to generate alternative investment ideas\nImproper handling of sunk and opportunity costs\n\n\n\n\nEconomic income \\[\n\\\\\\operatorname{economic income} = \\operatorname{cash flow} + (\\operatorname{ending market value} - \\operatorname{beginning market value}) = \\operatorname{cash flow} - \\operatorname{economic depreciation}\n\\] A project’s accounting income is the reported net income on a company’s financial statements that results from an investment in a project. Accounting income will differ from economic income because:\n\nAccounting depreciation is based on the original cost (not market value) of the investment.\nFinancing costs are considered as a separate line item and subtracted out to arrive at net income. In the basic capital budgeting model, financing costs are reflected in the WACC.\n\n\n\n\nimage-20211017203855837\n\n\n\n\n\nimage-20211017203901374\n\n\n\n\n\nimage-20211017203907124\n\n\nThe accounting income differs from the economic income for two reasons.\n\nAccounting depreciation is based on the original cost of the investment, while economic depreciation (beginning - ending value) is based on the market value of the asset. The economic depreciation for the project is much larger than the accounting depreciation, resulting in an economic income amount that is much smaller than accounting income.\nInterest expense is deducted from the accounting income figure. Interest expense is ignored when computing economic income because it is reflected in the WACC.\n\n\n\n\nEconomic profit is a measure of profit in excess of the dollar cost of capital invested in a project. \\[\n\\\\EP = NOPAT - \\$WACC\n\\\\NOPAT = \\text{net operating profit after tax} = EBIT(1 - t)\n\\\\\\$WACC = \\text{dollar cost of capital} = WACC * capital\n\\\\Capital = \\text{dollar amount of investment}\n\\]  \\[\n\\\\NPV = MVA = \\sum_{t = 1}^{\\infty}{\\frac{EP_{t}}{(1 + WACC)^{t}}}\n\\\\MVA = \\text{market value added}\n\\] Residual income focuses on returns on equity and is determined by subtracting an equity charge from the accounting net income. \\[\n\\\\\\operatorname{residual income} = \\operatorname{net income} - \\operatorname{equity charge}\n\\\\RI_{t} = NI_{t} - r_{e}B_{t - 1}\n\\\\NPV = \\sum_{t = 1}^{\\infty}{\\frac{RI_{t}}{(1 + r_{e})^{t}}}\n\\] The residual income approach focuses only on returns on equityholders; therefore, the appropriate discount rate is the required return on equity.\n\n\n\nimage-20211017205118120\n\n\n\n\n\nimage-20211017205123097\n\n\nThe claims valuation approach (청구권 방식, FCFF, FCFE 활용) divides operating cash flows based on the claims of debt and equityholders that provide capital to the company. These debt and equity cash flows are valued separately and then added together to determine the value of the company.\nThe claims valuation method calculate the value of the company not the project. This is different from the economic profit and residual income approaches, which calculate both project and company value.\nThe claims valuation approach is based on the balance sheet concept that assets equal liabilities plus equity.\n\nThe cash flows to debtholders consist of interest and principal payments and are discounted at the cost of debt.\nThe cash flows to equityholders are dividends and share repurchases and are discounted at the cost of equity.\n\nThe sum of the present value of each stream of cash flows will equal the value of the company.\n\n\n\nimage-20211017205412535\n\n\n\n\n\nimage-20211017205421790\n\n\n\n\n\nimage-20211017205532597\n\n\n\n\n\nimage-20211017205538033\n\n\n\n\n\nimage-20211017205544205\n\n\n\n\n\nimage-20211017205550042\n\n\n\n\n\nimage-20211017205556196\n\n\n\n\n\nimage-20211017205601907\n\n\n\n\n\nimage-20211017205607235\n\n\n\n\n\nimage-20211017215634145\n\n\n\n\n\nimage-20211017215641187\n\n\n\n\n\nimage-20211017215647237\n\n\n\n\n\nimage-20211017215654934\n\n\n\n\n\n\n\n\nimage-20211017205619151\n\n\n\n\nProgression is from MM 1958 (no taxes, no costs of financial distress) to MM 1963 (with taxes, no costs of financial distress) to the static trade-off theory (with taxes and with costs of financial distress).\nIn moving to MM 1963, we are able to see how introducing taxes affects the cost of capital and firm value. Remember, the goal of managers in a capital structure decision is to minimize the weighted average cost of capital (and thereby maximize the value of the company).\n\n\n\nIn 1958, Professors Franco Modigliani and Merton Miller (MM) published their seminal work on capital structure theory. Under a very restrictive set of assumptions, MM proved that the value of a firm is unaffected by its capital structure. MM’s results suggest that in a perfect world, it does not matter how a firm finances its operations.\nAssumptions\n\nCapital markets are perfectly competitive - There are no transaction costs, taxes, or bankruptcy costs.\nInvestors have homogeneous expectations\nRiskless borrowing and lending - Investors can borrow/lend at the risk-free rate.\nNo agency costs - No conflict of interest between managers and shareholders.\nInvestment decisions are unaffected by financing decisions - Operating income is independent of how assets are financed.\n\nIn the MM no-tax world, the value of a company is not affected by its capital structure.\n\n\n\nimage-20211017213434459\n\n\nConsider why the pie analogy holds true. The operating earnings (EBIT) of a firm are available to all providers of capital. In a company with no debt, all of the operating earnings are available to equityholders, and the value of the company is the discounted present value of these earnings. \\[\n\\\\V_{L} = V_{U}\n\\\\V_{L} = \\text{value of levered firm}\n\\\\V_{U} = \\text{value of unlevered firm}\n\\] Given our assumptions, an investor can have homemade leverage.\n\n\n\nMM’s second proposition with no taxes states that the cost of equity increases linearly as a company increases its proportion of debt financing. Therefore, the benefits of using a larger proposition of debt as a cheaper source of financing are offset by the rise in the cost of equity, resulting in no change in the firm’s weighted average cost of capital (WACC). \\[\n\\\\r_{e} = r_{0} + \\frac{D}{E}(r_{0} - r_{d})\n\\] As leverage increases, the cost of equity increases, but WACC and the cost of debt are unchanged.\n\n\n\nimage-20211017213748033\n\n\nMM’s second proposition supports their first proposition. Because the benefits of lower cost debt are offset by the increased cost of equity, the relative amount of debt versus equity in the firm’s capital structure does not affect the overall value of the firm.\n\n\n\nTax shield provided by debt. Under the tax code of most countries, interest payments are a pretax expense and are therefore tax deductible, while dividends are paid on an after-tax basis. The differential tax treatment encourages firms to use debt financing because debt provides a tax shield that adds to the value of the company. \\[\n\\\\V_{L} = V_{U} + t * d\n\\] If we maintain MM’s other assumptions, the value of the company increases with increasing levels of debt, and the optimal capital structure is 100% debt.\n\n\n\n\\[\n\\\\r_{E} = r_{0} + \\frac{D}{E}(r_{0} - r_{D})(1 - T_{C})\n\\]\n\n\n\nimage-20211017214135286\n\n\n\n\n\nCosts of financial distress - static-trade-off theory (1973)\n\nCosts of financial distress and bankruptcy can be direct or indirect. Direct costs of financial distress include the cash expenses associated with the bankruptcy, such as legal fees and administrative fees. Indirect costs include foregone investment opportunities and the costs that result from losing the trust of customers, creditors, suppliers, and employees.\nProbability of financial distress is related to the firm’s use of operating and financial leverage.\n\nAgency costs of equity refer to the costs associated with the conflicts of interest between managers and owners. Managers who do not have a stake in the company do not bear the costs associated with excessive compensation or taking on too much (or too little) risk.\nNet agency cost of equity\n\nMonitoring costs\nBonding costs\nResidual losses\n\nAccording to agency theory, the use of debt forces managers to be disciplined with regard to how they spend cash because they have less free cash to use for their own benefit.\nCost of asymmetric information refer to costs resulting from the fact that managers typically have more information about a company’s prospects and future performance that owners or creditors. Firms with complex products or little transparency in financial statements tend to have higher costs of asymmetric information, which results in higher required returns on debt and equity capital.\nBecause shareholders and creditors are aware that the asymmetric information problems exist, these investors will look for management behavior that ‘signals’ what knowledge management may have. Specifically, management’s choice of debt or equity financing may provide a signal regarding management’s opinion of the firm’s future prospects.\n\nTaking on the commitment to make fixed interest payments through debt financing sends a signal that management has confidence in the firm’s ability to make these payment in the future.\nIssuing equity is typically viewed as a negative signal that managers believe a firm’s stock is overvalued.\n\nPecking order theory, based on asymmetric information, is related to the signals management sends to investors through its financing choices. Financing choices under pecking order theory follow a hierarchy based on visibility to investors with internally generated capital being the most preferred, debt being the next best choice, and external equity being the least preferred financing option.\n\nInternally generated equity (RE)\nDebt\nExternal equity (newly issued shares)\n\nTherefore, the pecking order theory predicts that the capital structure is a by-product of the individual financing decisions.\n\n\n\nThe static trade-off theory seeks to balance the costs of financial distress with the tax shield benefits from using debt. Under the static trade-off theory, there is an optimal capital structure that has an optimal proportion of debt.\nIf we remove the assumption that there are no costs of financial distress, there comes a point where the additional value added from the debt tax shield is exceeded by the value-reducing costs of financial distress from the additional borrowing. This point represents the optimal capital structure for a firm where the WACC is minimized and the value of the firm is maximized. \\[\n\\\\V_{L} = V_{U} + t * d - PV(\\operatorname{costs of financial distress})\n\\] \n\n\n\nimage-20211017215253064\n\n\n\n\n\nimage-20211017215339695\n\n\n\n\n\nimage-20211017215347996\n\n\n\n\n\nimage-20211017215406596\n\n\n\n\n\nimage-20211017215430554\n\n\n\n\n\nimage-20211017215443414\n\n\n\n\n\nimage-20211017215521917\n\n\n\n\n\nimage-20211017215545652\n\n\n\n\n\nimage-20211017215552466\n\n\n\n\n\n\nTarget capital structure\nFor managers trying to maximize the value of the firm, the target capital structure will be the same as the optimal capital structure.\n\nManagement may choose to exploit opportunities in a specific-financing source. For example, a temporary rise in the firm’s stock price may create a good opportunity to issue additional equity.\nMarket value fluctuations will occur. Changes in stock and bond markets will cause fluctuations in the firm’s stock and bond prices.\n\n\n\n\nDebt ratings\n\n\n\nimage-20211017220434928\n\n\n\n\n\n\nChanges in the company’s capital structure over time.\nCapital structure of competitors with similar business risk.\nCompany-specific factors (e.g., quality of corporate governance)\n\n\n\n\n\n\n\nimage-20211017220611233\n\n\n\n\n\n\n\n\nimage-20211018040012018\n\n\n\n\n\nimage-20211018040552671\n\n\n\n\n\nimage-20211018040600842\n\n\n\n\n\nimage-20211017220732343\n\n\n\nRegular cash dividends\nExtra or special (irregular) dividends\nLiquidating dividend\nStock dividend\nStock splits\n\n\n\n\nDividend irrelevance. Merton Miller and Franco Modigliani (MM) maintain that dividend policy is irrelevant, as it has no effect on the price of a firm’s stock or its cost of capital. MM’s argument of dividend irrelevance is based on their concept of homemade dividends. Assume, for example, that you are a shareholder and you don’t like the firm’s dividend policy. If the firm’s cash dividend is too big, you can just take the excess cash received and use it to buy more of the firm’s stock. If the cash dividend you received was too small, you can sell a little bit of your stock in the firm to get the cash flow you want. In either case, the combination of the value of your investment in the firm and your cash in hand will be the same.\nBird-in-hand (dividend preference theory) argument for dividend policy.\nMyron Gordon and John Lintner, however, argue that \\(r_{s}\\) decreases as the dividend payout increases. Why? Because investors are less certain of receiving future capital gains from the reinvested earnings than they are of receiving current (and therefore certain) dividend payments.\nTax aversion. In many countries, dividends have historically been taxed at higher rates than capital gains. In the 1970s, US tax rates on dividend income were as high as 70%, while the taxes on capital gains were 35%.\nIn the real world, tax laws often prevent companies form accumulating excess earnings, making dividend payments necessary. Also note that in 2003, tax laws in the United States changed so that dividends and long-term capital gins are both taxed at the same 15% rate.\nConclusions from the three theories. The results of empirical tests are unclear as to which of these theories best explains the empirical observations of dividend policy.\n\n\n\nInformation asymmetry refers to differences in information available to a company’s board and management (insiders) as compared to the investors (outsiders).\nThe information conveyed by dividend initiation is ambiguous. On one hand, a dividend initiation could mean that a company is optimistic about the future and is sharing its wealth with stockholders-a positive signal. On the other hand, initiating a dividend could mean that a company has a lack of profitable reinvestment opportunities-a negative signal.\nAn unexpected dividend increase can signal to investors that a company’s future business prospects are strong and that managers will share the success with shareholders. Studies have found that companies with a long history of dividend increase, such as GE and Exxon Mobil, are dominant in their industries and have high returns on assets and low debt ratios.\nUnexpected dividend decreases or omissions are typically negative signals that the business is in trouble and that management does not believe that the current dividend payment can be maintained.\nmarket에 시그널을 줄 수 있다.\n\n\n\n\n\nClientele effect. This refers to the varying dividend preferences of different groups of investors, such as individuals, institutions, and corporations.\n\nTax considerations - High-tax-bracket investors (like some individuals) tend to prefer low dividend payouts, while low-tax-bracket investors (like corporations and pension funds) may prefer high dividend payouts.\n\nWhen the stock goes ex-dividend: \\[\n\\\\\\Delta{P} = \\frac{D(1 - T_{D})}{1 - T_{CG}}\n\\]\n\nRequirements of institutional investors. For legal or strategic reasons, some institutional investors will invest only in companies that pay a dividend or have a dividend yield above some target threshold.\n\n\n\n\nBetween shareholders and managers: Agency costs reflect the inefficiencies due to divergence of interests between managers and stockholders. One aspect of agency issue is that managers may have an incentive to overinvest (“empire building”). This may lead to investment in some negative NPV projects, which reduces stockholders wealth. One way to reduce agency cost is to increase the payout of free cash flow as dividends. Generally, it makes sense for growing firms to retain a larger proportion of their earnings.\nBetween shareholders and bondholders: When there is risky debt outstanding, shareholders can pay themselves a large dividend, leaving the bondholders with a lower asset base as collateral.\n\n\n\nimage-20211018004125250\n\n\n\n\n\n\n\nInvestment opportunities - If a firm faces many profitable investment opportunities and has to react quickly to capitalize on the opportunities, the dividend payout may be low.\nExpected volatility of future earnings - Hence, when earnings are volatile, firms are more cautious in changing dividend payout.\nFinancial flexibility - Firms with excess cash and a desire to maintain financial flexibility may resort to stock repurchases instead of dividends as a way to pay out excess cash. Since stock repurchase plans are not considered sticky (i.e., there is no implicit expectation by the market of an ongoing repurchase program), they don’t entail reduction in financial flexibility going forward.\nTax considerations - Investors are concerned about after-tax returns.\nFloatation costs - When a company issues new shares of common stock, a floatation cost of 3% to 7% is taken from the amount of capital raised to pay for investment bankers and other costs associated with issuing the new stock. Since retained earnings have no such fee, the cost of new equity capital is always higher than the cost of retained earnings. Generally, the higher the floatation costs, the lower the dividend payout, given the need for equity capital in positive NPV projects.\nContractual and legal restrictions - Companies may be restricted from paying dividends either by legal requirements or by implicit restrictions caused by cash needs of the business.\n\nImpairment of capital rule\nDebt covenants\n\n\n\n\n\nDouble-taxation system\n\n\n\nimage-20211017223108553\n\n\nSplit-rate corporate tax system\nThe calculation of the effective tax rate under a split rate system is similar to the computation of the effective tax rate under double taxation except that the corporate tax rate applicable would be the corporate tax rate for distributed income\n\n\n\nimage-20211018000243493\n\n\nUnder an imputation tax system, taxes are paid at the corporate level but are attributed to the shareholder, so that all taxes are effectively paid at the shareholder rate.\n\n\n\nimage-20211018000319720\n\n\n\n\n\nimage-20211018000329959\n\n\n\n\n\n\n\n\nimage-20211018000500042\n\n\n\n\n\n\n\nimage-20211018000658312\n\n\n\n\n\n\n\n\nimage-20211018000704109\n\n\n\n\n\n\n\n\nimage-20211018000711086\n\n\nAdvantages of the residual dividend model:\n\nThe model allows management to pursue profitable investment opportunities without being constrained by dividend considerations.\n\nDisadvantages of the residual dividend model:\n\nIf a firm follows the residual dividend policy, its dividend payments may be unstable. Investment opportunities and earnings often vary from year to year.\n\n\n\n\n\nFour common methods are used for share buybacks, though due to varying rules, some markets may not allow all methods. Outside the United States and Canada, method 1 is used almost exclusively.\n\nOpen market transactions are the most flexible approach, allowing a company to buy back its shares in the open market at the favorable terms.\nFixed price tender offer is an approach where the firm buys a predetermined number of shares at a fixed price, typically at a premium over the current market price. It allows a company to buy back its shares rather quickly. If more than the desired number of shares are tendered in response to the offer, the company will typically buy back a prorated number of shares from each shareholders responding to the offer.\nDutch auction is a tender offer in which the company specifies not a single fixed price but rather a range of prices. Dutch auctions identify the minimum clearing price for the desired number of shares that need to be repurchased. Each participating shareholder indicates the price and the number of shares tendered. Bids are accepted based on lowest price first until the desired quantity is filled.\nRepurchase by direct negotiation entails purchasing shares from a major shareholder, often at a premium over market price. This method is often used in a greenmail scenario (where a hostile bidder is offered a premium to go away) to the detriment of the remaining shareholders.\n\n\n\n\n\n\n\nimage-20211018003032756\n\n\n\n\n\n\n\n\nimage-20211018003107909\n\n\n\n\n\nimage-20211018003113202\n\n\n\n\n\n\nPotential tax advantages - When the tax rate on capital gains is lower than the tax rate on dividend income, share repurchases have a tax advantage over cash dividends.\nShare price support/signaling - Companies may purchase their own stock, thereby signaling to the market that the company views its own stock as a good investment.\nAdded flexibility - A company could declare a regular cash dividend and periodically repurchase shares as a supplement to the dividend. Unlike dividends, share repurchases are not a long-term commitment. Since paying a cash dividend and repurchasing shares are economically equivalent.\nOffsetting dilution from employee stock options - Repurchases offset EPS dilution that results from the exercise of employee stock options.\nIncreasing financial leverage\n\n\n\n\nimage-20211018003404416\n\n\n\n\n\nimage-20211018003409746\n\n\n\n\n\nimage-20211018003414478\n\n\n\n\n\nimage-20211018003422065\n\n\n\n\n\nimage-20211018003451036\n\n\n\n\n\nimage-20211018003517189\n\n\n\n\n\n\nA lower proportion of US companies pay dividends compared to their European counterparties.\nGlobally, in developed markets, the proportion of companies paying cash dividends has trended downwards over the long term.\nThe percentage of companies making stock repurchases has been trending upwards in the United States since the 1980s and in the United Kingdom and continental Europe since the 1990s.\n\n\n\n\n\n\n\nDividend payout ratio = dividends / net income\ndividend coverage ratio = net income / dividends\n\n\n\nimage-20211018004044901\n\n\n\n\n\n\n\nConcentrated ownership, where a single shareholder or a group of shareholders have control over the corporation. These controlling shareholders can be a family, other companies, or a sovereign entity. Dispersed ownership refers to the situation where the shareholders are numerous and non has control.\nPercentage ownership is not always a reliable indicator of concentration of control: certain shareholders may have a greater degree of control than their ownership percentage would suggest. A minority shareholder could have control of a corporation through a vertical ownership arrangement (where the group has a controlling interest in holding companies, which in turn have controlling interests in operating companies) or a horizontal ownership arrangement (where companies with common suppliers or customers cross-hold each other’s shares).\nAnother disconnect between control and percentage ownership comes in the form of dual-class shares, wherein one class of shareholders has fewer voting rights, while the other class has superior voting rights.\n\n\n\nDispersed ownership and dispersed voting power describes a situation where shareholders (called weak shareholders) do not hold power over managers (called strong managers). In this situation, principal-agent conflict is likely: shareholders want shareholder value maximized, while managers may use the firm’s resources to their own advantages. This problem can be mitigated by the presence of controlling shareholders.\nConcentrated ownership and concentrated voting power refers to a situation where so-called ‘strong’ shareholders hold power over minority shareholders and ‘weak’ managers. This arrangement allows controlling shareholders to control the board of directors and effectively control and monitor management. The downside of this situation is that a principal-principal problem may arise: controlling owners can take advantage of firm resources to the detriment of minority owners.\nDispersed ownership and concentrated voting power indicates that the majority of the shares of a company are not owned by strong controlling shareholders. Rather, the controlling shareholders gain control over other minority shareholders through pyramid structures or dual-class shares, despite the controlling shareholders having less-than-majority ownership (again, the principal-principal problem). These controlling shareholders’ voting power also allows them to monitor management.\nConcentrated ownership and dispersed voting power occurs in the presence of voting caps, where the voting rights of large share positions are restricted. Sovereign countries sometimes enact voting caps to discourage foreign investors from taking a controlling position in a company belonging to an industry that is considered important.\n\n\n\n\nBanks - When a bank lends money to a firm or holds an equity stake, the bank can often exert some control over that corporation. (This is especially common in Asia and Europe.) The bank does not take advantage of its role as lender at the expense of other shareholders.\nFamilies - In Latin America and some other places, family ownership is common. One advantage of family control is that principal-agent issues may be reduced. On the other hand, family ownership can make it difficult to recruit quality outsiders for management and often leads to lack of concern for minority shareholders, as well as minimal transparency and low accountability by management.\nState-owned enterprises (SOE) - A listed SOE is partly owned by the government and also trades on an exchange.\nInstitutional investors - In many countries, institutional investors can represent a large portion of equity ownership.\nGroup companies - Such as South Korea’s Samsung, can achieve an outsized amount of control through cross-holding of shares via vertical and horizontal ownership.\nPrivate equity firms - Introducing performance-based compensation for managers, or the addition of codes.\nForeign investors - Foreign investors typically demand greater accountability and transparency. This is particularly true in emerging market countries.\nManagers and board directors\n\n\n\n\n\nDirector independence - When a board member has no significant remuneration, ownership, or employment relationship with the firm, the board member is considered independent. Independent directors are important in countries with dispersed ownership where the principal-agent problem is greater and thus the board’s role of monitoring mangers is key.\nBoard structures - Under a two-tier structure, the management board is overseen by a supervisory board. This supervisory board performs functions such as determining management compensation, supervising external auditors, and reviewing the firm’s financial records. In some jurisdictions, representatives of stakeholders such as labor groups sit on the supervisory board.\nSpecial voting arrangements - Some countries attempt to provide an advantage to minority shareholders through special voting arrangements for board nomination and election.\nCorporate governance codes, laws, and listing requirements - Some countries have adopted national ‘comply or explain’ corporate governance codes that require firms to either best practices of corporate governance or explain why they have not.\nStewardship codes - Stewardship codes exist in some countries that seek to engage investors in corporate governance by exercising their legal rights.\n\n\n\n\n\nWhen a company is not behaving according to stakeholders’ wishes, shareholder activism may occur: this is the term for techniques used by shareholders to force management to act in shareholders’ interests.\n\n\n\nStructure of board of directors - CEO duality occurs when the chairperson of the board is also the chief executive officer (CEO). CEO duality raises concerns that the chairperson’s oversight and monitoring responsibilities may not be effectively rendered.\nBoard independence - Ideally, a majority of board members should be independent; independent directors are more likely to prevent management from self-serving behavior.\nBoard committees - Such as compensation, nomination, and audit committees. When analyzing a corporation’s board, an analysts should consider whether the key committees related to financial reporting, management selection, and compensation are sufficiently independent.\nSkills and experience of board\nComposition of board - It has been observed in recent years that small, diverse boards of directors are generally more effective than large boards made up of members with similar backgrounds.\nOther board evaluation considerations. - A board of directors may be evaluated from a number of perspectives.\n\n\n\n\nClawback policies allow firms to reclaim past compensation if inappropriate conduct comes to light later. Say-on-pay rules can give stakeholders the opportunity to vote on executive compensation. The pay differential between the CEO and the firm’s average worker has come under scrutiny in recent years.\nearn-out - guarantee한 성과가 나오면 compensation을 더 주는 조항\n\n\n\nHolding various classes of shares. Some firms structure share offerings as a single class with equal voting rights. Alternatively, in a dual-class structure, the shares held by the firm’s founders or management have more voting power than hose sold to external investors.\n\n\n\n\n\n\n\nESG data providers\nIndustry organizations\nProprietary methods - ESG data specific to a particular firm can be derived from sources such as 10-K regulatory filings, corporate sustainability reports, and annual reports.\n\n\n\n\n\n\nFixed-income analysts usually will focus on ESG factors’ downside risk.\n\n\n\nEquity analysts consider both the upside and downside impact of ESG factors when valuing a firm’s stock.\n\n\n\n\n\n\n\nFor example, a company’s projected cash flow statements or income statement may be adjusted in terms of earnings, margins, revenues, costs, capital expenditures, or other items. Adjustments to the balance sheet often take the form of altering the value of assets of reflect impairment. The credit spread of a fixed-income instrument may be adjusted to reflect ESG concerns. Similarly, an adjustment to discount rate or cost of capital may be used to reflect ESG considerations in an equity analysis.\nGreen bonds are fixed-income instruments used to fund projects related to the environment.\n\n\n\n\nEnvironment factors - Concerned about wasting water, a soft-drink company has been able to lower its water usage in its manufacturing process over the past three years.\nSocial factors - A drug company has experienced a long string of product recalls, product quality controversies, and fines and warning letters from regulators. Negative valuation impacts for stock and bondholders are also appropriate, especially if the firm’s brand reputation is impaired.\nGovernance factors - Compared to its peers, a bank’s board is found to be lagging in a number of measure: the bank’s chairperson is not independent, the board’s overall independence and diversity is low, board members’ industry experience is low, and a number of board members have long tenures. In addition to these factors, the bank has a higher ratio of nonperforming loans than its peers. To reflect these additional risks, the analyst uses a higher risk premium to value the bank’s stock and also increases the credit spread used to value the bank’s debt.\n\n\n\n\n\n\n\n\nimage-20211018040633506\n\n\nThe terms mergers and acquisitions, or M&A for short, generally refers to two businesses combining in some manner. But almost always someone is left unhappy (usually the managers that are removed, or workers that will be laid off).\nAn acquisition refers to one company buying only part of another company. If the acquirer absorbs the entire target company, the transaction is considered a merger. The initiator of the venture is referred to as the bitter, or acquirer, while the opposite side of the transaction is known as the target.\n\n\n\n\nThe acquiring company acquires all of the target’s assets and liabilities. As a result, the target company ceases to exist as a separate entity. Note that in a statutory merger, the target company is usually smaller than the purchaser, but this is not always the case.\n흡수합병, A + B = A\n\n\n\nAcquisition\n\n\n\n신설합병, A + B = C\n\n\n\n\n\n\nThe two business operate in the same or similar industries, and may often be competitors.\n\n\n\nThe acquiring company seeks to move up or down the product supply chain.\nForward integration, where the acquirer is moving up the supply chain toward the ultimate consumer.\nBackward integration, the company is moving down the supply chain toward the raw material inputs.\n\n\n\n다각적 인수\nThe two companies operate in completely separate industries.\n\n\n\n\n\nSynergies - Cost synergies are exactly the strategy behind a pure horizontal merger. Revenue synergies are typically created by cross-selling products, increasing market share, or raising prices to take advantage of reduced competition.\nAchieving more rapid growth - External growth via M&A activity is usually a much faster way for managers to increase revenues than making investments internally (i.e., organic growth). Growth through M&A is especially common in mature industries where organic growth opportunities are limited. In addition, it is typically a less risky way to generate growth by acquiring resources through a merger with another company rather than developing them internally.\nIncreased market power - When a horizontal merger occurs in an industry with few competitors, the newly combined company will typically come away with increased market share and a greater ability to influence market prices. Vertical mergers may also increase market power by reducing dependence on outside suppliers.\nGaining access to unique capabilities - If a company is lacking a specific capability or resource, it can either try to develop it internally or seek to acquire something that already exists. M&A activity can be a cost effective way to acquire proven capabilities or resources.\nDiversification - Managers may cite the need to diversify the firm’s cash flows as grounds for a merger. This makes no sense for shareholders but may be rational for the managers. It is much easier and cheaper for the shareholders to diversify simply by investing in the shares of unrelated companies themselves rather than having one company go through the long, expensive process of acquiring and merging the two firms’ operations and corporate cultures. In fact, research has revealed that conglomerates trade at a discount relative to the sum of the value of individual businesses. In this case, the whole is less than the sum of the individual parts. This finding demonstrates that mergers are not likely to increase value purely for diversification reasons.\nBootstrapping EPS - Another motivation for mergers is the bootstrapping effect on earnings per share that sometimes results from a stock deal.\nPersonal benefits for managers - This means that there is a strong financial incentive for managers to maximize the size of the firm rather than shareholder value. In addition, being part of the executive team for a larger company implies greater power and prestige and is probably good for managerial egos.\nTax benefits\nUnlocking hidden value - When a company has struggled for an extended period of time, an acquirer may believe it can pay a lower price to buy the company and unlock hidden value by improving managements, adding resources, or improving the organizational structure.\nAchieving international business goals\n\nTaking advantages of market inefficiencies - Acquiring a manufacturing plant in a country where labor costs are less expensive is a prime example of gaining an advantage from an inefficient global marketplace.\nWorking around disadvantageous government policies - International M&A is a potential way to overcome barriers to free trade, such as tariffs or quotas.\nUse technology in new markets\nProduct differenciation\nProvide support to existing multinational clients\n\n\n\n\nBootstrapping is a way of packaging the combined earnings from two companies after a merger so that the merger generates an increase in the earnings per share of the acquirer, even when no real economic gains have been achieved.\nThe ‘Bootstrap effect’ occurs when a high P/E firm (generally a firm with high growth prospects) acquires a low P/E firm (generally a firm with low growth prospects) in a stock transaction. Post-merger, the earnings of the combined firm are simply the sum of the respective earnings prior to the merger. However, by purchasing the firm with a lower P/E, the acquiring firm is essentially exchanging higher-priced shares of lower-priced shares. As a result, the number of shares outstanding for the acquiring firm increases, but at a ratio that is less than 1-for-1. When we compute the EPS for the combined firm, the numerator (total earnings) is equal to the sum of the combined firms, but the denominator (total shares outstanding) is less than the sum of the combined firms. The result is higher reported EPS, even when the merger creates no additional synergistic value.\n\n\n\nimage-20211018023952955\n\n\nIn practice, the market tends to recognize the bootstrapping effect and post-merger P/E’s adjust accordingly. However, there have been periods in history, such as the technology bubble in the late 1990s, where bootstrapping helped high P/E companies show EPS growth, even in cases where the mergers created no value for shareholders.\n\n\n\nimage-20211018040648612\n\n\n\n\n\n\n\n\nimage-20211018024118507\n\n\n\n\n\n\n\n\nstock purchase\n\nFinally, most stock purchases involves purchasing the entire company and not just a portion of it. This means that not only will the acquirer gain the target company’s assets, but it will also assume the target’s liabilities.\n\nasset purchase\n\nAsset purchase acquisitions usually focuses on specific parts of the company that are of particular interest to the acquirer, rather than the entire company, which means that the acquirer generally avoids assuming any of the target company’s liabilities.\n\n\n\n\n\nimage-20211018024339238\n\n\n\n\n\n\n\nexchange ratio\nThe total compensation ultimately paid by the acquirer in a stock offering is based on three factors: the exchange ratio, the number of shares outstanding of the target company, and the value of the acquirer’s stock on the day the deal is completed.\n\n\n\nWhen an acquirer is negotiating with a target over the method of payment, there are three main factors that should be considered:\n\nDistribution between risk and reward for the acquirer and target shareholders. - In a stock offering, since the target company’s shareholders receive new shares in the post-merger company, they share in the risk related to the ultimate value that is realized from the merger. In a cash offering, all of the risk related to the value of the post-merger company is borne by the acquirer. As a result, when the acquirer is highly confident in the synergies and value that will be created by the merger, it is more inclined to push for a cash offering.\nRelative valuations of companies involved. - If the acquirer’s shares are considered overvalued by the market, the acquirer is likely to want to use its overpriced shares as currency in the merger transaction. In fact, investors sometimes interpret a stock offering as a signal that the acquirer’s shares may be overvalued.\nChanges in capital structure. - If the acquirer borrows money to raise cash for a cash offering, the associated debt will increase the acquirer’s financial leverage and risk. Issuing new stock for a securities offering can dilute the ownership interest for the acquirer’s existing shareholders.\n\n\n\n\n\nFriendly merger offers usually begin with the acquirer directly approaching the target’s management. If both parties like the idea of a potential deal, they will negotiate the method of payment and the terms of the transaction. At this point, each party to the merger will conduct due diligence on the other party by examining financial statements and other records.\nHostile merger offers\n\ntender offer - the acquirer offers to buy the shares directly from the target shareholders, and each individual shareholder either accepts or rejects the offer.\nproxy battle - the acquirer seeks to control the target by having shareholders approve a new ‘acquirer approved’ board of directors.\n\n\n\n\n\n\n\n\n\nflip-in pill - where the target company’s shareholders have the right to buy the target’s shares at a discount\nflip-over pill - where the target shareholders have the right to buy the acquirer’s shares at a discount.\nIn case of a friendly merger offer, most poison pill plans give the board of directors the right to redeem the pill prior to a triggering event.\n\n\n\nThese puts give bondholders the option to demand immediate repayment of their bonds if there is a hostile takeover. This additional cash burden may fend off a would-be acquirer.\n\n\n\nHistorically, Ohio and Pennsylvania have been considered to provide target companies with the most protection.\n\n\n\nEach group is elected for a 3-year term in a staggered system: in the first year the first group is elected, the following year the next group is elected, and in the final year the third group is elected.\n\n\n\nimage-20211018035647856\n\n\n\n\n\n\n\n\nA supermajority provision in the corporate charter requires shareholder support in excess of a simple majority. For example, a supermajority provision may required 66.7%, 75%, or 80% of votes in favor of a merger.\n\n\n\nA fair price amendment restricts a merger offer unless a fair price is offered to current shareholders.\n\n\n\nGolden parachutes are compensation agreements between the target and its senior management that give the managers lucrative cash payouts if they leave the target company after a merger.\n\n\n\n\n\n\nThe first step in avoiding a hostile takeover offer is to simply say no. If the potential acquirer goes directly to shareholders with a tender offer or a proxy fight, the target can make a public case to the shareholders concerning why the acquirer’s offer is not in the shareholder’s best interests.\n\n\n\nThe basic idea is to file a lawsuit against the acquirer that will require expensive and time-consuming legal efforts to right. The typical process is to attack the merger on anti-trust grounds or for some violation of securities law. The courts may disallow the merger or provide a temporary injunction delaying the merger, giving managers more time to load up their defense or seek a friendly offer from a while knight.\n\n\n\nEssentially, greenmail is a payoff to the potential acquirer to terminate the hostile takeover attempt. Greenmail is an agreement that allows the target to repurchase its shares from the acquiring company at a premium to the market price.\n\n\n\nThe target company can submit a tender offer for its own shares.\n\n\n\nIn a leveraged recapitalization, the target assumes a large amount of debt that is used to finance share repurchases. Like the share repurchase, the effect is to create a significant change in capital structure that makes the target less attractive while delivering value to shareholders.\n\n\n\nAfter a hostile takeover offer, a target may decide to sell a subsidiary or major asset to a neutral third party. If the hostile acquirer views this asset as essential to the deal (i.e., a crown jewel), then it may abandon the takeover attempt. The risk here is that courts may declare the strategy illegal if a significant asset sale is made after the hostile bid is announced.\n\n\n\nAfter a hostile takeover offer, the target can defend itself by making a counteroffer to acquire the acquirer. In practice, the Pac-Man defense is rarely used because it means a smaller company would have to acquire a larger company, and the target may also lose the use of other defense tactics as a result of its counteroffer.\n\n\n\nA white knight is a friendly third party that comes to the rescue of the target company. The target will usually seek out a third party with a good strategic fit with the target that can justify a higher price than the hostile acquirer. In many cases, the white knight defense can start a bidding war between the hostile acquirer and the third party, resulting in the target receiving a very good price when a deal is ultimately completed. This tendency for the winner to overpay in a competitive bidding situation is called the winner’s curse.\n대신 인수해줄 사람 찾기\n\n\n\nThe target seeks a friendly third party that bus a minority stake in the target without buying the entire company.\n의결권 지원\n\n\n\n\n\nHHI (Herfindahl-Hirschman Index) \\[\n\\\\HHI = \\sum_{i = 1}^{n}{(MS_{i} * 100)^{2}}\n\\] \n\n\n\nimage-20211018031159064\n\n\n\n\n\n\n\n\nComparable company analysis uses relative valuation metrics for similar firms to estimate market value and then adds a takeover premium to determine a fair price for the acquirer to pay for the target.\n\n\n\nimage-20211018031345117\n\n\n\n\n\nimage-20211018031349930\n\n\n\n\n\nimage-20211018031356358\n\n\n\n\n\nimage-20211018031402055\n\n\n\n\nComparable transactions analysis uses details from recent takeover transactions of similar companies to estimate the target’s takeover value. The methodology behind the approach is very similar to the comparable company approach we just showed you, except that all of the comparables are firms that have recently been taken over. The biggest challenge is finding enough relevant takeover transactions for firms that are similar to the target being analyzed.\n\n\n\nimage-20211018031542012\n\n\n\n\n\nimage-20211018031546578\n\n\n\n\n\n\n\n\nAdvantages\n\nIt is relatively easy to model nay changes in the target company’s cash flow resulting from operating synergies or changes in cost structure that may occur after the merger.\nThe estimate of company value is based on forecasts of fundamental conditions in the future rather than on current data.\nThe model is easy to customize.\n\nDisadvantages\n\nThe model is difficult to apply when free cash flows are negative.\nEstimates of cash flows and earnings are highly subject to error, especially when those estimates are for time periods far in the future.\nDiscount rate changes over time can have a large impact on the valuation esimtate.\nEstimation error is a major concern since the majority of the estimated value for the target is based on the terminal value, which is highly sensitive to estimates used for the constant growth rate and discount rate.\n\n\n\n\nAdvantages\n\nData for comparable companies is easy to access.\nAssumption that similar assets should have similar values is fundamentally sound.\nEstimates of value are derived directly from the market rather than assumptions and estimates about the future.\n\nDisadvantages\n\nThe approach implicitly assumes that the market’s valuation of the comparable companies is accurate.\nUsing comparable companies provides an estimate of a fair stock price, but not a fair takeover price. An appropriate takeover premium must be determined separately.\nIt is difficult to incorporate merger synergies or changing capital structures into the analysis.\nHistorical data used to estimate a takeover premium may not be timely, and therefore may not reflect current conditions in the M&A market.\n\n\n\n\nComparable transaction analysis uses details from completed M&A deals for companies similar to the target being analyzed to calculate an estimated value for the target.\nAdvantages\n\nSince the approach uses data from actual transactions, there is no need to estimate a separate takeover premium.\nEstimates of value are derived directly from recent prices for actual deals completed in the marketplace rather than from assumptions and estimates about the future.\nUse of prices established by recent transactions reduces the risk that the target’s shareholders could file a lawsuit against the target’s managers and board of directors for mispricing the deal.\n\nDisadvantages\n\nThe approach implicitly assumes that the M&A market valued past transactions accurately. If past transactions were over or underpriced, the mispricing will be carried over toe the estimated value for the target.\nThere may not be enough comparable transactions to develop a reliable data set for use in calculating the estimated target value. If the analyst isn’t able to find enough similar companies, she may try to use M&A deals from other industries that are not similar enough to the deal being considered.\nIt is difficult to incorporate merger synergies or changing capital structures into the analysis.\n\n\n\n\n\n\n\n\nimage-20211018040726025\n\n\n\n\n\nimage-20211018040733760\n\n\n\n\n\nimage-20211018040742138\n\n\n\n\n\n\n\nimage-20211018033745234\n\n\n\n\n\n\n\n\nimage-20211018033756444\n\n\n\n\n\n\n\n\nimage-20211018033806574\n\n\n\n\n\nWith a cash offer, the target firm’s shareholders will profit by the amount paid over its current share price (i.e., takeover premium). However, this gain is capped at that amount.\nWith a stock offer, the gains will be determined in part by the value of the combined firm, because the target firm’s shareholders do not received cash and just walk away, but rather retain ownership in the new firm. Accordingly, for a stock deal we must adjust our formula for the price of the target: \\[\n\\\\P_{T} = N * P_{AT}\n\\] \n\n\n\nimage-20211018034021067\n\n\n\n\n\nimage-20211018034027765\n\n\n\n\n\n\n\n\nThis means that the acquirer will want to pay the lowest possible price (the pre-merger value of the target, \\(V_{T}\\)), while the target wants to receive the highest possible price (the pre-merger value of the target plus the expected synergies, \\(V_{T} + S\\))\n\n\n\nIn a cash offer, the acquirer assumes the risk and receives the potential reward form the merger, while the gain for the target shareholders is limited to the takeover premium.\n\n\n\nIn a stock offer, some of the risks and potential rewards from the merger shift to the target firm.\nThe main factor that affects the method of payment decision is confidence in the estimate of merger synergies. The more confident both parties are synergies will be realized, the more acquirer will prefer to pay cash and the more the target will prefer to receive stock. Conversely, if estimates of synergies are uncertain, the acquirer may be willing to shift some of the risk (and potential reward) to the target by paying for the merger with stock, but the target may prefer the guaranteed gain that comes from a cash deal.\n\n\n\n\nShort-term performance studies that look at stock returns before and after merger announcement dates conclude that targets gain approximately 30%, while acquirers lose stock value of between 1% and 3%.\nWinner’s curse\nManagers also may overestimate the synergies and expected benefits of the merger. This tendency is called managerial hubris.\nSome mergers do enhance value for the acquirer. Acquirer are likely to earn positive returns on a deal characterized by:\n\nStrong buyer\nLow premium\nFew bidders\nFavorable market reaction\n\n\n\n\n\n\nDivestitures refer to a company selling, liquidating, or spinning off a division or subsidiary.\n\n\n\nEquity carve-outs create a new, independent company by giving an equity interest in a subsidiary to outside shareholders. Shares of the subsidiary are issued in a public offering of stock, and the subsidiary becomes a new legal entity whose management team and operations are separate from the parent company.\n\n\n\n\n\n\nimage-20211018040534546\n\n\nSpin-offs are like carve-outs in that they create a new independent company that is distinct from the parent company. The primary difference is the shares are not issued to the public, but are instead distributed proportionately to the parent company’s shareholders. This means that the shareholder based of the spin-off will be the same as that of the parent company, but the management team and operations are completely separate.\n인적분할\n\n\n\nSplit-offs allow shareholders to receive new shares of a division of the parent company in exchange for a portion of their shares in the parent company. The key here is that shareholders are giving up a portion of their ownership in the parent company to receive the new shares of stock in the division.\n물적분할\n\n\n\n\n\n\n\nDivision no longer fits into management’s long-term strategy\nLack of profitability\nIndividual parts are worth more than the whole\nInfusion of cash"
  },
  {
    "objectID": "posts/2021-10-19-digital-system-circuits-midterm/2021-10-19-digital-system-circuits-midterm.html",
    "href": "posts/2021-10-19-digital-system-circuits-midterm/2021-10-19-digital-system-circuits-midterm.html",
    "title": "Digital System Circuits midterm",
    "section": "",
    "text": "Digital signal - signal that at any time one of a finite of possible values\nAnalog signal or Continuous signal - one of any infinite of possible values\nAnalog signal -> sampling, quantization, coding -> digital signal\n\n\n\nA single binary signal: binary digit or bit\nTypically represented as 0 and 1, on or off\nDigital system - takes digital input, generates digital output\nDigital circuit - digital components that together comprise a digital system\n\n\n\nimage-20211019142744917\n\n\n\n\n\n\n\n7- (or 8-) bit encoding of each letter, number, or symbol\n\n\n\nIncreasingly popular 16-bit encoding\n데이터 교환을 원활하게 하기 위하여 문자 1개에 부여되는 값을 16bit으로 통일\n\n\n\n\nMost important use of digital circuit - perform arithmetic computations\nto perform arithmetic computation -> encode numbers as bits\n\n\n\n\n\n\nimage-20211019143049569\n\n\n\n\n\n\n\n\nimage-20211019143143025\n\n\n\n\n\nimage-20211019143152970\n\n\n\n\n\nimage-20211019143202562\n\n\n\n\n\n\n\n\nimage-20211019143232866\n\n\n\n\n\nimage-20211019143917198\n\n\n\n\n\nimage-20211019143934759\n\n\n\n\n\nimage-20211019143945772\n\n\n\n\n\n1 Byte = 8 bits\n1KB = \\(2^{10}\\) bytes\nKB for kilobyte vs. Kb for kilobit\n\n\n\n\n\n\nimage-20211019144427248\n\n\n\n\n\nWith Microprocessors so easy, cheap, and available, why design a digital circuit?\n\nMicroprocessor may be to slow\nor too big, power hungry, or costly\n\nCommonly, designers partition a system among a microprocessor and custom digital circuits\n\n\n\nimage-20211019144639391\n\n\n\n\n\nimage-20211019144643649\n\n\nMicroprocessors (themselves digital) can implement many digital systems easily and inexpensively\n\nbut often not good enough-need custom digital circuits\n\n\n\n\nElectronic switches are the basis of binary digital circuits\nVoltage - difference in electric potential between two points\nResistance - tendency of wire to resist current flow\nCurrent - flow of charged particles\nA switch has three parts\n\nSource input, and output - current tries to flow from source input to output\nControl input - voltage that controls whether that current can flow\n\n\n\n\nimage-20211019144910074\n\n\n\n\n\nBasic switch in modern ICs\n\n\n\nimage-20211019144929260\n\n\n\n\n\n\n\n\nimage-20211019145027125\n\n\nLogic gates are better digital circuit building blocks than switches (transistors)\n\n\n\nVariables represent 0 or 1 only\nOperators return 0 or 1 only\nBasic operators\n\nAND\nOR\nNOT\n\n\n\n\nimage-20211019145129771\n\n\n\n\n\n\n\n\nimage-20211019145214878\n\n\n\n\n\n\n\n\nimage-20211019145355561\n\n\n\n\n\n\n\n\nimage-20211019145417114\n\n\n\n\n\n\n\n\nimage-20211019145437083\n\n\n\n\n\n\n\n\nimage-20211019150627132\n\n\n\n\n\nimage-20211019150731693\n\n\n\n\n\n\n\n\nimage-20211019150804826\n\n\nDefine value for F for each possible combination of input values\n\n2-input function: 4 rows\n3-input function: 8 rows\n4-input function: 16 rows\n\n\n\n\n\n\n\nimage-20211019151016513\n\n\n\n\nParity bit - Extra bit added to data, intended to enable detection of error (a bit changed unintentionally)\nEven parity - set parity bit so total number of 1s (data + parity) is even\n\n\n\n\nOnly one truth table representation of a given function\n\nStandard representation-for given function, only one version in standard form exists\n\n\n\n\nTruth table too big for numerous inputs\nUse standard form of equation instead\n\nKnown as canonical form\nBoolean algebra - create sum of minterms\n\nminterms - product term with every function literal appearing exactly once, in true or complemented form\nJust multiply-out equation until sum of product terms\nThen expand each term until all terms are minterms\n\n\n\n\n\n\nCapture behavior - Capture the function\n\nCreate a truth table or equations, whichever is most natural for the given problem, to describe the desired behavior of each output of the combinational logic.\n\nConvert to circuit\n\nCreate equations\nThis substep is only necessary if you captured the function using a truth table instead of equations. Create an equation for each output by ORing all the minterms for that output. Simplify the equations if desired.\nImplement as a get-based circuit\nFor each output, create a circuit corresponding to the output’s equation. (Sharing gates among multiple outputs is OK optionally)\n\n\n\n\n\n\n\n\nimage-20211019153813107\n\n\n\n\n\nimage-20211019153818241\n\n\n\n\n\n\n\n\nimage-20211019153832137\n\n\n\n\n\nimage-20211019153936469\n\n\n\n\n\nNAND - Opposite of AND\nNOR - Opposite of OR\nXOR - Exactly 1 input is 1, for 2-input XOR\nXNOR - Opposite of XOR (‘NOT XOR’)\n\n\n\nimage-20211019154029357\n\n\n\n\n\nAny boolean function can be implemented using just NAND gates.\n\nNOT - 1-input NAND\nAND - NAND followed by NOT\nOR - NAND preceded by NOTs\n\n\n\n\n\n\n\nimage-20211019154401521\n\n\n\n\n\nDecoder - popular combinational logic building block, in addition to logic gates\n\nConvert input binary number to one high output\n2-input decoder: four possible input binary numbers\nn-input decoder: \\(2^{n}\\) outputs\n\n\n\n\nimage-20211019154603116\n\n\n\n\n\nRoutes one of its N data inputs to its one output, based on binary value of select inputs\n\n4 input mux -> needs 2 select inputs to indicate which input to route through\n8 input mux -> 3 select inputs\nN inputs -> \\(\\log_{2}{N}\\) selects\n\n\n\n\nimage-20211019154756284\n\n\n\n\n\nimage-20211019154825139\n\n\n\n\n\nimage-20211019154835456\n\n\n\n\n\nimage-20211019154949871\n\n\n\n\n\nimage-20211019155001367\n\n\n\n\n\n\n\n\nimage-20211019155103235\n\n\n\n\n\n\n\n\nimage-20211019155118740\n\n\n\n\n\nData inputs - flow through component\nControl input - influence component behavior\n\nNormally active high - 1 causes input to carry out its purpose\nActive low - Instead, 0 causes input to carry out its purpose\n\n\n\n\nimage-20211019155317792\n\n\n\n\n\nSchematic capture - computer tool for user to capture logic circuit graphically\nSimulator - computer tool to show what circuit outputs would be for given inputs\n\noutputs commonly displayed as waveform\n\n\n\n\nimage-20211019155433412\n\n\n\n\n\n\n\n\nimage-20211019155449646\n\n\n\n\n\n\n\n\nimage-20211019155513430\n\n\n\n\n\nimage-20211019155525955\n\n\n\n\n\nimage-20211019155533495\n\n\n\n\n\n\n\n\nimage-20211019155551514\n\n\n\n\n\n\n\n\nimage-20211019155604379\n\n\n\n\n\n\n\n\nimage-20211019155641260\n\n\n\n\n\nDelay - the time from inputs changing to new correct stable output\nSize - the number of transistors\n\n\n\nimage-20211019161704366\n\n\n\n\n\nImproves some, but worsens other, criteria of interest\n\n\n\nimage-20211019161711162\n\n\n\n\n\n\n\n\nimage-20211019161722998\n\n\n\n\n\nTwo-level size optimization using algebraic methods\n\n\n\nimage-20211019161833339\n\n\n\nSum-of-products yield two levels\nTransform sum-of-products equation to have fewest literals and terms\n\n\n\n\nEasy to miss possible opportunities to combine terms when doing algebraically\nKarnaugh Maps (K-maps)\n\nGraphical method to help us find opportunities to combine terms\nMinterms differing in one variable are adjacent in the map\nCan clearly see opportunities to combine terms\n\n\n\n\nimage-20211019162046523\n\n\n\n\n\nimage-20211019162056139\n\n\n\n\n\nimage-20211019162103174\n\n\n\n\n\nimage-20211019162110035\n\n\n\n\n\nimage-20211019162117868\n\n\n\n\n\nConvert the function’s equation into sum-of-minterms form\nPlace 1s in the appropriate K-map cells for each minterm\nCover all 1s by drawing the fewest largest circles, with every 1 included at least once; write the corresponding term for each circle\nOR all the resulting terms to create the minimized function\n\n\n\n\nimage-20211019162257738\n\n\n\n\n\nimage-20211019162303804\n\n\n\n\n\nimage-20211019162317921\n\n\n\n\n\n\nWhat if we know that particular input combinations can never occur?\n\nOn K-map\n\nDraw Xs for don’t care combinations\n\nInclude X in circle ONLY if minimizes equation\nDon’t include other Xs\n\n\n\n\n\n\nimage-20211019162415602\n\n\n\n\n\nimage-20211019162442823\n\n\n\n\n\nimage-20211019162512356\n\n\n\n\n\nSpecification - planning - design entry - functional test - synthesis - post-synthesis test - APR, Parasitic Extraction & Timing checks - Manufacture & hardware validation\n\n\n\nimage-20211019162758455\n\n\n\n\n\nimage-20211019162808160\n\n\n\n\n\nHDL = Hardware Description Language\n\nAllows for modeling & simulation (with timing) of digital design\nCan be synthesized into hardware (netlist) by synthesis tools (Synopsys, Ambit, FPGA compilers)\nTwo major standard in industry & academia\n\nVerilog - flexible, loose, more common\nVHDL - strongly typed, more common in defense and automotive\n\nIt looks like a programming language.\nIt is not a programming language.\n\nIt is always critical to recall you are describing hardware.\nThis code’s primary purpose is to generate hardware.\nThe hardware this code describes (a counter) can be simulated on a computer. In this secondary use of the language, it does act more like programming language.\n\n\n\n\n\n\n\n\nimage-20211019163149377\n\n\n\n\n\n\nTakes a description of what a circuit does.\nCreates the hardware to do it.\n\n\n\n\nimage-20211019163252484\n\n\n\n\n\n\nAll hardware created during synthesis\n\n\n\n\nEnables larger design via rich syntax, modularity\n\nmore abstract than schematics, allows larger designs\n\nregister transfer level description\nwide datapath can be abstracted to a single vector\nsynthesis tool does the bulk of the tedious repetitive work vs. schematic capture\n\nWork at transistor/gate level for large designs - cumbersome\n\n\n\nBehavioral or dataflow Verilog can be synthesized to a new process library with little effort.\nVerilog written in ASCII text. The ultimate in portability. Much more portable than the binary files of a GUI schematic capture tool.\n\n\n\nSynthesis options can help optimize (power, area, speed)\nSynthesis options and coding styles can help examine tradeoffs\n\nspeed\npower\narea\n\nBetter validated designs\n\nVerilog itself is used to create the testbench\n\nflexible method that allows self checking tests\nunified environment\n\nsynthesis tools are very good from the boolean correctness point of view\n\nif you have a logic error in your final design there is a 99.999% change that error exists in your behavioral code\nerror caused in synthesis fall in the following categories\n\ntiming\nbad library definitions\nbad coding style\n\n\n\n\n\n\n\nAre highly portable (text)\nAre self-documenting (when commented well)\nDescribe multiple levels of abstraction\nRepresent parallelism\nProvides many descriptive styles\n\nstructural\nregister transfer level (RTL)\nbehavioral\n\nServe as input for synthesis tools\n\n\n\n\n\nHDLs can be compiled to semi-custom and programmable hardware implementations\n\n\n\nimage-20211019164417861\n\n\n\n\n\nimage-20211019164457446\n\n\n\n\n\nLibrary of common gates and structures (cells)\nDecompose hardware in terms of these cells\nArrange the cells on the chip\nConnect them using metal wiring\n\n\n\nProgrammable hardware\nUse small memories as truth tables of functions\nDecompose circuit into these blocks\nConnect using programmable routing\nSRAM bits control functionally\n\n\n\nimage-20211019164903281\n\n\n\n\n\nA netlist is a ASCII text representation of the interconnect of a schematic.\nMany standard exists\n\nSpice netlist\nEDIF (Electronic Data Interchange Format)\nStructural Verilog Netlist\n\n\n\n\nSince HDLs try to abstract hardware design, do we even have to consider the hardware?\n\nGood hardware design requires ability to analyze a problem to find simplifications\nMultiple variables: throughput, area, latency, power\nFinding an optimal hardware implementation is a computationally complex problem. This synthesis tools need guidance on where to start.\n\n\n\n\nimage-20211019165132636\n\n\n\n\n\n\n\n\nimage-20211019165205061"
  },
  {
    "objectID": "posts/2021-11-02-CFA-Level-2-Ethical-and-professional-standards/2021-11-02-CFA-Level-2-Ethical-and-professional-standards.html",
    "href": "posts/2021-11-02-CFA-Level-2-Ethical-and-professional-standards/2021-11-02-CFA-Level-2-Ethical-and-professional-standards.html",
    "title": "CFA Level 2 Ethical and Professional Standards",
    "section": "",
    "text": "image-20211102061104051\n\n\n\n\n\nimage-20211102061109493\n\n\n\n\n\n\n윤리\n\nAct with integrity, competence, diligence, and respect, and in an ethical manner with the public, clients, prospective clients, employers, employees, colleagues in the investment protection, and other participants in the global capital markets.\nPlace the integrity of the investment profession and the interests of clients above their own personal interests.\nUse reasonable care and exercise independent professional judgment when conducting investment analysis, making investment recommendations, taking investment actions, and engaging in other professional activities.\nPractice and encourage others to practice in a professional and ethical manner that will reflect credit on themselves and the profession.\nPromote the integrity and viability of the global capital markets for the ultimate benefit of society.\nMaintain and improve their professional competence and strive to maintain and improve the competence of other investment professionals.\n\n\n\n\n\nProfessionalism\nIntegrity of capital markets\nDuties of clients\nDuties of employers\nInvestment analysis, recommendations, and actions\nConflicts of interest\nResponsibilities as a CFA Institute member or CFA candidate\n\n\n\n\n\n\nProfessionalism\nA. Knowledge of the Law\nB. Independence and Objectivity\nC. Misrepresentation\nD. Misconduct\nIntegrity of capital markets\nA. Material Nonpublic Information\nB. Market Manipulation\nDuties of clients\nA. Loyalty, Prudence, and Care\nB. Fair Dealing\nC. Suitability\nD. Performance Presentation\nE. Preservation of Confidentiality\nDuties of employers\nA. Loyalty\nB. Additional Compensation Agreements\nC. Responsibilities of Supervisors\nInvestment analysis, recommendations, and actions\nA. Diligence and Reasonable Basis\nB. Communications with Clients and Prospective Clients\nC. Record Retention\nConflicts of interest\nA. Disclosure of Conflicts\nB. Priority of Transactions\nC. Referral Fees\nResponsibilities as a CFA Institute member or CFA candidate\nA. Conduct as Participants in CFA Institute Programs\nB. Reference to CFA Institute, the CFA Designation, and the CFA Program\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062044269\n\n\nMembers and Candidates must understand and comply with all applicable laws, rules and regulations (including the CFA Institute Code of Ethics and Standards of Professional Conduct) of any government, regulatory organization, licensing agency, or professional association governing their professional activities. In the event of conflict, Members and Candidates must comply with the more strict law, rule, or regulation. Members and Candidates must not knowingly participate or assist in and must dissociate from any violation of such laws, rules, or regulations.\n\n\nMembers must know the laws and regulations relating to their professional activities in all countries in which they conduct business. Members must comply with applicable laws and regulations relating to their professional activity. Do not violate Code or Standards even if the activity is otherwise legal. Always adhere to the most strict rules and requirements (law or CFA Institute Standards) that apply.\n\n\n\nMembers should dissociate, or separate themselves, from any ongoing client or employee activity that is illegal or unethical, even if it involves leaving an employer(an extreme case). While a member may confront the involved individual first, he must approach his supervisor or compliance department. Inaction with continued association may be construed as knowing participation. (미필적 고의)\n\n\n\n\nMembers should seek advice of counsel or their compliance department when in doubt.\nMembers should document any violations when they disassociate themselves from prohibited activity and encourage their employers to bring an end to such activity.\nThere is no requirement under the Standards to report violations to governmental authorities, but this may be advisable in some circumstances and required by law in others.\nMembers are strongly encouraged to report other members’ violations of the Code and Standards.\n\n\n\n\n\n\n\nimage-20211102030059920\n\n\n\n\n\nimage-20211102030107395\n\n\n\n\n\nimage-20211102030114866\n\n\n\n\n\nimage-20211102030119642\n\n\n\n\n\n\n\n\n\nimage-20211102062058190\n\n\nMembers and Candidates must use reasonable care and judgment to achieve and maintain independence and objectivity in their professional activities. Members and Candidates must not offer, solicit, or accept any gift, benefit, compensation, or consideration that reasonably could be expected to compromise their own or another’s independence and objectivity.\n\n\nDo not let the investment process be influenced by any external sources. Modest gifts are permitted. Allocation of shares is oversubscribed IPOs to personal accounts is NOT permitted. Distinguish between gifts from clients and gifts from entities seeking influence to the detriment of the client. Gifts must be disclosed to the member’s employer in any case, either prior to acceptance if possible, or subsequently.\n\n\n\nDo not be pressured by sell-side firms to issue favorable research on current or prospective investment-banking clients. It is appropriate to have analysts work with investment bankers in “road shows” only when the conflicts are adequately and effectively managed and disclosed. Be sure there are effective “firewalls” between research/investment management and investment banking activities.\n\n\n\nAnalysts should not be pressured to issue favorable research by the companies they follow. Do not confine research to discussions with company management, but rather use a variety of sources, including suppliers, customers, and competitors.\n\n\n\nBuy-side clients may try to pressure sell-side analysts. Portfolio managers may have large positions in a particular security, and a rating downgrade may have an effect on the portfolio performance. As a portfolio manager, there is a responsibility to respect and foster intellectual honesty of sell-side research.\n\n\n\nMembers responsible for selecting outside managers should not accept gifts, entertainment, or travel that might be perceived as impairing their objectivity.\n\n\n\n성과기여도 발라내기\nPerformance analysts may experience pressure from investment managers who have produced poor results or acted outside their mandate. Members and candidates who analyze performance must not let such influences affect their analysis.\n\n\n\npay-to-play\nMembers and candidates must exercise independence and objectivity when they select investment managers. They should not accept gifts or other compensation that could be seen as influencing their hiring decisions, nor should they offer compensation when seeking to be hired as investment managers.\n\n\n\nopinion임\nMembers employed by credit rating firms should make sure that procedures prevent undue influence by the firm issuing the securities. Members who use credit ratings should be aware of this potential conflict of interest and consider whether independent analysis is warranted.\n\n\n\n\nCreate a restricted list and distribute only factual information about companies on the list.\nRestrict special cost arrangements\nLimit gifts\nRestrict employee investments in equity IPOs and private placements. Require pre-approval of IPO purchases.\nFirms should have formal written policies on independence and objectivity of research.\n\n\n\n\n\n\n\nimage-20211102031251757\n\n\n\n\n\nimage-20211102031257712\n\n\n\n\n\nimage-20211102031303564\n\n\n\n\n\nimage-20211102031308977\n\n\n\n\n\n\n\n\n\nimage-20211102062145872\n\n\nMembers and Candidates must not knowingly make any misrepresentations relating to investment analysis, recommendations, actions, or other professional activities.\n\n\nTrust is a foundation in the investment profession. Do not make anFy misrepresentations or give false impressions. This includes oral, electronic, and social media communications. Misrepresentations include guaranteeing investment performance and plagiarism. Plagiarism encompasses using someone else’s work (reports, forecasts, models, ideas, charts, graphs, and spreadsheet models) without giving them credit. Knowingly omitting information that could affect an investment decision or performance evaluation is considered misrepresentation.\nModels and analysis developed by others at a member’s firm are the property of the firm and can be used without attribution. A report written by another analyst employed by the firm cannot be released as another analyst’s work.\n\n\n\nA written list of the firm’s available services and a description of the firm’s qualifications. Employee qualifications should be accurately presented as well. To avoid plagiarism, maintain records of all materials used to generate reports or other firm products and properly cite sources (quotes and summaries) in work products. Information from recognized financial and statistical reporting services need not be cited.\nMembers should encourage their firms to establish procedures for verifying marketing claims of third parties whose information the firm provides to clients.\n\n\n\n\n\n\nimage-20211102061157062\n\n\n\n\n\nimage-20211102061205888\n\n\nPlagiarism is defined as copying or using in substantially the same form materials prepared by others without acknowledging the source of the material or identifying the author and publisher of such material. Members and candidates must not copy (or represent as their own) original ideas or material without permission and must acknowledge and identify the source of ideas or material that is not their own.\nMisrepresentation through plagiarism in investment management can take various form. The simplest and most flagrant example is to take a research report or study done by another firm or person, change the names, and release the material as one’s own original analysis. This action is a clear violation of Standard I(C). Other practices include 1) using excerpts from articles or reports prepared by other either verbatim or with only slight changes in wording without acknowledgement, 2) citing specific quotations as attributable to “leading analysts” and “investment experts” without naming the specific references, 3) presenting statistical estimates of forecasts prepared by others and identifying the sources but without including the qualifying statements or caveats that may have been used, 4) using charts and graphs without stating their sources, and 5) copying proprietary computerized spreadsheets or algorithms without seeking the cooperation or authorization of their creators.\n\n\n\n\n\n\nimage-20211102031922926\n\n\n\n\n\nimage-20211102031930706\n\n\n\n\n\nimage-20211102031939706\n\n\n\n\n\n\n\n\n\nimage-20211102062229666\n\n\nMembers and Candidates must not engage in any professional conduct involving dishonesty, fraud, or deceit or commit any act that reflects adversely on their professional reputation, integrity, or competence.\n\n\nDo not abuse CFA Institute’s Professional Conduct Program by seeking enforcement of this Standard to settle personal, political, or other disputes that are not related to professional ethics.\n\n\n\n\n\n\nimage-20211102032135978\n\n\n\n\n\nimage-20211102032141526\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062250120\n\n\nMembers and Candidates who posses material nonpublic information that could affect the value of an investment must not act or cause others to act on the information.\n\n\nInformation is “material” if its disclosure would impact the price of a security or if reasonable investors would want the information before making an investment decision. Ambiguous information, as far as its likely effect on price, may not be considered material. Information is “nonpublic” until it has been made available to the marketplace. an analyst conference call is not public disclosure. Selectively disclosing information by corporations creates the potential for insider-trading violations.\nSome members and candidates may be involved in transactions during which they receive material nonpublic information provided by firms (e.g., investment banking transactions). Members and candidates may use the provided nonpublic information for its intended purpose, but must not use the information for any other purpose unless it becomes public information.\n\n\n\nThere is no violation when a perceptive analyst reaches an investment conclusion about a corporate action or event through an analysis of public information together with items of nonmaterial nonpublic information.\n\n\n\nnon-public 취급\nWhen gathering information from internet or social media sources, members and candidates need to be aware that not all of it is considered public information. Members and candidates should confirm that any material information they receive from these sources is also available from public sources, such as company press releases or regulatory filings.\n\n\n\nMembers and candidates may seek insight from individuals who have specialized expertise in an industry. However, they may not act or cause others to act on any material nonpublic information obtained from these experts until that information has been publicly disseminated.\n\n\n\nMake reasonable efforts to achieve public dissemination of the information.\n\nSubstantial control of relevant interdepartmental communications, through a clearance area such as the compliance or legal departement.\nMonitor and restrict proprietary trading while a firm is in possession of material nonpublic information. - risk-arbitrage trading is not allowed.\nProhibition of all proprietary trading while a firm is in possession of material nonpublic information may be inappropriate because it may send a signal to the market. In these cases, firms should take the contra side of only unsolicited customer trades. - market making is allowed\n\n\n\n\n\n\n\nimage-20211102033151157\n\n\n\n\n\nimage-20211102033157979\n\n\n\n\n\n\nMembers and Candidates must not engage in practices that distort prices or artificially inflate trading volume with the intent to mislead market participants.\nprice or volume + intent\n\n\n\n\n\nimage-20211102033310016\n\n\n\n\n\nimage-20211102033317042\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062332887\n\n\nMembers and Candidates have a duty of loyalty to their clients and must act with reasonable care and exercise prudent judgment. Members and Candidates must act for the benefit of their clients and place their clients’ interests before their employer’s or their own interests.\n\n\nClient interests always come first. Although this Standard does not impose a fiduciary duty on members or candidates where one did not already exist, it does require members and candidates to act in their clients’ best interest and recommend products that are suitable given their clients’ investment objectives and risk tolerances.\n\nmanage polls of client assets in accordance with the terms of the governing documents, such as trust documents or investment management agreements.\n\nIII(C) 고려, 문건대로 운영, beneficiary 고려 -> 그래도 원하면 IPS 변경\n\nMake investment decisions in the context of the total portfolio.\nVote proxies in an informed and responsible manner. Due to cost benefit considerations, it may not be necessary to vote all proxies.\nClient brokerage, or “soft dollar” or “soft commissions” must be used to benefit the client.\nThe “client” may be the investing public as a whole rather than a specific entity or person.\n\n\n\n\nSubmit to clients, at least quarterly, itemized statements (상세설명서) showing all securities in custody and all debits, credits, and transactions.\n\n\n\n\n\n\nimage-20211102062417228\n\n\n\n\n\n\n\n\nimage-20211102034240248\n\n\n\n\n\nimage-20211102034248914\n\n\n\n\n\nimage-20211102034255175\n\n\n\n\n\n\n\n\n\nimage-20211102062436181\n\n\nMembers and Candidates must deal fairly and objectively with all clients when providing investment analysis, making investment recommendations, taking investment action, or engaging in other professional activities.\n\n\nDo not discriminate against any clients when disseminating recommendations or taking investment action. Fairly does not mean equally. In the normal course of business, there will be differences in the time emails, faxes, etc., are received by different clients. Different service levels are okay, but they must not negatively affect or disadvantage any clients. Disclose the different service levels to all clients and prospects, and make premium levels of service available to all who wish to pay for them.\n\n\n\nGive all clients a fair opportunity to act upon every recommendation. Client who are unaware of a change in a recommendation should be advised before the order is accepted.\n\n\n\nTreat clients fairly in light of their investment objectives and circumstances. Treat both individual and institutional clients in a fair and impartial manner. Members and candidates should not take advantage of their position in the industry to disadvantage clients.\n\n\n\n\nLimit the number of people who are aware that a change in recommendation will be made.\nShorten the time frame between decision and dissemination.\nPublish personnel guidelines for pre-dissemination-have in place guidelines prohibiting personnel who have prior knowledge of a recommendation from discussing it or taking action on the pending recommendation.\nSimultaneous dissemination of new or changed recommendations to all clients who have expressed an interest or for whom an investment is suitable.\nDevelop written trade allocation procedures-ensure fairness to clients, timely and efficient order execution, and accuracy of client positions.\nDisclose trade allocation procedures.\nDisclose available levels of service.\n\n\n\n\n\n\n\nimage-20211102035025605\n\n\n\n\n\nimage-20211102035030062\n\n\n\n\n\n\n\n\n\nimage-20211102062504871\n\n\n\nWhen Members and Candidates are in an advisory relationship with a client, they must:\n\nMake a reasonable inquiry into a client’s or prospective clients’ investment experience, risk and return objectives, and financial constraints prior to making nay investment recommendation or taking investment action and must reassess and update this information regularly.\nDetermine that an investment is suitable to the client’s financial situation and consistent with the client’s written objectives, mandates, and constraints before making an investment recommendation or taking investment action.\nJudge the suitability of investments in the context of the client’s total portfolio.\n\nWhen Members and Candidates are responsible for managing a portfolio to a specific mandate, strategy, or style, they must make only investment recommendations or take only investment actions that are consistent with the stated objectives and constraints of the portfolio.\n\n\n\nIn advisory relationships, be sure to gather client information at the beginning of the relationship, in the form of an investment policy statement (IPS). Consider clients’ needs and circumstances and thus their risk tolerance. Consider whether or not the use of leverage is suitable for the client.\nIf a member is responsible for managing a fund to an index or other stated mandate, be sure investments are consistent with the stated mandates.\n\n\n\nMembers should put the needs and circumstances of each client and the client’s investment objectives into a written IPS for each client.\n\n\n\n\n\n\nimage-20211102061918428\n\n\n\n\n\nimage-20211102061812169\n\n\n\n\n\nimage-20211102035627037\n\n\n\n\n\nimage-20211102035632679\n\n\n\n\n\nimage-20211102035637707\n\n\n\n\n\n\nWhen communicating investment performance information, Members or Candidates must make reasonable efforts to ensure that it is fair, accurate, and complete.\n\n\nMembers must avoid misstating performance or misleading clients/prospects about investment performance of themselves or their firms, should not misrepresent past performance or reasonably expected performance, and should not state or imply the ability to achieve a rate of return similar to that achieved in the past. For brief presentations, members must make detailed information available on requests and indicate that the presentation has offered limited information.\n\n\n\nEncourage firms to adhere to Global Investment Performance Standards.\n\nConsidering the sophistication of the audience to whom a performance presentation is addressed.\nPresenting performance of weighted composite of similar portfolios rather than a single account.\nIncluding terminated accounts as part of historical performance and clearly stating when they were terminated.\nIncluding all appropriate disclosures to fully explained results (e.g., model results included, gross or net of fees, etc.).\nMaintaining data and records used calculate the performance being presented.\n\n\n\n\n\n\n\nimage-20211102040200685\n\n\n\n\n\nimage-20211102040206104\n\n\n\n\n\nimage-20211102040210753\n\n\n\n\n\n\n\n\n\nimage-20211102062558495\n\n\nMembers and Candidates must keep information about current, former, and prospective clients confidential unless:\n\nThe information concerns illegal activities on the part of the client or prospective client,\nDisclosure is required by law, or\nThe client or prospective client permits disclosure of the information.\n\n\n\nIf illegal activities by a client are involved, members may have an obligation to report the activities to authorities. The confidentiality Standard extends to former clients as well.\nThe requirements of this Standard are not intended to prevent Members and Candidates from cooperating with a CFA Institute Professional Conduct Program (PCP) investigation.\n\n\n\nMembers should avoid disclosing information received from a client except to authorized co-workers who are also working for the client.\n\n\n\n\n\n\nimage-20211102040547353\n\n\n\n\n\nimage-20211102040551788\n\n\n\n\n\nimage-20211102040557386\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062607886\n\n\nIn matters related to their employment, Members and Candidates must act for the benefit of their employer and not deprive their employer of the advantage of their skills and abilities, divulge confidential information, or otherwise cause harm to their employer.\n\n\nmembers must not engage in any activities which would injure the firm, deprive it of profit, or deprive it of the advantage of employees’ skills and abilities. Members should always place client interests above interests of their employer but consider the effects of their actions on firm integrity and sustainability.\n\n\n\nIndependent practice for compensation is allowed if a notification is provided to the employer fully describing all aspects of the services, including compensation, duration, and the nature of the activities and if the employer consents to all terms of the proposed independent practice before it begins.\n\n\n\nMembers must continue to act in their employer’s best interests until resignation is effective. Activities which may constitute a violation include:\n\nMisappropriation of trade secrets\nMisuse of confidential information\nSoliciting employer’s clients prior to leaving\nSelf-dealing\nMisappropriation of client lists\n\nEmployer records on any medium (e.g., home computer, PDA, cell phone) are the property of the firm.\nOnce an employee has left a firm, simple knowledge of names and existence of former clients is generally not confidential. There is also no prohibition on the use of experience or knowledge gained while with a former employer. If an agreement exists among employers that permits brokers to take certain client information when leaving a firm, a member or candidate may act within the terms of the agreement without violating the Standard.\n\n\n\nMembers and candidates must adhere to their employers’ policies concerning social media. When planning to leave an employer, members and candidates must ensure that their social media use complies with their employers’ policies for notifying clients about employer separations. A best practice is to use separate social media accounts for personal and professional communications.\n\n\n\nThere may be isolated cases where a duty to one’s employer may be violated in order to protect clients or the integrity of the market, and not for personal gain.\n\n\n\nIf Members and Candidates are independent contractors, they still have a duty to abide by the terms of the agreement.\n\n\n\n\n\n\nimage-20211102052746583\n\n\n\n\n\nimage-20211102052752483\n\n\n\n\n\nimage-20211102052800860\n\n\n\n\n\nimage-20211102052806321\n\n\n\n\n\n\n\n\n\nimage-20211102062655608\n\n\nMembers and Candidates must not accept gifts, benefits, compensation, or consideration that competes with or might reasonably be expected to create a conflict of interest with their employer’s interest unless they obtain written consent from all parties involved.\n\n\nCompensation includes direct and indirect compensation from a client and other benefits received from third parties.\n\n\n\n\n\n\nimage-20211102052953089\n\n\n\n\n\nimage-20211102052959922\n\n\nIf a client gives us money for doing a good job (one time), we need to disclose it (though not necessarily in writing).\nIf we have an agreement with a client that we will receive money in the future for outperformance, we need to disclose that in writing.\nFor any side job that potentially competes with our employer, written permission is required.\nFor a side job (e.g., bartender) that’s unrelated to our primary job, no disclosure is required.\n\n\n\n\n\n\n\nimage-20211102062725039\n\n\nMembers and Candidates must make reasonable efforts to ensure that anyone subject to their supervision or authority complies with applicable laws, rules, regulations, and the Code and Standards.\n\n\nDuty - Delegation 가능 / Responsibility - Delegation 불가\nMembers must make reasonable efforts to prevent employees from violating laws, rules, regulations, or the Code and Standards, as well as make reasonable efforts to detect violations.\n\n\n\nUnderstand that an adequate compliance system must meet industry standards, regulatory requirements, and the requirements of the Code and Standards.\nA member or candidate faced with no compliance procedures or with procedures he believes are inadequate must decline supervisory responsibility in writing until adequate procedures are adopted by the firm.\n\n\n\n\nDesignate a compliance officer with authority clearly defined.\n\nIf there a violation, respond promptly and conduct a thorough investigation while increasing supervision or placing limitations on the wrongdoer’s activities.\n\n\n\n\n\n\nimage-20211102053544336\n\n\n\n\n\nimage-20211102053550948\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062736908\n\n\n\n\n\nGlobal and national economic conditions\nA firm’s financial results, operating history, and business cycle stage\nFees and historical results for a mutual fund\nLimitations of any quantitative models used\nA determination of whether peer group comparisons for valuation are appropriate\n\n\n\n\nMembers must be able to explain the basic nature of the quantitative research and how it is used to make investment decisions. Members should consider scenarios outside those typically used to assess downside risk and the time horizon of the data used for model evaluation to ensure that both positive and negative cycle results have been considered.\n\n\n\nThe Standard requires greater diligence of members and candidates who create quantitative techniques than of those who use techniques developed by others. Members and candidates must understand the technical details of the products they offer to clients. A member or candidate who has created a quantitative strategy must test it thoroughly, including extreme scenarios with inputs that fall outside the range of historical data, before offering it to clients.\n\n\n\nTo review any external advisers\n\n\n\nEven if a member does not agree with the independent and objective view of the group, he does not necessarily have to decline to be identified with the report, as long as there is a reasonable and adequate basis.\n\n\n\n\n\n\nimage-20211102054206726\n\n\n\n\n\nimage-20211102054211906\n\n\n\n\n\nimage-20211102054217064\n\n\n\n\n\n\nMembers and Candidates must:\n\nDisclose to clients and prospective clients the basic format and general principles of the investment processes they use to analyze investments, select securities, and construct portfolios and must promptly disclose any changes that might materially affect those processes.\nDisclose to clients and prospective clients significant limitations and risks associated with the investment process.\nUse reasonable judgment in identifying which factors are important to their investment analyses, recommendations, or actions and include those factors in communications with clients and prospective clients. - factors up to you, but disclose\nDistinguish between fact and opinion in the presentation of investment analysis and recommendations.\n\n\n\nIn preparing recommendations for structured securities, allocation strategies, or any other nontraditional investment, members should communicate those risk factors specific to such investments. In all cases, members should communicate the potential gains and losses on the investment clearly in terms of total return. Members are required to communicate significant changes in the risk characteristics of an investment process, including any risks and limitations that have been newly identified.\nWhen using projections from quantitative models and analysis, members may violate the Standard by not explaining the limitations of the model and the assumptions it uses, which provides a context for judging the uncertainty regarding the estimated investment result.\nMembers and candidates must inform clients about limitations inherent to an investment. Liquidity refers to the ability to exit an investment readily without experiencing a significant extra cost for doing so. Capacity refers to an investment vehicle’s ability to absorb additional investment without reducing the returns it is able to achieve.\n\n\n\nSelection of relevant factors in a report can be a judgment call, so be sure to maintain records indicating the nature of the research, and be able to supply additional information if it is requested by the client or other users of the report.\n\n\n\n\n\n\nimage-20211102054948130\n\n\n\n\n\nimage-20211102054954359\n\n\n\n\n\nimage-20211102055000821\n\n\n\n\n\nimage-20211102055005700\n\n\n\n\n\n\n\n\n\nimage-20211102062012526\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102055401329\n\n\nMembers and Candidates must make full and fair disclosure of all matters that could reasonably be expected to impair their independence and objectivity or interfere with respective duties to their clients, prospective clients, and employers. Members and candidates must ensure that such disclosures are prominent, are delivered in plain language, and communicate the relevant information effectively.\n\n\nMembers must fully disclose to clients, prospects, and their employers all actual and potential conflicts of interest in order to protect investors and employers. These disclosures must be clearly stated. - All, actual, potential clients\n\n\n\nThe requirement that all potential areas of conflict be disclosed allows clients and prospects to judge motives and potential biases for themselves. Disclosure of broker/dealer market-making activities would be included here. Board service is another area of potential conflict.\nThe most common conflict which requires disclosure is actual ownership of stock in companies that the member recommends or that clients hold.\nAnother common source of conflicts of interest is a member’s compensation/bonus structure, which can potentially create incentives to take actions that produce immediate gains for the members with little or no concern for longer-term returns for the client. Such conflicts must be disclosed when the member is acting in an advisory capacity and must be updated in the case of significant change in compensation structure.\n\n\n\n\n\n\nimage-20211102055636988\n\n\n\n\n\nimage-20211102055643014\n\n\n\n\n\nimage-20211102055649775\n\n\n\n\n\nimage-20211102055654371\n\n\n\n\n\n\nInvestment transactions for clients and employers must have priority over investment transactions in which a Members or Candidates is the beneficial owner.\n\n\nClient transactions take priority over personal transactions and over transactions made on behalf of the member’s firm. Personal transactions may be undertaken only after clients and the member’s employer have had an adequate opportunity to act on a recommendation. Note that family member accounts that are clients accounts should be treated just like any client account; they should not be disadvantaged.\n\n\n\nThe following areas should be included:\n\nLimited participation in equity IPOs. Members can avoid these conflicts by not participating in IPOs.\nRestrictions on private placements.\nEstablish blackout/restricted periods\nReporting requirements. Supervisors should establish reporting procedures, including duplicating trade confirmations, disclosure of personal holdings/beneficial ownership positions, and preclearance procedures.\n\n\n\n\n\n\n\nimage-20211102060055431\n\n\n\n\n\nimage-20211102060059966\n\n\n\n\n\n\nImpartiality, full cost of service -> disclosed by written\n\n\n\nimage-20211102060135980\n\n\n\n\nMembers must inform employers, clients, and prospects of any benefit received for referrals of customers and clients, allowing them to evaluate the full cost of the service as well as any potential partility.\n\n\n\nAt least quarterly, the nature and value of referral compensation received\n\n\n\n\n\n\nimage-20211102060257524\n\n\n\n\n\nimage-20211102060302176\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211102062029961\n\n\nMembers and Candidates must not engage in any conduct that compromises the reputation or integrity of CFA Institute or the CFA designation or the integrity, validity, or security of CFA Institute programs.\nIt is not intended to prevent anyone from expressing any opinions or beliefs concerning CFA Institute or the CFA program.\nThis Standard applies to conduct which includes:\n\nCheating on the CFA exam or any exam administered by CFA Institute (e.g., CIPM).\nRevealing anything about either broad or specific topics tested, content of exam questions, or formulas required or not required on the exam.\nNot following rules and policies of any CFA Institute program.\nGiving confidential information on the CFA program to candidates or the public.\nImproperly using the designation to further personal and professional goals.\nMisrepresenting information on the Professional Conduct Statement (PCS) or the CFA Institute Professional Development Program.\n\nMembers and candidates are not precluded from expressing their opinions regarding the exam program or CFA Institute but must not reveal confidential information about the CFA program.\n\n\n\n\n\nimage-20211102060813503\n\n\n\n\n\nimage-20211102060819550\n\n\n\n\n\n\n\n\n\nimage-20211102062758264\n\n\nMembers and Candidates must not misrepresent or exaggerate the meaning or implications of membership in CFA Institute, holding the CFA designation, or candidacy in the CFA Program.\n\n\nDo not:\n\nOver-promise individual competence\nOver-promise investment results in the future\n\n\n\n\n\nSign PCS annually\nPay CFA Institute membership dues annually\n\n\n\n\n\n\n\nimage-20211102061041020\n\n\n\n\n\n\n\n\nimage-20211102061846251\n\n\n\n\n\nimage-20211102061851269"
  },
  {
    "objectID": "posts/2021-10-02-kaggle-Data-Cleaning/2021-10-02-kaggle-Data-Cleaning.html",
    "href": "posts/2021-10-02-kaggle-Data-Cleaning/2021-10-02-kaggle-Data-Cleaning.html",
    "title": "Kaggle - Data Clearning",
    "section": "",
    "text": "# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]\n# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)\n\n\n\nThis is the point at which we get into the part of data science that I like to call “data intution”, by which I mean “really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis”.\nFor dealing with missing values, you’ll need to use your intution to figure out why the value is missing. One of the most important questions you can ask yourself to help figure this out is this:\n\nIs this value missing because it wasn’t recorded or because it doesn’t exist?\n\nIf a value is missing becuase it doesn’t exist (like the height of the oldest child of someone who doesn’t have any children) then it doesn’t make sense to try and guess what it might be. These values you probably do want to keep as NaN. On the other hand, if a value is missing because it wasn’t recorded, then you can try to guess what it might have been based on the other values in that column and row. This is called imputation.\nIf you’re doing very careful data analysis, this is the point at which you’d look at each column individually to figure out the best strategy for filling those missing values.\n\n\n\nIf you’re in a hurry or don’t have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don’t generally recommend this approch for important projects! It’s usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)\nIf you’re sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this.\n\n\n\nWe can use the Panda’s fillna() function to fill in missing values in a dataframe for us. One option we have is to specify what we want the NaN values to be replaced with.\nI could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)\n# replace all NA's the value that comes directly after it in the same column, \n# then replace all the remaining na's with 0\nsubset_nfl_data.fillna(method='bfill', axis=0).fillna(0)\n\n\n\n# TODO: Your code here!\nsf_permits.head()\n# TODO: Your code here!\npercent_missing = sf_permits.isnull().sum().sum() / np.product(sf_permits.shape) * 100\n\n# Check your answer\nq2.check()\n\n\n\nimage-20211002030542339\n\n\n\n\n\nimage-20211002030629934\n\n\n# TODO: Your code here\nsf_permits_with_na_dropped = sf_permits.dropna(axis=1)\n\ndropped_columns = len(sf_permits.columns) - len(sf_permits_with_na_dropped.columns)\n\n# Check your answer\nq5.check()\n# TODO: Your code here\nsf_permits_with_na_imputed = sf_permits.fillna(method='bfill', axis=0).fillna(0)\n\n# Check your answer\nq6.check()\n\n\n\n\n\n\n\nin scaling, you’re changing the range of your data, while\nin normalization, you’re changing the shape of the distribution of your data.\n\n\n\n\nThis means that you’re transforming your data so that it fits within a specific scale, like 0-100 or 0-1.\nBy scaling your variables, you can help compare different variables on equal footing.\nNotice that the shape of the data doesn’t change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n\n\nNormalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\nIn general, you’ll normalize your data if you’re going to be using a machine learning or statistics technique that assumes your data is normally distributed\nNotice that the shape of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence “bell curve”).\n\n\n\n# TODO: Your code here\nscaled_goal_data = minmax_scaling(original_goal_data, ['goal'], 0, 1)\n\n# Check your answer\nq1.check()\n\n\n\n\n\n\n\nimage-20211002032243408\n\n\nNotice that at the bottom of the output of head(), you can see that it says that the data type of this column is “object”.\n\nPandas uses the “object” dtype for storing various types of data types, but most often when you see a column with the dtype “object” it will have strings in it.\n\nIf you check the pandas dtype documentation here, you’ll notice that there’s also a specific datetime64 dtypes. Because the dtype of our column is object rather than datetime64, we can tell that Python doesn’t know that this column contains dates.\n\n\n\nimage-20211002032324133\n\n\nYou may have to check the numpy documentation to match the letter code to the dtype of the object. “O” is the code for “object”, so we can see that these two methods give us the same information.\n\n\nWe can pandas what the format of our dates are with a guide called as “strftime directive”, which you can find more information on at this link. The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date, but the most common are %d for day, %m for month, %y for a two-digit year and %Y for a four digit year.\nSome examples:\n\n1/17/07 has the format “%m/%d/%y”\n17-1-2007 has the format “%d-%m-%Y”\n\n\n\n\nimage-20211002032411835\n\n\n\n\n\nimage-20211002032419381\n\n\n\nWhat if I run into an error with multiple date formats? While we’re specifying the date format here, sometimes you’ll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:\n\nlandslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)\n\nWhy don’t you always use infer_datetime_format = True? There are two big reasons not to always have pandas guess the time format. The first is that pandas won’t always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it’s much slower than specifying the exact format of the dates.\n\n\n\n\n\n\n\nimage-20211002032513853\n\n\n\n\n\nOne of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn’t hurt to double-check that the days of the month we’ve extracted make sense.\nTo do this, let’s plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there’s no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.) Let’s see if that’s the case:\n\n\n\nimage-20211002032619957\n\n\n\n\n\n\n\n\nCharacter encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like “hi”). There are many different encodings, and if you tried to read in text with a different encoding than the one it was originally written in, you ended up with scrambled text called “mojibake” (said like mo-gee-bah-kay). Here’s an example of mojibake:\næ–‡å—åŒ–ã??\nYou might also end up with a “unknown” characters. There are what gets printed when there’s no mapping between a particular byte and a character in the encoding you’re using to read your byte string in and they look like this:\n����������\nCharacter encoding mismatches are less common today than they used to be, but it’s definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n\nUTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It’s when things aren’t in UTF-8 that you run into trouble.\n\n\n\n\nimage-20211002033900935\n\n\nIf you look at a bytes object, you’ll see that it has a b in front of it, and then maybe some text after. That’s because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn’t really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like “” when it’s printed as if it were an ASCII string.\n\n\n\nimage-20211002033947123\n\n\nHowever, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we’re trying to use doesn’t know what to do with the bytes we’re trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\nLike I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we’ll create problems.\n\n\n\nimage-20211002034050924\n\n\n\n\n\nNotice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn’t actually UTF-8. We don’t know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It’s not 100% guaranteed to be right, but it’s usually faster than just trying to guess.\n\n\n\nimage-20211002034205908\n\n\n\n\n\nimage-20211002034214164\n\n\nWhat if the encoding chardet guesses isn’t right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n\n\n\nThe good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n\n\n\nnew_entry = sample_entry.decode('big5-tw').encode('utf-8')\n\n# Check your answer\nq1.check()\n# TODO: Load in the DataFrame correctly.\nwith open(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(30000))\nprint(result)\n\npolice_killings = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", encoding='Windows-1252')\n\n# Check your answer\nq2.check()\n# TODO: Save the police killings dataset to CSV\npolice_killings.to_csv(\"/kaggle/working/my_file.csv\")\n\n# Check your answer\nq3.check()\n\n\n\n\n\n\n\n\n\nimage-20211002035130731\n\n\n\n\n\nWe’re going to use the fuzzywuzzy package to help identify which strings are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn’t scale well.\n\nFuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered “closer” to another one the fewer characters you’d need to change if you were transforming one string into another. So “apple” and “snapple” are two changes away from each other (add “s” and “n”) while “in” and “on” and one change away (rplace “i” with “o”). You won’t always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n\nFuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings.\n\n\n\nimage-20211002035245766\n\n\n\n\n\nimage-20211002035303096"
  },
  {
    "objectID": "posts/2021-12-07-algorithms-week-14/2021-12-07-algorithms-week-14.html",
    "href": "posts/2021-12-07-algorithms-week-14/2021-12-07-algorithms-week-14.html",
    "title": "Algorithms Week 14",
    "section": "",
    "text": "image-20211207141940992\n\n\n\n\n\n\n\n\nimage-20211207141951071\n\n\n\n\n\nimage-20211207141957715\n\n\n\n\n\n\n\n\nimage-20211207142007729\n\n\n\n\n\n\n\n\nimage-20211207142026959\n\n\noptimization problems를 decision problems로 바꿀 수 있다.\n\n\n\n\n\n\nimage-20211207142758253\n\n\n\n\n\n\n\n\nimage-20211207142809652\n\n\n\n\n\n\n\n\nimage-20211207142827277\n\n\n\n\n\nimage-20211207142837761\n\n\n\n\n\n\n\n\nimage-20211207190519099\n\n\n\n\n\n\n\n\nimage-20211207190533317\n\n\n\n\n\n\n\n\nimage-20211207190545005\n\n\n\n\n\nimage-20211207190550406\n\n\n\n\n\n\n\n\nimage-20211207190602257\n\n\n\n\n\nimage-20211207190607504\n\n\n\n\n\nimage-20211207190613192\n\n\n\n\n\n\n\n\nimage-20211207190623321\n\n\n\n\n\n\n\n\nimage-20211207190631332\n\n\n\n\n\n\n\n\nimage-20211207191524388\n\n\n\n\n\n\n\n\nimage-20211207191537537\n\n\n\n\n\nimage-20211207191542505\n\n\n\n\n\n\n\n\nimage-20211207191926825\n\n\n\n\n\n\n\n\nimage-20211207191934940\n\n\n\n\n\n\n\n\nimage-20211207191947479\n\n\n\n\n\n\n\n\nimage-20211207192359045\n\n\n\n\n\n\n\n\nimage-20211207192409180\n\n\n\n\n\n\n\n\nimage-20211207192418865\n\n\n\n\n\n\n\n\nimage-20211207205120564\n\n\n\n\n\n\n\n\nimage-20211207205230194\n\n\n\n\n\n\n\n\nimage-20211207205421881\n\n\n\n\n\n\n\n\nimage-20211207205432994\n\n\n\n\n\n\n\n\nimage-20211207205842011\n\n\n\n\n\n\n\n\nimage-20211207210101522\n\n\n\n\n\n\n\n\nimage-20211207210158316\n\n\n\n\n\n\n\n\nimage-20211207210230497\n\n\n\n\n\n\n\n\nimage-20211207210414996\n\n\n\n\n\n\n\n\nimage-20211207210428472\n\n\n\n\n\n\n\n\nimage-20211207210541959\n\n\n\n\n\n\n\n\nimage-20211207210555900\n\n\n\n\n\nimage-20211207210728587\n\n\n\n\n\n\n\n\nimage-20211207210740997\n\n\n\n\n\n\n\n\nimage-20211207210752591\n\n\n\n\n\n\n\n\nimage-20211207211036350\n\n\n\n\n\n\n\n\nimage-20211207211046609\n\n\n\n\n\n\n\n\nimage-20211207211127902\n\n\n\n\n\n\n\n\nimage-20211207211855855\n\n\n\n\n\n\n\n\nimage-20211207211937111\n\n\n\n\n\n\n\n\nimage-20211207212038514\n\n\n\n\n\nimage-20211207212044736\n\n\n\n\n\nimage-20211207212050068\n\n\n\n\n\n\n\n\nimage-20211207212059368\n\n\n\n\n\n\n\n\nimage-20211207212756726"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2021-12-07-algorithms-week-13/2021-12-07-algorithms-week-13.html",
    "href": "posts/2021-12-07-algorithms-week-13/2021-12-07-algorithms-week-13.html",
    "title": "Algorithms Week 13",
    "section": "",
    "text": "image-20211207124410764\n\n\n\n\n\n\n\n\nimage-20211207125150921\n\n\n\n\n\n\n\n\nimage-20211207125159135\n\n\n\n\n\nimage-20211207125207413\n\n\n\n\n\n\n\n\n\n\nimage-20211207125230959\n\n\n\n\n\n\n\n\n\nimage-20211207125243425\n\n\n\n\n\n\n\n\nimage-20211207125255106\n\n\n\n\n\nimage-20211207125643256\n\n\n\n\n\nimage-20211207125650404\n\n\n\n\n\nimage-20211207125659201\n\n\n\n\n\nimage-20211207125707809\n\n\n\n\n\nimage-20211207125714728\n\n\n\n\n\nimage-20211207125723888\n\n\n\n\n\nimage-20211207125731144\n\n\n\n\n\n\n\n\nimage-20211207125743376\n\n\n\n\n\n\n\n\nimage-20211207125758193\n\n\n\n\n\n\n\n\nimage-20211207130139784\n\n\n\n\n\n\n\n\nimage-20211207130150462\n\n\n\n\n\n\n\n\nimage-20211207130204867\n\n\n\n\n\n\n\n\nimage-20211207130216147\n\n\n\n\n\n\n\n\nimage-20211207130228086\n\n\n\n\n\n\n\n\nimage-20211207130521888\n\n\n\n\n\nimage-20211207130532927\n\n\n\n\n\nimage-20211207130541551\n\n\n\n\n\nimage-20211207130550915\n\n\n\n\n\nimage-20211207130601278\n\n\n\n\n\nimage-20211207130610059\n\n\n\n\n\nimage-20211207130631355\n\n\n\n\n\n\n\n\nimage-20211207130650827\n\n\n\n\n\n\n\n\nimage-20211207131200965\n\n\n\n\n\nimage-20211207131207103\n\n\n\n\n\nimage-20211207131214932\n\n\n\n\n\nimage-20211207131223065\n\n\n\n\n\n\n\n\nimage-20211207131233461\n\n\n\n\n\n\n\n\nimage-20211207131250435\n\n\n\n\n\nimage-20211207131258723\n\n\n\n\n\n\n\n\nimage-20211207131316733\n\n\n\n\n\n\n\n\nimage-20211207131323243\n\n\n\n\n\nimage-20211207131328719\n\n\n\n\n\nimage-20211207131805446\n\n\n\n\n\n\n\n\nimage-20211207131828773\n\n\n\n\n\n\n\n\nimage-20211207131851032\n\n\n\n\n\nimage-20211207131907776\n\n\n\n\n\nimage-20211207131916155\n\n\n\n\n\nimage-20211207131924584\n\n\n\n\n\n\n\n\nimage-20211207131940349\n\n\n\n\n\n\n\n\nimage-20211207131953121\n\n\n\n\n\n\n\n\nimage-20211207132002609\n\n\n\n\n\n\n\n\nimage-20211207132012989\n\n\n\n\n\n\n\n\nimage-20211207132026488\n\n\n\n\n\ngreedily build subtrees, starting with the infrequent letters\n\n\n\nimage-20211207132045635\n\n\n\n\n\nimage-20211207132054696\n\n\n\n\n\nimage-20211207132100843\n\n\n\n\n\nimage-20211207132107452\n\n\n\n\n\nimage-20211207132116352\n\n\n\n\n\nimage-20211207132125129\n\n\n\n\n\n\n\n\nimage-20211207132138686\n\n\n\n\n\n\n\n\nimage-20211207132717578\n\n\n\n\n\n\n\n\nimage-20211207132729107\n\n\n\n\n\n\n\n\nimage-20211207132738317\n\n\n\n\n\n\n\n\nimage-20211207132748707\n\n\n\n\n\n\n\n\nimage-20211207132800492\n\n\n\n\n\nimage-20211207133312410\n\n\n\n\n\nimage-20211207133320125\n\n\n\n\n\nimage-20211207133327857\n\n\n\n\n\nimage-20211207133335355\n\n\n\n\n\n\n\n\nimage-20211207133352369\n\n\n\n\n\n\n\n\nimage-20211207133405724\n\n\n\n\n\nimage-20211207133413229\n\n\n\n\n\nimage-20211207133420158\n\n\n\n\n\nimage-20211207133427278\n\n\n\n\n\nimage-20211207133434180\n\n\n\n\n\nimage-20211207133441720\n\n\n\n\n\nimage-20211207133449792\n\n\n\n\n\nimage-20211207133455751\n\n\n\n\n\n\n\n\nimage-20211207133518905\n\n\n\n\n\n\n\n\nimage-20211207134032338\n\n\n\n\n\n\n\n\nimage-20211207134047136\n\n\n\n\n\nimage-20211207134148373\n\n\n\n\n\nimage-20211207134136742\n\n\n\n\n\n\n\n\nimage-20211207134430747\n\n\n\n\n\nimage-20211207134445691\n\n\n\n\n\nimage-20211207134453471\n\n\n\n\n\nimage-20211207134502336\n\n\n\n\n\nimage-20211207134511684\n\n\n\n\n\nimage-20211207134519342\n\n\n\n\n\nimage-20211207134527768\n\n\n\n\n\nimage-20211207134535359\n\n\n\n\n\nimage-20211207134543507\n\n\n\n\n\nimage-20211207134552336\n\n\n\n\n\nimage-20211207134601368\n\n\n\n\n\nimage-20211207134609199\n\n\n\n\n\nimage-20211207134618035\n\n\n\n\n\nimage-20211207134625973\n\n\n\n\n\nimage-20211207134632861\n\n\n\n\n\nimage-20211207134640126\n\n\n\n\n\nimage-20211207134646017\n\n\n\n\n\n\n\n\nimage-20211207134700872\n\n\n\n\n\n\n\n\nimage-20211207135209167\n\n\n\n\n\n\n\n\nimage-20211207135217731\n\n\n\n\n\n\n\n\nimage-20211207135232436\n\n\n\n\n\nimage-20211207135240236\n\n\n\n\n\nimage-20211207135257433\n\n\n\n\n\nimage-20211207135303049\n\n\n\n\n\nimage-20211207135308324\n\n\n\n\n\nimage-20211207135312825\n\n\n\n\n\nimage-20211207135321039\n\n\n\n\n\nimage-20211207135329210\n\n\n\n\n\nimage-20211207135334453\n\n\n\n\n\nimage-20211207135339882\n\n\n\n\n\n\n\n\nimage-20211207135355389\n\n\n\n\n\n\n\n\nimage-20211207140949415\n\n\n\n\n\n\n\n\nimage-20211207141009574\n\n\n\n\n\nimage-20211207141015822\n\n\n\n\n\nimage-20211207141022055\n\n\n\n\n\nimage-20211207141028740\n\n\n\n\n\nimage-20211207141034734\n\n\n\n\n\n\n\n\nimage-20211207141052286\n\n\n\n\n\nimage-20211207141058341\n\n\n\n\n\nimage-20211207141104364\n\n\n\n\n\n\n\n\nimage-20211207141117012\n\n\n\n\n\n\n\n\nimage-20211207141448215\n\n\n\n\n\nimage-20211207141454494\n\n\n\n\n\nimage-20211207141501260\n\n\n\n\n\nimage-20211207141506607\n\n\n\n\n\nimage-20211207141512251\n\n\n\n\n\nimage-20211207141517618\n\n\n\n\n\nimage-20211207141524970\n\n\n\n\n\nimage-20211207141531540\n\n\n\n\n\nimage-20211207141538068\n\n\n\n\n\nimage-20211207141541742\n\n\n\n\n\n\n\n\nimage-20211207141551496\n\n\n\n\n\n\n\n\nimage-20211207141601905\n\n\n\n\n\n\n\n\nimage-20211207141612359\n\n\n\n\n\n\n\n\nimage-20211207141621642"
  },
  {
    "objectID": "posts/2021-10-02-kaggle-Pandas/2021-10-02-kaggle-Pandas.html",
    "href": "posts/2021-10-02-kaggle-Pandas/2021-10-02-kaggle-Pandas.html",
    "title": "Kaggle - Pandas",
    "section": "",
    "text": "image-20211002094657783\n\n\n\n\n\nimage-20211002094819228\n\n\n\n\n\n\n\n\n\n\nimage-20211002095402026\n\n\n\n\n\nimage-20211002095420960\n\n\n\n\n\n\n\n\n\nimage-20211002100732122\n\n\n\n\nA map is a term, borrowed from mathematics, for a function that takes one set of values and “maps” them to another set of values.\n\n\n\nimage-20211002100817452\n\n\nThe function you pass to map() should expect a single value from the Series (a point value, in the above example), and return a transformed version of that value. map() returns a new Series where all the values have been transformed by your function.\n\n\n\nimage-20211002100913583\n\n\n\n\n\nimage-20211002101001344\n\n\n\n\n\ndescriptor_counts = pd.Series([reviews['description'].map(lambda x: 'tropical' in x).sum(), reviews['description'].map(lambda x: 'fruity' in x).sum()], index=['tropical', 'fruity'])\n# descriptor_counts\n\n# Check your answer\nq6.check()\n\n\n\n\n\n\ngroupby() created a group of reviews which allotted the same point values to the given wines. Then, for each of these groups, we grabbed the points() column and counted how many times it appeared. value_counts() is just a shortcut to this groupby() operation.\n\n\n\nimage-20211002145008983\n\n\n\n\n\nimage-20211002145105647\n\n\n\n\n\nimage-20211002145157713\n\n\nAnother groupby() method worth mentioning is agg(), which lets you run a bunch of different functions on your DataFrame simultaneously. For example, we can generate a simple statistical summary of the dataset as follows:\n\n\n\nimage-20211002145227678\n\n\n\n\n\n\n\n\nimage-20211002145549134\n\n\nMulti-indices have several methods for dealing with their tiered structure which are absent for single-level indices. They also require two levels of labels to retrieve a value.\n\n\n\nimage-20211002145648037\n\n\n\n\n\nsort_values() defaults to an ascending sort, where the lowest values go first. However, most of the time we want a descending sort, where the higher numbers go first.\n\n\n\nimage-20211002145731562\n\n\n\n\n\nimage-20211002145742868\n\n\n\n\n\n\n\n\nThe data type for a column in a DataFrame or a Series is known as the dtype.\n\n\n\nimage-20211002152246617\n\n\nfloat64 means that it’s using a 64-bit floating point number; int64 means a similarly sized integer instead, and so on.\nOne peculiarity to keep in mind (and on display very clearly here) is that columns consisting entirely of strings do not get their own type; they are instead given the object type.\nIt’s possible to convert a column of one type into another wherever such a conversion makes sense by using the astype() function.\n\n\n\nimage-20211002152325928\n\n\n\n\n\nEntries missing values are given the value NaN, short for “Not a Number”. For technical reasons these NaN values are always of the float64 dtype.\nPandas provides some methods specific to missing data. To select NaN entries you can use pd.isnull() (or its companion pd.notnull()). This is meant to be used thusly:\n\n\n\nimage-20211002152400996\n\n\nReplacing missing values is a common operation. Pandas provides a really handy method for this problem: fillna(). fillna() provides a few different strategies for mitigating such data.\n\n\n\nimage-20211002152429049\n\n\nOr we could fill each missing value with the first non-null value that appears sometime after the given record in the database. This is known as the backfill strategy.\n\n\n\nimage-20211002152459962\n\n\n\n\n\n\n\n\n\n\n\nimage-20211002153750555\n\n\n\n\n\nimage-20211002153803885\n\n\n\n\n\nimage-20211002153827767\n\n\n\n\n\n\n\n\nimage-20211002153859890\n\n\n\n\n\nimage-20211002153919768"
  },
  {
    "objectID": "posts/2021-11-06-probability-and-statistics-week-10/2021-11-06-probability-and-statistics-week-10.html",
    "href": "posts/2021-11-06-probability-and-statistics-week-10/2021-11-06-probability-and-statistics-week-10.html",
    "title": "Probability and Statistics Week 10",
    "section": "",
    "text": "모르는 분포에 대해서 가장 근사(best guess / best estimate)적인 단일 값을 구하는 것\n\n\n\nimage-20211106124712947\n\n\n\n\n\n\n\n\nimage-20211106124816375\n\n\n\n\n\n\n\nimage-20211106124922303\n\n\n\n\n\n\n\n\n\nimage-20211106125010623\n\n\n\n\n\nimage-20211106125047631\n\n\n\n데이터를 보고 가능한 분포의 후보를 정함\n각 분포를 estimation\nbest fit distiburion을 찾음 - 보통 한 데이터를 설명할 수 있는 분포에는 여러 종류가 있음\n\n\n\n\n\n\n\nimage-20211106125455695\n\n\nCentral Limit Theorem\n\n\n\nimage-20211106125538055\n\n\nsample mean의 probability density function을 그려보고 비슷한 distribution을 찾아서 statistical inference\n\n\n\n\n\nimage-20211106125613063\n\n\n\n\n\n\n\n\n\nimage-20211106125650805\n\n\n\n\n\n\n\nimage-20211106125804710\n\n\n\n\n\nimage-20211106125926479\n\n\n\n\n\n\n\n\n\nimage-20211106130035063\n\n\n\n\n\n\n\n\nimage-20211106130045055\n\n\n만약 분포가 n -1이 아니라 n이라면\n\n\n\nimage-20211106130306310\n\n\n항상 작게 측정되는 문제가 발생함\n\n\n\n\n\n\nimage-20211106130342325\n\n\n\n\n\n\n\n\nimage-20211106130448998\n\n\n\n\n\n\n\n\nimage-20211106130541142\n\n\n\n\n\nimage-20211106130556872\n\n\nbiased estimator라서 bias가 존재할 경우 서로 bias가 다르고, variance도 다른데 이 경우에는 어떻게 비교해야 할까? 어떤 estimator가 더 좋은 estimator일까?\n\n\n\n\n\n\nimage-20211106130647262\n\n\n\n\n\n\n\n\nimage-20211106130827670\n\n\n\n\n\nimage-20211106130904671\n\n\n\n\n\nimage-20211106130958085\n\n\n\n\n\n\n\n\nimage-20211106131027623\n\n\n\n\n\n\n\n\nimage-20211106131102623\n\n\n\n\n\nimage-20211106131218366\n\n\n\n\n\n\n\n\nimage-20211106131251192\n\n\n\n\n\n\n\n\nimage-20211106131419878\n\n\n\n\n\n\n\nimage-20211106131717037\n\n\n\n\n\n\n\n\n\nimage-20211106131835104\n\n\n\n\n\n\n\n\nimage-20211106131857096\n\n\nUnbiased estimator임을 보장하지 않음\n\n\n\n\n\nimage-20211106132011655\n\n\n\n\n\nimage-20211106132053382"
  },
  {
    "objectID": "posts/2021-12-10-probability-and-inferential-statistics-week-11/2021-12-10-probability-and-inferential-statistics-week-11.html",
    "href": "posts/2021-12-10-probability-and-inferential-statistics-week-11/2021-12-10-probability-and-inferential-statistics-week-11.html",
    "title": "Probability and Inferential Statistics Week 11",
    "section": "",
    "text": "prob_infer_stats_week_11_1\n\n\n\n\n\nprob_infer_stats_week_11_2\n\n\n\n\n\nprob_infer_stats_week_11_3\n\n\n\n\n\nprob_infer_stats_week_11_4"
  },
  {
    "objectID": "posts/2021-11-05-algorithms-week-10/2021-11-05-algorithms-week-10.html",
    "href": "posts/2021-11-05-algorithms-week-10/2021-11-05-algorithms-week-10.html",
    "title": "Algorithms Week 10",
    "section": "",
    "text": "image-20211105154620461\n\n\n\n\n\n\n\n\nimage-20211105154634909\n\n\n\n\n\n\n\nimage-20211105154644192\n\n\n\n\n\nimage-20211105154653856\n\n\n\n\n\nimage-20211105154659840\n\n\n\n\n\nimage-20211105154706273\n\n\n\n\n\nimage-20211105154713292\n\n\n\n\n\nimage-20211105154720858\n\n\n\n\n\nimage-20211105154730966\n\n\n\n\n\n\n\n\n\nimage-20211105155406001\n\n\n\n\n\n\n\n\nimage-20211105155420483\n\n\n\n\n\nimage-20211105155431038\n\n\n\n\n\nimage-20211105155439388\n\n\n\n\n\nimage-20211105155447468\n\n\n\n\n\nimage-20211105155622120\n\n\n\n\n\nimage-20211105155631690\n\n\n\n\n\nimage-20211105155640018\n\n\n\n\n\nimage-20211105155649627\n\n\n\n\n\nimage-20211105155656987\n\n\n\n\n\nimage-20211105155704100\n\n\n\n\n\nimage-20211105155711081\n\n\n\n\n\nimage-20211105155719083\n\n\n\n\n\nimage-20211105155726009\n\n\n\n\n\nimage-20211105155735415\n\n\n\n\n\n\n\n\nimage-20211105155747640\n\n\n\n\n\nimage-20211105160347966\n\n\n\n\n\nimage-20211105160354164\n\n\n\n\n\nimage-20211105160403823\n\n\n\n\n\n\n\n\nimage-20211105160413886\n\n\n\n\n\n\n\n\nimage-20211105160426595\n\n\n\n\n\n\n\n\nimage-20211105160437243\n\n\n\n\n\nimage-20211105161455991\n\n\n\n\n\nimage-20211105161506768\n\n\n\n\n\nimage-20211105161518358\n\n\n\n\n\nimage-20211105161543479\n\n\n\n\n\nimage-20211105161552447\n\n\n\n\n\nimage-20211105161600521\n\n\n\n\n\n\n\n\nimage-20211105161616726\n\n\n\n\n\n\n\n\nimage-20211105161628610\n\n\n\n\n\n\n\n\nimage-20211105161810146\n\n\n\n\n\n\n\n\nimage-20211105161826227\n\n\n\n\n\n\n\n\nimage-20211105162124216\n\n\n\n\n\nimage-20211105162331921\n\n\n\n\n\n\n\n\nimage-20211105162136427\n\n\n\n\n\n\n\n\nimage-20211105162147658\n\n\n\n\n\nimage-20211105162606249\n\n\n\n\n\n\n\n\nimage-20211105162619212\n\n\n\n\n\n\n\n\nimage-20211105162636344\n\n\n\n\n\n\n\n\nimage-20211105162647934\n\n\n\n\n\n\n\n\nimage-20211105162913151\n\n\ntime complexity가 Dijkstra’s algorithm보다 높다\n\n\n\n\n\n\nimage-20211105194544656\n\n\n\n\n\n\n\n\nimage-20211105194756414\n\n\n\n\n\n\n\n\nimage-20211105195008495\n\n\n음의 가중치가 존재할 때, shortest path는 무엇인가 - cycle이 없어야 한다. 무한히 negative cycle을 돌면 -inf 값으로 수렴하기 때문\n그래서 negative cycle이 있을 때 shortest path는 의미가 없다.\n\n\n\n\n\n\nimage-20211105195238896\n\n\nu의 인접노드를 전부 돌면서 최단거리 계산\n\n\n\n\n\n\nimage-20211105195254112\n\n\n\n\n\n\n\n\nimage-20211105200639135\n\n\n\n\n\nimage-20211105200756719\n\n\n\n\n\nimage-20211105200804777\n\n\n\n\n\nimage-20211105200813046\n\n\n\n\n\nimage-20211105200819910\n\n\n\n\n\nimage-20211105200829993\n\n\n\n\n\n\n\n\nimage-20211105200844470\n\n\nm = number of edges\nn = number of vertices\n\n\n\n\n\n\nimage-20211105201504489\n\n\n\n\n\n\n\n\nimage-20211105201626437\n\n\n\n\n\n\n\n\nimage-20211105201837777\n\n\n\n\n\nimage-20211105201845614\n\n\n\n\n\n\n\n\nimage-20211105201858968\n\n\n\n\n\n\n\n\nimage-20211105202208598\n\n\n\n\n\n\n\n\nimage-20211105202220591\n\n\n\n\n\nimage-20211105204044922\n\n\n\n\n\nimage-20211105204054892\n\n\n\n\n\n\n\n\nimage-20211105204109802\n\n\n\n\n\n\n\n\nimage-20211105204123292\n\n\n\n\n\n\n\n\nimage-20211105204135725\n\n\n\n\n\n\n\n\nimage-20211105205648715"
  },
  {
    "objectID": "posts/2021-12-06-probability-and-statistics-week-10/2021-12-06-probability-and-statistics-week-11.html",
    "href": "posts/2021-12-06-probability-and-statistics-week-10/2021-12-06-probability-and-statistics-week-11.html",
    "title": "Probability and Statistics Week 11",
    "section": "",
    "text": "image-20211206024738160\n\n\n\n\n\nimage-20211206024748518\n\n\n\n\n\nimage-20211206024755365\n\n\n\n\n\nimage-20211206024802036\n\n\n\n\n\nimage-20211206024809133\n\n\n\n\n\nimage-20211206024816317\n\n\n\n\n\nimage-20211206024823660\n\n\n\n\n\n\n\n\nimage-20211206033541834\n\n\n\n\n\nimage-20211206033638265\n\n\n\n\n\n\n\n\nimage-20211206033650027\n\n\n\n\n\n\n\n\nimage-20211206033752181\n\n\n\n\n\nimage-20211206033853007\n\n\nUpper case는 Random variable, lower case는 sample observations\n\n\n\n\n\n\nimage-20211206033924773\n\n\n\n\n\nimage-20211206034025732\n\n\n\n\n\nimage-20211206034301459\n\n\n\n\n\nimage-20211206034409819\n\n\n\n\n\nimage-20211206034418384\n\n\n\n\n\n\n\n\nimage-20211206034555612\n\n\n\n\n\nimage-20211206034602293\n\n\n\n\n\nimage-20211206034608090\n\n\n\n\n\n\n\n\nimage-20211206034622709\n\n\n\n\n\nimage-20211206034917674\n\n\n\n\n\nimage-20211206035151310\n\n\n\n\n\n\n\n\nimage-20211206035306472\n\n\n\n\n\n\n\n\nimage-20211206035320848\n\n\n\n\n\n\n\n\nimage-20211206035434673\n\n\n\n\n\nimage-20211206035455172\n\n\n\n\n\n\n\n\nimage-20211206035544513\n\n\n\n\n\nimage-20211206035502182\n\n\n\n\n\nimage-20211206035512963\n\n\n\n\n\nimage-20211206035519676\n\n\n\n\n\nimage-20211206035524521\n\n\n\n\n\nimage-20211206035532547\n\n\n\n\n\n\n\n\nimage-20211206040325272\n\n\n\n\n\nimage-20211206040330630\n\n\n\n\n\n\n\n\nimage-20211206040339993\n\n\n\n\n\n\n\n\nimage-20211206040350877\n\n\n\n\n\nimage-20211206040627183\n\n\n\n\n\nimage-20211206040742797\n\n\n\n\n\n\n\n\nimage-20211206040756903\n\n\nunbiased estimator임을 보장하지 않음\n\n\n\nimage-20211206041051670\n\n\n\n\n\nimage-20211206041101192\n\n\n\n\n\nimage-20211206041111403"
  },
  {
    "objectID": "posts/2021-11-02-CFA-Level-2-Quantitative-Methods/2021-11-02-CFA-Level-2-Quantitative-Methods.html",
    "href": "posts/2021-11-02-CFA-Level-2-Quantitative-Methods/2021-11-02-CFA-Level-2-Quantitative-Methods.html",
    "title": "CFA Level 2 Quantitative Methods",
    "section": "",
    "text": "image-20211103195000777\n\n\n\n\n\nimage-20211103195015140\n\n\n\n\n\nimage-20211103195023085\n\n\n\n\n\nimage-20211103195031609\n\n\n\n\n\nimage-20211103195128087\n\n\n\n\n\nimage-20211103195136301\n\n\n\n\n\nimage-20211103195512974\n\n\n\n\n\nimage-20211103195523261\n\n\n\n\n\nimage-20211103195541485\n\n\n\n\n\nimage-20211103195609644\n\n\n\n\n\nimage-20211103200516776\n\n\n\n\n\nimage-20211103200500839\n\n\n\n\n\nimage-20211103200505112\n\n\n\n\n\nimage-20211103200528841\n\n\n\n\n\nimage-20211103200534392\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\\\\nY_{i} = b_{0} + b_{1}X_{i} + \\epsilon_{i}, i = 1, ..., n \\\\\nY = \\text{dependent variable} \\\\\nX = \\text{independent variable} \\\\\n\\epsilon = \\text{error term} \\\\\nb_{0} = \\text{intercept} \\\\\nb_{1} = \\text{slope coefficients}\n\\]\nWe refer to \\(b_{0}, b_{1}\\) as the regression coefficients. \\[\n\\\\\n\\text{minimize} = \\sum^{n}_{i = 1}{(Y_{i} - \\hat{b_{0}} - \\hat{b_{1}}X_{i})^{2}}\n\\] \n \\[\n\\\\\n\\text{long-term inflation} = -0.0008 + 0.3339 (\\text{long-term money supply growth}) \\\\\n\\text{slope coefficient} = \\frac{Cov(Y, X)}{Var(X)} \\approx \\beta = \\frac{Cov(R_{m}, R_{i})}{Var(R_{m})}\n\\]\n\n\n\n\n\n\n\\[\n\\\\\n\\text{SEE (Standard Error of Estimate)} = (\\frac{\\sum^{n}_{i = 1}{(Y_{i} - \\hat{b_{0}} - \\hat{b_{1}}X_{i})^{2}}}{n-2})^{1/2} = (\\frac{\\sum^{n}_{i = 1}{(\\hat{\\epsilon_{i}}})^{2}}{n-2})^{1/2}\n\\]\n\n\n\n\n\n\nimage-20211103195211431\n\n\n \\[\n\\\\\nR^{2} = \\frac{\\text{Explained variation}}{\\text{Total variation}} = \\frac{\\text{Total variation} - \\text{Unexplained variation}}{\\text{Total variation}}\n\\]\n\n\n\n\n\n\nimage-20211103200549988\n\n\n\n\n\nimage-20211103200606161\n\n\n\n\n\nimage-20211103200617507\n\n\nSuppose we regress a stock’s returns on a stock market index’s returns and find that the slope coefficient \\((b_{1})\\) is 1.5 with a standard error \\((s_{\\hat{b_{1}}})\\) of 0.200. Assume we used 62 monthly observations in our regression analysis. The hypothesized value of the parameter \\((\\hat{b_{1}})\\) is 1.0, the market average slope efficient. The estimated and the population slope coefficients are often called beta, because the population coefficient is often represented by the Greek symbol beta \\((\\beta)\\) rather than the \\(b_{1}\\) we use in this reading. Our null hypothesis is that \\(b_{1} = 1.0\\) and \\(\\hat{b_{1}}\\) is the estimate for \\(b_{1}\\). We will use a 95 percent confidence interval for our test, or we could say that the test has a significance level of 0.05.\nOur Confidence interval will span the range \\(\\hat{b_{1}} - t_{c}s_{\\hat{b_{1}}}\\, to\\, \\hat{b_{1}} + t_{c}s_{\\hat{b_{1}}}\\).\nTo test the hypothesis, we can compute the statistic \\[\n\\\\\nt = \\frac{\\hat{b_{1} - b_{1}}}{s_{\\hat{b_{1}}}}\n\\] \n\n\n\nimage-20211103091532462\n\n\n\n\n\n\n\n\nimage-20211103200626977\n\n\n\n\n\nimage-20211103200641562\n\n\nAnalysis of variance (ANOVA)\n\n\n\nimage-20211103091620899\n\n\n\n\n\nimage-20211103091628571\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103200655136\n\n\nFist, regression relations can change over time, parameter instability.\nA second limitation is public knowledge of regression relationships may negate their future usefulness.\nFinally, if the regression assumptions are violated, hypothesis tests and predictions based on linear regression will not be vaild.\n\n\n\nimage-20211103091806132\n\n\n\n\n\nimage-20211103091814731\n\n\n\n\n\nimage-20211103091820633\n\n\n\n\n\nimage-20211103200715501\n\n\n\n\n\nimage-20211103200723207\n\n\n\n\n\nimage-20211103091827905\n\n\n\n\n\nimage-20211103091833620\n\n\n\n\n\nimage-20211103091839987\n\n\n\n\n\nimage-20211103200743764\n\n\n\n\n\n\n\n\nimage-20211103200758726\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple linear regression model \\[\n\\\\\nY_{i} = b_{0} + b_{1}X_{1i} + b_{2}X_{2i} + ... + b{k}X_{ki} + \\epsilon_{i}, i = 1, 2, ..., n \\\\\nb_{1}, ..., b_{k} = \\text{regression coefficient}\n\\] \n \\[\n\\\\\nt = \\frac{\\hat{b_{j}} - b_{j}}{s_{\\hat{b_{j}}}}\n\\] \npartial regression coefficient or partial slope coefficient - a slope coefficient holing the other independent variable constant\n\n\n\n\n\n\nimage-20211103200956502\n\n\n\nThe relationship between the dependent variable, Y, and the independent variables, \\(X_{1}, X_{2}, ..., X_{k}\\), is linear.\nThe independent variables (\\(X_{1}, X_{2}, ..., X_{k})\\) are not random. Also, no exact linear relationship exists between two or more of the independent variables.\nThe expected value of the error term, conditioned on the independent variables, is \\(E(\\epsilon | X_{1}, X_{2}, ..., X_{k}) = 0\\).\nThe variance of the error term is the same for all observations, \\(E(\\epsilon_{i}^{2} = \\sigma_{\\epsilon}^{2})\\).\nThe error term is uncorrelated across observations: \\(E(\\epsilon_{i}\\epsilon_{j}) = 0, i \\neq j\\).\nThe error term is normally distributed.\n\nIf the independent variable is random, then is the regression model incorrect? Fortunately, no. Even if the independent variable is random but uncorrelated with the error term, we can still rely on the results of regression models.\n\n\n\nimage-20211103094528099\n\n\n\n\n\nimage-20211103094535415\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103200849555\n\n\n\n\n\nimage-20211103200907175\n\n\nWhat if we now want to test the significance of the regression as a whole? As a group, do the independent variables help explain the dependent variable? \\[\n\\\\\nF = \\frac{RSS / k}{SSE / [n - (k + 1)]} = \\frac{\\text{mean regression of squares}}{\\text{mean squared error}} = \\frac{\\text{MSR}}{\\text{MSE}}\n\\] \n\n\n\n\\[\n\\\\\n\\bar{R^{2}} = 1 - \\frac{n-1}{n - k - 1}(1 - R^{2})\n\\]\n\n\n\n\n\n\nimage-20211103200923517\n\n\n\n\n\nimage-20211103200929596\n\n\n\n\n\nimage-20211103094912456\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103201022396\n\n\nthe variance of the errors differs across observations.\n\n\nAlthough heteroskedasticity does not affect the consistency of the regression parameter estimators, it can lead to mistakes in inference. When errors are heteroskedastic, the F-test for the overall significance of the regression is unreliable. Furthermore, t-tests for the significance of individual regression coefficients are unreliable. If a regression show significant heteroskedasticity, the standard errors and test statistics computed by regression programs will be incorrect.\n\n\n\nimage-20211103095210422\n\n\nUnconditional heteroskedasticity occurs when heteroskedasticity of the error variance is not correlated with the independent in the multiple regression. - x에 pattern 없음\nConditional heteroskedasticity-heteroskedasticity in the error variance that is correlated with (conditional on) the value of the independent variables in the regression. - x에 pattern 있음\n\n\n\n\n\n\nimage-20211103201045268\n\n\nBreusch and Pagan suggested the following test for conditional heteroskedasticity.\n\n\n\nimage-20211103132915227\n\n\n\n\n\nimage-20211103132924578\n\n\nBreusch-Pagan test\n\n\n\nThe first method, computing robust standard errors, corrects the standard errors of the linear regression model’s estimated coefficients to account for the conditional heteroskedasticity. The second method, generalized least squares, modifies the original equation in an attempt to eliminate the heteroskedasticity.\n\n\n\nimage-20211103133116181\n\n\n\n\n\nimage-20211103201324924\n\n\n\n\n\n\n\n\n\nimage-20211103201332344\n\n\n\n\n\nimage-20211103201424049\n\n\n\n\nAs with heteroskedasticity, the principal problem caused by serial correlation in a linear regression is an incorrect estimate of the regression coefficient standard errors computed by statistical software package. Then the estimated parameters themselves will be consistent.\nPositive serial correlation is serial correlation in which a positive error for one observation increases the change of a positive error for another observation.\nIn contrast, with negative serial correlation, a positive error for one observation increases the change of a negative error for another observation, and a negative error for one observation increases the chance of a positive error for another.\nfirst-order serial correlation - 1기간 serial correlation\n\n\n\n\n\n\nimage-20211103201431615\n\n\n\n\n\nimage-20211103201439302\n\n\n\n\n\nimage-20211103201508346\n\n\n\n\n\nimage-20211103201517830\n\n\n\n\n\nimage-20211103201542880\n\n\nDurbin-Watson statistic \\[\n\\\\\nDW = \\frac{\\sum^{T}_{t=2}{(\\hat{\\epsilon_{t}} - \\hat{\\epsilon_{t - 1}})^{2}}}{\\sum^{T}_{t = 1}{\\hat{\\epsilon_{t}^{2}}}}\n\\] Therefore, we can test the null hypothesis that the errors are not serially correlated by testing whether the Durbin-Watson statistic differs significantly from 2.\nIf the sample is very large, the Durbin-Watson statistic will be approximately equal to 2(1 - r), where r is the sample correlation between the regression residuals from one period and those from the previous period.\n\nIf the regression has no serial correlation, then the regression residuals will be uncorrelated through time and the value of the Durbin-Watson statistic will be equal to 2(1 - 0) = 2.\nIf the regression residuals are positively serially correlated, then the Durbin-Watson statistic will be less than 2.\n\n\n\n\nimage-20211103133850315\n\n\n\n\n\nFirst, we can adjust the coefficient standard errors for the linear regression parameter estimates to account for the serial correlation. Second, we can modify the regression equation itself to eliminate the serial correlation. We recommend using the first method for dealing with serial correlation.\nTwo of the most prevalent methods for adjusting standard errors were developed by Hansen and Newey and West. These methods are standard features in many statistical software package. An additional advantage of these methods is that they simultaneously correct for conditional heteroskedasticity.\n\n\n\nimage-20211103134059361\n\n\n\n\n\n\n\n\n\nimage-20211103201614656\n\n\nThe second assumption of the multiple linear regression model is that no exact linear relationship exists between two or more of the independent variables.\nMulticollinearity occurs when two or more independent variables (or combinations of independent variables) are highly (but not perfectly) correlated with each other.\n\n\nAlthough the presence of multicollinearity does not affect the consistency of the OLS estimates of the regression coefficients, the estimates become extremely imprecise and unreliable. Furthermore, it becomes practically impossible to distinguish the individual impacts of the independent variables on the dependent variable.\n\n\n\nIn contrast to the cases of heteroskedasticity and serial correlation, we shall not provide a formal statistical test for multicollinearity. In practice, multicollinearity is often a matter of degree rather than of absence of presence.\nThe analyst should be aware that using the magnitude of pairwise correlations among the independent variables to assess multicollinearity.\nThe classic symptom of multicollinearity is a high \\(R^{2}\\) (and significant F-statistic) even though the t-statistics on the estimated slope coefficients are not significant.\n\n\n\nimage-20211103134610968\n\n\n\n\n\nimage-20211103134617775\n\n\n\n\n\nThe most direct solution to multicollinearity is excluding one or more of the regression variables.\n\n\n\n\n\n\n\nimage-20211103134719162\n\n\n\n\n\n\n\n\nimage-20211103201656532\n\n\nModel specification refers to the set of variables included in the regression and the regression equation’s functional form.\n\n\n\n\n\n\nimage-20211103201730252\n\n\n\nOne or more important variables could be omitted from regression.\nOne or more of the regression variables may need to be transformed (For example, by taking the natural logarithm of the variable) before estimating the regression.\nThe regression model pools data from different samples that should not be pooled.\n\nIf the true regression model was\n\n\n\nimage-20211103135031807\n\n\n\n\n\nimage-20211103135039676\n\n\n\n\n\nimage-20211103135045292\n\n\n\n\n\n\n\n\nimage-20211103201752790\n\n\n\n\n\nimage-20211103201805991\n\n\nThree common problems that create this type of time-series misspecification are:\n\nIncluding lagged dependent variables as independent variables in regressions with serially correlated errors;\nIncluding a function of a dependent variable as an independent variable, sometimes as a result of the incorrect dating of variables; and\nindependent variables that are measured with error.\n\nlagged dependent variable \\[\nY_{i} = b_{0} + b_{1}X_{1i} + b_{2}Y_{i - 1} + \\epsilon_{t}\n\\] forecast the past \\[\n\\\\\nY_{t + 1} = b_{0} + b_{1}X_{1t} + \\epsilon_{t + 1}\n\\] independent variable is measured with error\n\n\n\nimage-20211103135736898\n\n\n\n\n\nUnfortunately, the predicted value of the probability could be much greater than 1 or much lower than 0 depending on the estimated coefficients \\(b_{i}\\), and the value observed \\(X_{i}^{`}\\)s. Of course, these results would be invalid. The probability of bankruptcy (or of anything, for that matter) cannot be greater than 1.0 or less than 0.0.\nprobit regression (probit model), logistic regression (logit model) \\[\n\\\\\n\\ln{\\frac{p}{1 - p}} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3} + \\epsilon \\\\\n\\hat{p} = \\frac{exp[\\hat{b_{0}} + \\hat{b_{1}}X_{1} + \\hat{b_{2}}X_{2} + \\hat{b_{3}}X_{3}]}{1 + exp[\\hat{b_{0}} + \\hat{b_{1}}X_{1} + \\hat{b_{2}}X_{2}]}\n\\] \n\n\n\n\n\n\n\nimage-20211103201825047\n\n\n\n\n\n\n\nimage-20211103140354817\n\n\n\n\n\n\n\n\nimage-20211103201901550\n\n\n\n\n\n\\[\n\\\\\ny_{t} = b_{0} + b_{1}t + \\epsilon_{t}, t = 1, 2, ..., T\n\\]\n\n\n\nimage-20211103140455523\n\n\n\n\n\nimage-20211103140504777\n\n\n\n\n\nTherefore, exponential growth is growth at a constant rate. continuous compounding is a mathematical convenience that allows us to restate the equation in a form that is easy to estimate.\nA linear trend regression for quarterly sales at Starbucks\n\n\n\nimage-20211103140628431\n\n\n\n\n\nimage-20211103140634219\n\n\n\n\n\nimage-20211103140640645\n\n\nA log-linear regression for quarterly sales at Starbucks\n\n\n\nimage-20211103140701929\n\n\n\n\n\nimage-20211103140707772\n\n\n\n\n\nWe can conduct that a regression equation that uses a linear trend to model inflation has positive serial correlation in the errors. We will need a different kind of regression model because this one violates the least-squares assumption of no serial correlation in the errors.\nWe can conclude that a regression equation that uses a trend to model the log of Starbucks’ quarterly sales has positive serial correlation in the errors. So, for this series as well, we need to build a different kind of model.\n\n\n\n\n\n\nimage-20211103202052730\n\n\n\n\n\nimage-20211103202116396\n\n\n \\[\n\\\\\nAR(1): x_{t} = b_{0} + b_{1}x_{t - 1} + \\epsilon_{t}\n\\]\n\n\n\nFirst, the expected value of the time series must be constant and finite in all periods: \\(E(y_{t}) = \\mu\\) and \\(|\\mu| < \\infin, t = 1, 2, ..., T\\). Second, the variance of the time series must be constant and finite in all periods. Third, the covariance of the time series with itself for a fixed number of periods in the past or future must be constant and finite in all periods. the second and third requirements can be summarized as follows.\n\n\n\n\n\n\nimage-20211103202221439\n\n\nWe can estimate an autoregressive model using ordinary least squares if the time series is covariance stationary and the errors are uncorrelated. Unfortunately, our previous test for serial correlation, the Durbin-Watson statistic, is invailid when the independent variables include pas values of the dependent variable. \\[\n\\\\\n\\rho_{k} = \\frac{Cov(x_{t}, x_{t - k})}{\\sigma_{x}^{2}}\n\\] We can determine whether we are using the correct time-series model by testing whether the autocorrelations of the error term (error autocorrelations) differ significantly from 0.\nHow can we use information about the error autocorrelations to determine whether an autoregressive time-series model is correctly specified? First, estimate a particular autoregressive model, say an AR(1) model, Second, compute the autocorrelations of the residuals from the model. Third, test to see whether the residual autocorrelations differ significantly from 0.\n\n\n\nimage-20211103162408097\n\n\n\n\n\nWe say that a time series shows mean reversion if it tends to fall when its level is above its mean and rise when its level is below its mean.\nIf a time series is currently at its mean-reverting level, then the model predicts that the value of the time series will be the same in the next period. At its mean-reverting level, we have the relationship \\(x_{t + 1} = x_{t}\\). For an AR(1) model, \\(x_{t + 1} = b_{0} + b_{1}x_{t}\\), the equality \\(x_{t + 1} = x_{t}\\) implies the level \\(x_{t} = b_{0} + b_{1}x_{t}\\) or that the mean-reverting level, is given by \\[\n\\\\\nx_{i} = \\frac{b_{0}}{1 - b_{1}}\n\\] As we will discuss later, all covariance-stationary time series have a finite mean-reverting level.\n\n\n\nimage-20211103162859231\n\n\n\n\n\nimage-20211103162905495\n\n\n\n\n\n\n\n\nimage-20211103202237500\n\n\nIn-sample forecast errors / out-of-sample forecast errors\n\n\n\nimage-20211103202249731\n\n\n\n\n\nimage-20211103163609960\n\n\n\n\n\n\n\n\nimage-20211103202304212\n\n\n\n\n\nimage-20211103163628622\n\n\n\n\n\n\n\n\nimage-20211103202314599\n\n\n\n\n\nimage-20211103202347456\n\n\n \\[\n\\\\\nx_{t} = x_{t - 1} + \\epsilon_{t}, E(\\epsilon_{t}^{2}) = \\sigma^{2}, Cov(\\epsilon_{t}, \\epsilon_{s}) = E(\\epsilon_{t}\\epsilon_{s}) = 0\\, \\text{if}\\, t \\neq s\n\\] First, this equation is a special case of an AR(1) model with \\(b_{0} = 0\\) and \\(b_{1} = 1\\). Second the expected value of \\(\\epsilon_{t}\\) is zero. Therefore, the best forecast of \\(x_{t}\\) that can be made in period t - 1 is \\(x_{t - 1}\\).\nUnfortunately, we cannot use the regression methods we have discussed so far to estimate an AR(1) model on a time series that is actually a random walk. Recall that if \\(x_{t}\\) is at its mean-reverting level, then \\(x_{t} = b_{0}\\, and\\, b_{1} = 1\\), so \\(b_{0} / (1 - b_{1}) = 0 / 0\\)\nWhat is the variance of a random walk?\nWhat is the practical implication of these issues? We cannot use standard regression analysis on a time series that is a random walk.\nfirst-differencing - equal to the difference between \\(x_{t}\\) and \\(x_{t - 1}\\)\nThe first-differenced variable, \\(y_{t}\\), is covariance stationary. How is this so? First, note that this model (\\(y_{t} = \\epsilon_{t}\\)) is an AR(1) model with \\(b_{0} = 0\\) and \\(b_{1} = 0\\).\n\n\n\nimage-20211103164424289\n\n\n\n\n\nimage-20211103164432139\n\n\n\n\n\nimage-20211103164449546\n\n\n\n\n\n\n\n\nimage-20211103202436557\n\n\n\n\n\nimage-20211103164506254\n\n\n\n\n\n\n\n\nimage-20211103202454854\n\n\nn-period moving average \\[\n\\\\\n\\frac{x_{t} + x_{t - 1} + ... + x_{t - (n - 1)}}{n}\n\\] \n\n\n\n\\[\n\\\\\nx_{t} = \\epsilon_{t} + \\theta\\epsilon_{t - 1}, E(\\epsilon_{t}) = 0, E(\\epsilon_{t}^{2}) = \\sigma^{2} \\\\\nCov(\\epsilon_{t}, \\epsilon_{s}) = E(\\epsilon_{t}\\epsilon_{s}) = 0\\, for\\, t \\neq s\n\\]\n\n\n\n \\[\n\\\\\nx_{t} = b_{0} + b_{1}x_{t - 1} + b_{2}x_{t - 4} + \\epsilon_{t}\n\\] \n\n\n\nimage-20211103164857070\n\n\n\n\n\nimage-20211103164903372\n\n\n\n\n\nimage-20211103164909315\n\n\n\n\n\nimage-20211103164915584\n\n\n\n\n\n\n\n\nimage-20211103202524458\n\n\n\n\n\n\n\n\nimage-20211103202543388\n\n\nARCH(1) \\[\n\\\\\n\\epsilon \\sim N(0, a_{0} + a_{1}\\epsilon_{t - 1}^{2})\n\\] If a time-series model has ARCH(1) errors, then the variance of the errors in period t + 1 can be predicted in period t using the formula \\(\\hat{\\sigma_{t + 1}^{2}} = \\hat{a_{0}} + \\hat{a_{1}}\\hat{\\epsilon_{t}^{2}}\\).\n\n\n\nimage-20211103165232529\n\n\n\n\n\nimage-20211103165305822\n\n\nSuppose a model contains ARCH(1) errors. What are the consequences of that fact? First, if ARCH exists, the standard errors for the regression parameters will not be correct. We will need to use generalized least squares. Second, if ARCH exists and we have it modeled, for example as ARCH(1), we can predict the variance of the errors.\nEngle and other researchers have suggested many generalizations of the ARCH(1) model, including ARCH(p) and generalized autoregressive conditional heteroskedasticity (GARCH) models. \\[\n\\\\\n\\sigma_{t}^{2} = a_{0} + a_{1}\\epsilon_{t - 1}^{2} + ... + a_{p}\\epsilon_{t - p}^{2}\n\\]\n\n\n\n\n\n\nimage-20211103202610101\n\n\nOne possible scenario is that we find that neither of the time series has a unit root. Then we can safely use linear regression to test the relations between the two time series.\nA second possible scenario is that we reject the hypothesis of a unit root for the independent variable but fail to reject the hypothesis of a unit root the dependent variable.\nA third possible scenario is the reverse of the second scenario: We reject the hypothesis of a unit root for the dependent variable but fail to reject the hypothesis of a unit root for the independent variable.\nThe next possibility is that both time series have a unit root. In this case, we need to establish whether the two time series are cointegrated before we can rely on regression analysis.\n\n\n\nimage-20211103165959772\n\n\n\n\n\nimage-20211103170005810\n\n\n\n\n\nimage-20211103170012694\n\n\n\n\n\nimage-20211103170018918\n\n\n\n\n\nimage-20211103170026859\n\n\n\n\n\nimage-20211103170809764\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103202731461\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised learning involves ML algorithms that infer patterns between a set of inputs (the X’s) and the desired output (Y). Supervised learning requires a labeled data set.\nThe dependent variable (Y) is the target, and the independent variables (X’s) are known as features.\n\n\n\nUnsupervised learning is machine learning that does not make use of labeled data.\nDimension reduction focuses on reducing the number of features while retaining variation across observations to preserve the information contained in that variation.\nClustering focuses on sorting observations into groups (clusters) such that observations in the same cluster are more similar to each other than they are to observations in other clusters.\n\n\n\n\n\n\nimage-20211103171205868\n\n\n\n\n\ntraining sample / validation sample / test sample\nComplexity refers to the number of features, terms, or branches in the model and to whether the model is linear or non-linear (non-linear is more complex).\n\n\n\n\nBias error, or the degree to which a model fits the training data.\nVariance error, or how much the model’s results change in response to new data from validation and test sample.\nBase error due to randomness in the data\n\n\n\n\nimage-20211103171655539\n\n\n\n\n\nimage-20211103171701920\n\n\n\n\n\n\npreventing the algorithm from getting too complex during selection and training, which requires estimating an overfitting penalty\nproper data sampling achieved by using cross-validation, a technique for estimating out-of-sample error directly by determining the error in validation samples\n\nholdout samples - data samples not used to train the model\nk-fold cross-validation - in which data (Excluding test sample and fresh data) are shuffled randomly and then are divided into k equal sub-samples, with k - 1 samples used as training samples and one samples the kth, used as a validation sample.\n\n\n\nimage-20211103171935664\n\n\n\n\n\n\n\n\npreventing overfitting\n\npenalty\ncross-validation\n\nto minimize the sum of the squared residuals <- original goal \\[\n\\\\\n\\sum^{n}_{t = 1}{(Y_{i} - \\hat{Y_{i}})^{2}}\n\\]\n\n\n\\[\n\\\\\n\\text{Penalty term} = \\lambda\\sum^{K}_{k = 1}{|\\hat{b_{k}}|} \\\\\n\\sum^{n}_{t = 1}{(Y_{i} - \\hat{Y_{i}})^{2}} + \\lambda\\sum^{K}_{k = 1}{|\\hat{b_{k}}|}\n\\]\nminimize LASSO\nlambda is a hyperparameter - 사전에 결정하는 변수\nRegularization describes methods that reduce statistical variability in high dimensional data estimation problems-in this case, reducing regression coefficient estimates toward zero and thereby avoiding complex models and the risk of overfitting.\n\n\n\n\nLinear classifier\n\n\n\nKNN. The idea is to classify a new observation by finding similarities (nearness) between new observation and the existing data.\n\n\n\nimage-20211103182308662\n\n\n\n\n\nCART\n\n\n\nimage-20211103182324728\n\n\nTo avoid such overfitting, regularization parameters can be added, such as the maximum depth of the tree, the minimum population at a node, or the maximum number of decision nodes. Alternatively, regularization can occur via a pruning (가지치기) technique that can be used afterward to reduce the size of the tree.\n\n\n\n\n\n\nimage-20211103203002548\n\n\n\n\n\nimage-20211103203037458\n\n\n\n\nA majority-vote classifier\nIf the SVM and KNN models are both predicting the category “stock outperformance” and the CART model is predicting the category “stock underperformance”. -> 여러 model 결과에서 많이 나온 결과 채택\n\n\n\nAlternatively, one can use the same machine learning algorithm but with different training data. Bootstrap aggregating (or bagging) is a technique whereby the original training data set is used to generate n new training data sets or bags of data.\n\n\n\n\n\n\nimage-20211103182700073\n\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103203047298\n\n\n\n\n\nimage-20211103203212187\n\n\n\n\n\nimage-20211103203220158\n\n\n\n\n\nA cluster contains a subset of observations from the data set such that all the observations within the same cluster are deemed “similar”.\n\n\n\n\n\nimage-20211103203252506\n\n\n\n\n\nimage-20211103203409374\n\n\nThe number of cluster, k, is a model hyperparameter-a parameter whose value must be set by the researcher before learning begins. Each cluster is characterized by its centroid, and each observation is assigned by the algorithm to the cluster with the centroid to which that observation is closet.\n\nK-means starts by determining the position of the k initial random centroid.\nThe algorithm then analyzes the features for each observation. Based on the distance measure that is utilized, k-means assigns each observation to its closet centroid, which defines a cluster.\nUsing the observations within each cluster, k-means then calculates the new (k) centroid for each cluster, where the centroid is the average value of their assigned observations.\nK-means then reassigns the observations to the new centroids, redefining the clusters in terms of included and excluded observations.\nThe process of recalculating the new (k) centroids for each cluster is reiterated.\nK-means then reassigns the observations to the revised centroids, again redefining the clusters in terms of observations that are included and excluded.\n\n\n\n\n\n\n\nimage-20211103203441896\n\n\n\n\n\nimage-20211103203447002\n\n\nAgglomerative clustering hierarchical clustering begins with each observation being treated as its own cluster.\nDivisive clustering (or top-down) hierarchical clustering starts with all the observations belonging to a single cluster.\n\n\n\nimage-20211103183341281\n\n\n\n\n\nA type of tree diagram for visualizing a hierarchical cluster analysis known as a dendrogram highlights the hierarchical relationships among the clusters.\n\n\n\nimage-20211103183431055\n\n\n\n\n\n\n\n\n\nnodes - circles\nlinks - arrows connecting nodes\n\n\n\nimage-20211103183521846\n\n\nThese nodes are sometimes called “neurons” because they process information received. Once the node receives the four input values, the summation operator multiples each value by a weight and sums the weighted values to form the total net input. The total net input is then passed to the activation function, which transforms this input into the final output of the node.\nforward propagation\nIf the process of adjustment works backward through the layers of the network, this process is called backward propagation.\nNew weight = Old weight - Learning rate * partial derivative of the total error with respect to the old weight\n\n\n\nimage-20211103184216705\n\n\n\n\n\nNeural networks with many hidden layers-at least 3 but often more than 20 hidden layers-are known as deep learning nets (DLNs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional ML model building steps\n\nConceptualization of the modeling task - This crucial fist step entails determining what the output of the model should be, how this model will be used and by whom, and how it will be embedded in existing or new business processes.\nData collection\nData preparation and wrangling\nData exploration\nModel training\n\nThe text ML model building steps used for the unstructured data sources of big data.\n\nText problem formulation - Identifying the exact inputs and outputs for the model. Analysts must also decide how the text ML model’s classification output will be utilized.\nData (text) curation\nText preparation and wrangling\nText exploration\n\n\n\n\nimage-20211103185515855\n\n\nAccount for the characteristics of big data: volume, velocity, variety, and veracity\n\n\n\nimage-20211103185459215\n\n\n\n\n\nData preparation (cleansing)\nData wrangling (preprocessing)\n\n\n\nData preparation (cleansing)\n\nImcompleteness error is where the data are not present, resulting in missing data\nInvalidity error is where the data are outside of a meaningful range, resulting in invalid data.\nInaccuracy error is where the data are not a measure of true value.\nInconsistency error is where the data conflict with the corresponding data points or reality.\nNon-uniformity error is where the data are not present in an identical format.\nDuplication error is where duplicate observations are present.\n\nData wrangling (preprocessing)\n\nExtraction - A new variable can be extracted from the current variable for ease of analyzing and using for training the ML model.\nAggregation - Two or more variables can be aggregated into one variable to consolidate similar variables.\nFiltration - The data rows that are not needed for the project must be identified and filtered.\nSelection - The data columns that are intuitively not needed for the project can be removed.\nConversion - The variables can be of different types: nominal, ordinal, continuous, and categorical. The variables in the dataset must be converted into appropriate types to further process and analyze them correctly. Before converting, values must be stripped out with prefixes and suffixes, such as currency symbols.\n\nAny outliers that are present must first be identified.\nIn practice, several techniques can be used to detect outliers in the data. Standard deviation can be used to identify outliers in normally distributed data. In general, a data value that is outside of 3 standard deviations from the mean may be considered an outlier. The interquartile range(IQR) can be used to identify outliers in data with any form of distribution.\n\n\n\nimage-20211103190305900\n\n\nThere are several practical methods for handling outliers. Outliers are simply removed from the dataset, it is known as trimming (also called truncation). When extreme values and outliers are replaced with the maximum (for large value outliers) and minimum (for small value outliers) values of data points that are not outliers, the process is known as winsorization.\nScaling a process of adjusting the range of a feature by shifting and changing the scale of data. Variables can have a diversity of ranges that result in a heterogeneous training dataset.\n\nNormalization is the process of rescaling numeric variables in the range of [0, 1].\nStandardization is the process of both centering and scaling the variables.\n\nNormalization is sensitive to outliers, so treatment of outliers is necessary before normalization is performed. Normalization can be used when the distribution of the data is not known. Standardization is relatively less sensitive to outliers as it depends on the mean and standard deviation of the data. However, the data must be normally distributed to use standardization.\n\n\n\n\n\nA regular expression (regex) is used to for patterns of interest in a given text.\n\nRemove html tags\nRemove punctuations\nRemove numbers\nRemove white spaces\n\n\n\n\nimage-20211103190750107\n\n\n\n\n\nToken is equivalent to a word, and tokenization is the process of splitting a given text into separate tokens.\n\n\n\nimage-20211103190831842\n\n\nSimilar to structured data, text data also require normalization.\n\nLowercasing\nStop words - 검색시 검색용어로 사용하지 않는 언어\nStemming is the process of converting inflected forms of a word into its base word (known as stem). - 어간 추출\nLemmatization - 표제어 추출\n\n\n\nimage-20211103190933331\n\n\n\nAfter the cleansed text is normalized, a bag-of-words is created. Bag-of-words (BOW) representation is a basic procedure used to analyze text.\n\n\n\nimage-20211103191019301\n\n\nDTM (document term matrix)\n\n\n\nimage-20211103203512705\n\n\n\n\n\nimage-20211103203519354\n\n\n\n\n\nimage-20211103191029776\n\n\nN-grams is representation of word sequences.\n\n\n\nimage-20211103202801682\n\n\n\n\n\nimage-20211103191045772\n\n\n\n\n\n\n\n\n\nimage-20211103191105451\n\n\n\n\n\nFor one-dimensional data\n\nhistogram\nbar charts\nbox plots\ndensity plots\n\nFor data with two or more dimensions -> \\(R^{2}\\) increases\n\n\n\nimage-20211103191149441\n\n\n\n\n\nThe most common applications are text classification, topic modeling, fraud detection, and sentiment analysis.\nTerm frequency (TF), the ratio of the number of times a given token occurs in all the texts in the dataset to the total number of tokens in the dataset.\nText statistics can be visually comprehended by using the same methods as explained in the structured data section.\n\n\n\nimage-20211103191318113\n\n\n\n\n독립변수 고르기\nnoisy features are tokens that do not contribute to ML model training and actually might detract from the ML model accuracy.\n매우 빈도가 높거나 낮은 단어들, sparse words, stop words\nThe general feature selection methods in text data are as follows:\n\nFrequency measures can be used for vocabulary pruning to remove noise features by filtering the tokens with very high and low TF values across all the texts. Document frequency (DF)\nChi-square test can be useful for feature selection in text data.\n\n\n\n\nimage-20211103203753220\n\n\n\n\n\nimage-20211103203806664\n\n\n\n\n\nimage-20211103203819362\n\n\n\n\n\nimage-20211103203904313\n\n\n\nMutual information (MI) measures how much information is contributed by a token to a class of texts. The mutual information value will be equal to 0 if the token’s distribution in all text classes is the same.\n\n\n\n\nimage-20211103203939091\n\n\n\n\n\nimage-20211103191625002\n\n\n\n\n\n\nNumbers - In text processing, numbers are converted into a token.\nN-grams - Multi-word patterns that are particularly discriminative can be identified and their connection kept intact.\nName entity recognition (NER) - The name entity recognition algorithm analyzes the individual tokens and their surrounding semantics while referring to its dictionary to tag an object class to the token.\n\n\n\n\nimage-20211103191816781\n\n\n\nParts of speech (POS) - Similar to NER, parts of speech uses languages structure and dictionaries to tag every token in the text with a corresponding part of speech. POS tags can be useful for separating verbs and nouns for text analytics.\n\n\n\n\n\n\n\n\n\n\n\nimage-20211103191936444\n\n\n\n\n\nSupervised or unsupervised learning - For supervised learning typical methods of choice are regression, ensemble trees, support vector machines and neural networks. Supervised learning would be used, for example, for default prediction based on high-yield corporate bond issuer data. For Unsupervised learning, common methods are dimensionality reduction, clustering, and anomaly detection. Unsupervised learning, for example, would be used for clustering financial institutions into different groups based on their financial attributes.\nType of data - For numerical data classification and regression tree (CART) methods may be suitable. For text data, such methods as generalized linear models (GLMs) and SVMs are commonly used. For image data, NNs and deep learning methods tend to perform better than others. For speech data, deep learning methods can offer promising results.\nSize of data - A typical dataset has two basic characteristics: number of instances and number of features. SVMs have been found to work well on ‘wider’ datasets with 10000 to 100000 features and with fewer instances. Conversely, NNs often work better on ‘longer’ datasets, where the number of instances is much larger than the number of features.\n\nOnce a method is selected, certain method-related decisions need to be made. These decisions include the number of hidden layers in a neural network and the number of trees in ensemble methods (discussed later in the sub-section on tuning).\nBefore model training begins, in the case of supervised learning the master dataset is split into three subsets used for model training and testing purposes. The first subset, a training set used to train the model, should constitute approximately 60% of the master dataset. The second subset, a cross-validation set (or validation set) used to tune and validate the model, should constitute approximately 20% of the master dataset. The third subset is a test set for testing the model and uses the remaining data. A commonly recommended split ratio is 60:20:20.\nClass imbalance,where the number of instances for a particular class is significantly larger than for other classes, may be a problem for data used in supervised learning because the ML classification method’s objective is to training a high-accuracy model.\n\n\n\nimage-20211103203955344\n\n\n\n\n\nimage-20211103193126872\n\n\n\n\n\n\nError analysis - confusion matrix\n\n\n\nimage-20211103193148867\n\n\n\n\\[\n\\\\\n\\text{precision P} = TP / (TP + FP) \\\\\n\\text{recall R} = TP / (TP + FN) \\\\\n\\text{accuracy} = (TP + TN) / (TP + FP + TN + FN) \\\\\n\\text{F1 score} = (2 * P * R) / (P + R)\n\\]\n\n\n\nimage-20211103202917731\n\n\n\n\n\nimage-20211103202924348\n\n\n\nReceiver operating characteristics (ROC)\n\n\\[\n\\\\\n\\text{False positive rate FFR} = FP / (FP + TN) \\\\\n\\text{True positive rate TPR} = TP / (TP + FN)\n\\]\nArea under the curve (AUC) is the metric that measures the area under the ROC curve.\n\n\n\nimage-20211103193417230\n\n\n\nRoot mean squared error (RMSE)\n\n\\[\n\\\\\nRMSE = \\sqrt{\\sum^{n}_{t = 1}{\\frac{(\\text{Predicted}_{i} - \\text{Actual}_{i})^{2}}{n}}}\n\\]\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nimage-20211103193856273\n\n\n\n\n\nimage-20211103203627677\n\n\n\n\n\nimage-20211103203635992\n\n\n\n\n\n\\[\n\\\\\nTF = \\text{Word count in sentence} / \\text{Total words in sentence}\n\\]\n\n\n\n\\[\n\\\\\nDF = \\text{Sentence Count with word} / \\text{Total number of sentences}\n\\]\n\n\n\n\\[\n\\\\\nIDF = log(1 / DF)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf scenario analysis and decision trees\n\n\n\nDetermine “probabilistic” variables\nDefine probabilistic distributions for these variables\n\nhistorical data\ncross sectional data\nstatistical distribution and parameters\n\nCheck for correlation across variables\nRun the simulation\n\n\n\n\nimage-20211103194248762\n\n\n\n\n\n\nGarbage in, garbage out\nReal data may not fit distributions\nNon-stationary distribution\nChanging correlation across inputs\n\n\n\n\n\n\n\n\nSelective versus full risk analysis - In the best-case/worst-case scenario analysis, we look at only three scenarios (the best case, the most likely case, and the worst case) and ignore all other scenarios. Even when we consider multiple scenarios, we will not have a complete assessment of all possible outcomes from risk investments or assets.\nType of risk\nCorrelation across risks - If the various risks that an investment is exposed to are correlated, simulations allow for explicitly modeling these correlations (assuming that you can estimate and forecast them).\n\n\n\n\nBoth decision trees and simulations are approaches that can be used as either complements to or substitutes for risk-adjusted value. Scenario analysis, on the other hand, will always be a complement to risk-adjusted value, since it does not look at the full spectrum of possible outcomes."
  },
  {
    "objectID": "posts/2021-12-10-probability-and-statistics-final/2021-12-10-probability-and-statistics-final.html",
    "href": "posts/2021-12-10-probability-and-statistics-final/2021-12-10-probability-and-statistics-final.html",
    "title": "Probability and Statistics final",
    "section": "",
    "text": "20170361_probability_and_statistics_final_4\n\n\n\n\n\n20170361_probability_and_statistics_final_5\n\n\n\n\n\n20170361_probability_and_statistics_final_6\n\n\n\n\n\n20170361_probability_and_statistics_final_1\n\n\n\n\n\n20170361_probability_and_statistics_final_2\n\n\n\n\n\n20170361_probability_and_statistics_final_3\n\n\n\n\n\n\n\n\nprob_stats_final_2\n\n\n\n\n\nprob_stats_final_3\n\n\n\n\n\nprob_stats_final_4\n\n\n\n\n\nprob_stats_final_5\n\n\n\n\n\nprob_stats_final_1"
  },
  {
    "objectID": "posts/2021-09-30-probability-and-inferential-statistics-week-5/2021-09-30-probability-and-inferential-statistics-week-5.html",
    "href": "posts/2021-09-30-probability-and-inferential-statistics-week-5/2021-09-30-probability-and-inferential-statistics-week-5.html",
    "title": "Probability and Inferenctial Statistics Week 5",
    "section": "",
    "text": "image-20210930163326777\n\n\n\n\n\nimage-20210930164013975\n\n\n\n\n\nimage-20210930164307433\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_3\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_5\n\n\n\n\n\nprobability_and_inferential_statistics_week_5_Page_1\n\n\n\n\n\nimage-20210930165036983"
  },
  {
    "objectID": "posts/2021-10-08-algorithms-week-6/2021-10-08-algorithms-week-6.html",
    "href": "posts/2021-10-08-algorithms-week-6/2021-10-08-algorithms-week-6.html",
    "title": "Algorithms Week 6",
    "section": "",
    "text": "direct-address tables, hash tables, hash functions\n\n\n\n\n\n\nimage-20211008190311416\n\n\n\n\n\nimage-20211008190720235\n\n\n\n\n\nimage-20211008191044379\n\n\n\n\n\nHash tables are another sort of data structure that allows fast INSERT/DELETE/SEARCH.\nHash families are the magic behind hash tables.\nUniversal hash families are even more magic.\nHash tables\n\n\\(O(1)\\) expected time INSERT/DELETE/SEARCH\nWorst worst-case performance, but often great in practice.\n\n\n\n\nimage-20211008191351776\n\n\n\n\n\n\n\n\nimage-20211008191419737\n\n\n\n\n\n\n\n\nimage-20211008191551590\n\n\nassumptions\n\nno duplications (중복이 없음)\n\n\n\n\nimage-20211008191522329\n\n\n\n\n\nimage-20211008191537816\n\n\n\n\n\nimage-20211008191605461\n\n\n\n\n\nimage-20211008191631264\n\n\n\n\n\nimage-20211008191709672\n\n\n\n\n\nimage-20211008191741399\n\n\n\n\n\n\n\n\nimage-20211008191844584\n\n\nBut we don’t know which ones will show up in advance.\n\n\n\nimage-20211008191902750\n\n\n\n\n\nimage-20211008191933786\n\n\n\n\n\nimage-20211008192047386\n\n\n\n\n\nimage-20211008192109631\n\n\n\n\n\nimage-20211008192406610\n\n\n\n\n\n\n\n\nimage-20211008192500232\n\n\n\n\n\nimage-20211008192545882\n\n\n\n\n\n\n\n\nimage-20211008192639447\n\n\nSolution: Randoness\n\n\n\nimage-20211008192736877\n\n\n\n\n\nimage-20211008193001984\n\n\n\n\n\nimage-20211008193010091\n\n\n\n\n\nimage-20211008193032714\n\n\n\n\n\n\n\n\nimage-20211008193056498\n\n\nmod: 나머지 연산\nn은 소수\n\n\n\nimage-20211008193205471\n\n\n\n\n\nimage-20211008193212473\n\n\n\n\n\nimage-20211008193242646\n\n\n\n\n\n\n\n\nimage-20211008193322293\n\n\n\n\n\n\n\n\nimage-20211008193546011\n\n\n\n\n\nTree basics, Binary search tree, red-black tree\n\n\n\n\n\n\nimage-20211008204015319\n\n\n\n\n\nimage-20211008204019470\n\n\n\n\n\nimage-20211008204029295\n\n\n\n\n\nimage-20211008204052191\n\n\n\n\n\nimage-20211008204112618\n\n\n\n\n\nimage-20211008204204193\n\n\n\n\n\n\n\n\nimage-20211008204233121\n\n\n\n\n\n\n\n\nimage-20211008204416888\n\n\n\n\n\nimage-20211008204523232\n\n\nconnected: 어떤 node에서 출발해도 그 어떤 node에 도달 할 수 있다.\nacyclic: 자기 자신으로 돌아오는 cycle이 없다.\n\n\n\nimage-20211008204804192\n\n\nforest: tree의 집합\n\n\n\nimage-20211008204845935\n\n\n\n\n\nchild를 최대 2개까지 가질 수 있음\n\n\n\nimage-20211008205456362\n\n\n\n\n\nimage-20211008205549192\n\n\n\n\n\n\n\n\nimage-20211008205617534\n\n\nnode 선택 -> node의 key 값에 따라 좌우 분류 -> 분류된 집합에서 node 선택 -> 좌우에 붙이고 반복 (recursion)\n\n\n\nimage-20211008205908586\n\n\n이 binary search tree는 unique하지는 않다.\n\n\n\nimage-20211008205937221\n\n\n\n\n\nimage-20211008205957919\n\n\n\n\n\n\n\n\nimage-20211008210404980\n\n\n\n\n\n\n\n\nimage-20211008210603171\n\n\nx.key == key의 경우엔 binary search tree내의 값들이 다 unique하다고 가정했기 때문에 그냥 넘어가는데, 구현의 문제로 남겨짐.\n\n\n\n\n\n\nimage-20211008210718619\n\n\n이 case에서는 그냥 2를 없애고 2의 자리에 1을 붙여주면 되지만…\n\n\nif 3 is a leaf -> just delete it\n\n\n\nif 3 has just one child\n\n\n\nimage-20211008210826746\n\n\n\n\n\nif 3 has two children, replace 3 with it’s immediate successor, aka next smallest thing after 3\n3의 right child 중에서 가장 left에 있는 descendant를 불러와서 3의 자리에 옮김\n\n\n\nimage-20211008210921690\n\n\n\nDoes this maintain the BST property? yes\nHow do we find the immediate successor? SEARCH for 3 in the subtree under 3.right\nHow do we remove it when we find it? if 3.1 has 0 or 1 children, do one of the previous cases.\nWhat if [3.1] has two children? It doesn’t. 3.right의 subtree 중에 가장 좌측에 있는 node이기 때문에 자신보다 작은 left child가 존재할 수 없음. 즉, 2개의 children을 가질 가능성이 없음\n\n\n\n\nimage-20211008211203361\n\n\n\n\n\nimage-20211008211311943\n\n\n\n\n\nimage-20211008211730429\n\n\n\n\n\nimage-20211008212344180\n\n\n\n\n\n\n\n\n\nimage-20211008212435141\n\n\n\n\n\nimage-20211008212519980\n\n\n\n\n\nimage-20211008212611534\n\n\n\n\n\nimage-20211008212721228\n\n\n\n\n\n\n\n\n\n\n\n\n주어진 길이 n의 정수 배열에서, K번째로 많이 등장한 원소를 반환하는 알고리즘을 작성하세요.\n\n\n\n\n(입력1) 10 5 1 1 2 2 3 3 4 4 4 5\n(출력1) 5\n4 - 3개\n1 - 2개\n2 - 2개\n3 - 2개\n5 - 1개 \n5번째로 많은 수 == 5\n(입력2) 10 2 1 1 2 2 3 3 4 4 4 5\n(출력2) 1\n4 - 3개\n1 - 2개\n2 - 2개\n3 - 2개\n5 - 1개 \n2번째로 많은 수 == 1 or 2 or 3\n\n첫째줄에, 배열의 길이 len이 입력됩니다.\n둘째줄에, K가 입력됩니다.\n셋째줄에, 배열의 원소들이 입력됩니다.\n제공한 sample1.txt 파일과, main.cpp 파일을 참고하세요.\n\n\n\n\n\n0 < len <= 10,000\n0 < K <= the number of unique elements in arr\n0 < each element <= 10,000\n정답이 다수인 경우, 그 중 하나만 출력해도 정답으로 인정 (위 입력2의 경우 1,2,3 모두 정답으로 인정함)\n자동화된 채점 시스템을 활용할 계획이므로 제공한 템플릿을 활용하되, 기본 제공한 함수 프로토타입 등은 변경하지 말 것.\n구현은 모두 solution.cpp 파일에 하고, 제출도 solution.cpp 파일만 제출.\n(main.cpp/ solution.h 파일은 테스트 용으로만 활용)\n시간제한: 3sec @ 3.0Ghz single core\n공간제한: 없음\n체점환경은 c++17 문법을 사용함.\n\n\n\n\n\n소스코드 (solution.cpp) 와 보고서 (학번.txt)를 압축 (학번.zip)하여 제출\nmain.cpp, soultion.h, sample1.txt 등 기타 파일들은 포함하지 말 것.\n\n\n\n\n\n4.5점 Correctness & Efficiency\n\n총 9개의 테스트 케이스에서 항상 정답을 찾아내는지 여부와 시간 제한을 만족하는지 여부를 검증.\n각 테스트케이스를 여러번 실행하여 항상 일관된 정답을 출력하는지 여부를 검증하고, 실행 시간은 평균값을 취하여 검증함.\n주의 C/C++ 에서 기본제공하는 sorting/selection 라이브러리 사용시 40% 감점 (e.g., nth_element(), sort() 등)\n\n0.5점 보고서\n\n위 문제 해결을 위한 본인의 알고리즘 설계 방법 제시 (소스코드 설명하지 않아도 됨).\n본인이 설계한 알고리즘의 correctness & efficiency 의견 제시.\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"p2_solution.h\"\n\n\nvoid merge(std::vector<int> &arr, int l, int m, int r) {\n    int i, j, k;\n    int n1 = m - l + 1;\n    int n2 = r - m;\n\n    std::vector<int> L(n1, 0), R(n2, 0);\n    i = 0;\n    for (i; i < n1; ++i) L[i] = arr[l + i];\n    j = 0;\n    for (j; j < n2; ++j) R[j] = arr[m + 1 + j];\n\n    i = 0;\n    j = 0;\n    k = 1;\n\n    while (i < n1 && j < n2) {\n        if (L[i] <= R[j]) {\n            arr[k] = L[i];\n            ++i;\n        }\n        else {\n            arr[k] = R[j];\n            ++j;\n        }\n        ++k;\n    }\n\n    while (i < n1) {\n        arr[k] = L[i];\n        ++i;\n        ++k;\n    }\n\n    while (j < n2) {\n        arr[k] = R[i];\n        ++i;\n        ++k;\n    }\n}\n\nvoid mergeSort(std::vector<int> &arr, int l, int r) {\n    if (l < r) {\n        int m = (l + r) / 2;\n        mergeSort(arr, l, m);\n        mergeSort(arr, m + 1, r);\n\n        merge(arr, l, m, r);\n    }\n}\n\nint Solution::kMost(std::vector<int> &arr, int len, int k)\n{\n    int i;\n    std::vector<int> count_arr(len, 0);\n    \n    i = 0;\n    for (i; i < len; ++i) {\n        int n = arr[i];\n        count_arr[n] += 1;\n    }\n\n    std::vector<int> rank_arr(k, 0);\n    std::vector<int> index_arr(k, 0);\n\n    i = 1;\n    for (i; i < len; ++i) {\n        int res = count_arr[i];\n        if (res > 0) {\n            int j = 0;\n            for (j; j < k; ++j) {\n                if (rank_arr[j] <= res) {\n                    int n = k - 1;\n                    for (n; n > j; --n) {\n                        rank_arr[n] = rank_arr[n - 1];\n                        index_arr[n] = index_arr[n - 1];\n                    }\n                    rank_arr[j] = res;\n                    index_arr[j] = i;\n                    break;\n                }\n            }\n        }\n    }\n    \n    return index_arr[k - 1];\n}"
  },
  {
    "objectID": "posts/2021-11-30-digital-system-circuits-week-12/2021-11-30-digital-system-circuits-week-12.html",
    "href": "posts/2021-11-30-digital-system-circuits-week-12/2021-11-30-digital-system-circuits-week-12.html",
    "title": "Digital System Circuits Week 12",
    "section": "",
    "text": "image-20211130032813685\n\n\n\n\n\n\n\n\nimage-20211130071228205\n\n\n\n\n\n\n\n\nimage-20211130071813523\n\n\n\n\n\n\n\n\nimage-20211130071824879\n\n\n\n\n\n\n\n\nimage-20211130074941030\n\n\n\n\n\nimage-20211130074947328\n\n\n\n\n\nimage-20211130074954354\n\n\n\n\n\n\n\n\nimage-20211130075008013\n\n\n\n\n\n\n\n\nimage-20211130080203712\n\n\n\n\n\nimage-20211130080219103\n\n\n\n\n\n\n\n\nimage-20211130080234705\n\n\n\n\n\n\n\n\nimage-20211130080250796\n\n\n\n\n\n\n\n\nimage-20211130080305673\n\n\n\n\n\n시험 출제됨\n\n\n\nimage-20211130080336033\n\n\n\n\n\nimage-20211130080343011\n\n\n\n\n\nimage-20211130080349294\n\n\n\n\n\n\n\n\nimage-20211130080446783\n\n\n\n\n\n\n\n\nimage-20211130080532056\n\n\n\n\n\n\n\n\nimage-20211130081007055\n\n\n\n\n\nimage-20211130081015881\n\n\n\n\n\n\n\n\nimage-20211130081024202\n\n\n\n\n\n\n\n\nimage-20211130081036684\n\n\n\n\n\nimage-20211130081043087\n\n\n\n\n\nimage-20211130081048198\n\n\n\n\n\nimage-20211130081055488\n\n\n\n\n\nimage-20211130081100639\n\n\n\n\n\nimage-20211130081107078\n\n\n\n\n\nimage-20211130081114203"
  },
  {
    "objectID": "posts/2022-03-23-leadership-by-persuasion.html",
    "href": "posts/2022-03-23-leadership-by-persuasion.html",
    "title": "Leadership by Persuation",
    "section": "",
    "text": "Why aren’t people already doing this behavior? One possible answer is they don’t think it is a good idea.\n\n\nConsequences, outcomes, are a natural source of good reasons for doing something. Persuade people by giving them consequences of their behaviors.\n\n\n\nDon’t too easily assume you know what consequences will be most persuasive to your audience. The argument that seems best to you, the argument that seems most persuative to you.\n\n\n\nPeople may have doubts about whether the outcomes you suggested will actually occur. Describe the underlying mechanism that is explained to people how the advocated action comes to produce the claimed consequences.\n\n\n\nMake your own positive arguments. If in addition to advancing those supportive arguments, you also undertake refutation of opposing arguments, you will be more persuasive. The best strategy is to try to overwhelm those opposing considerations by invoking all of your supportive arguments, all your positive points."
  },
  {
    "objectID": "posts/2022-03-23-leadership-by-persuasion.html#social-considerations-are-a-barrier",
    "href": "posts/2022-03-23-leadership-by-persuasion.html#social-considerations-are-a-barrier",
    "title": "Leadership by Persuation",
    "section": "Social considerations are a barrier",
    "text": "Social considerations are a barrier\n\n1. Changing descriptive norms\nThe descriptive norm is the person’s perception of whether other people perform the behavior. People can be influenced by social considerations, factors connected with people’ perceptions of what other people think about what you’re doing. Persuade people to do by conveying other people have done it.\n\n\n2. De-emphasizing prescriptive norm\nPrescriptive norm perceptions are people’s sense of what other people think they should do that’s preventing them from going forward. Suggest that in making their decision, they should place more weight on their own attitudes than on what other people think. De-emphasize people’s prescriptive norm to persuade them to do something.\n\n\n3. Changing prescriptive norms\nEmphasize people’s own attitude more than what other people think they should do. Also, find someone who has a great influence on people you want to persuade, and make them persuaded."
  },
  {
    "objectID": "posts/2022-03-23-leadership-by-persuasion.html#peoples-perceived-ability-to-perform-the-behavior",
    "href": "posts/2022-03-23-leadership-by-persuasion.html#peoples-perceived-ability-to-perform-the-behavior",
    "title": "Leadership by Persuation",
    "section": "People’s perceived ability to perform the behavior",
    "text": "People’s perceived ability to perform the behavior\nSometimes people have the appropriate positive attitudes and the social considerations about what other people think or do are all positive. But people don’t think they can perform the behavior. Maybe they don’t know how to perform the behavior. And so, they don’t even try.\n\nThe importance of perceived ability\nOne barrier to people doing what you want can be their perceived ability to perform the behavior. If I want to make them to act, I should make it easier than what they think.\n\n\nRemoving obstacles\nThere are two kinds of obstacles that people might face and so two kinds that you might remove. 1. Sometimes people just lack information that’s relevant to behavioral performance. 2. The second kind of obstacle is substantice, material, not just a lack of information, but something beyond that.\nThere are three abstract strategies, three general ways of potentially influencing perceived behavioral ability.\n\nThe first strategy is directly removing obstacles to performing the behavior\n\n\n\nRehearsal and modeling\nThe second and third strategies are rehearsal and modeling. Provide opportunities to people for rehearsal or practice of the behavior. Show them someone who already successfully perform the behavior before."
  },
  {
    "objectID": "posts/2022-03-23-leadership-by-persuasion.html#addressing-peoples-ability-to-perform-the-behavior",
    "href": "posts/2022-03-23-leadership-by-persuasion.html#addressing-peoples-ability-to-perform-the-behavior",
    "title": "Leadership by Persuation",
    "section": "Addressing people’s ability to perform the behavior",
    "text": "Addressing people’s ability to perform the behavior\nSometimes, a persuader faces the task of helping people convert their intentions to behavior.\n\n1. Prompt\nA prompt is a simple cue that makes performance of the behavior salient, brings it into conscious awareness. However, prompt won’t always work. There are right kinds of situations they can be very useful. At least two necessary conditions for a prompt to be successful: 1. The preson must already be willing to do the behavior. 2. Perceived behavioral ability must be sufficiently high.\n\n\n2. Explicit planning\nSpecify when and where and how people are doing to do the behavior. Specific concrete intention is much more effective than abstract general intention.\nEncouraging explicit planning interventions to work, there are at least two conditions to be met: 1. People must already have the appropriate abstract intention. 2. Perceived behavioral ability must be sufficiently high.\n\n\n3. Inducing guilt\nMake people to feel bad about their inconsistency. If people are going to experience these negative feelings, guilt, hypocrisy, dissonance, two things have to be salient to them at the same time. 1. Their existing positive attitudes and intentions on the one hand. 2. Their inconsistent behaviors on the other.\nFeeling uncomfortable is the motivation for people to change their behavior.\nHowever, this is a dangerous strategy. It can produce negative reactions. 1. People often react negatively to overt attempts to make them feel guilt, and that can make them resistant to change. 2. People’s perceived behavioral ability is not sufficiently high."
  },
  {
    "objectID": "posts/2022-03-23-leadership-by-persuasion.html#summary",
    "href": "posts/2022-03-23-leadership-by-persuasion.html#summary",
    "title": "Leadership by Persuation",
    "section": "Summary",
    "text": "Summary\nWhy don’t people already doing what I want? 1. Because they don’t think it is a good idea. - attitude 2. Because they are influenced by their environments - social factors 3. Because they don’t think they are able to do that - perceived ability 4. Because they have not a specific plan, but a vague intention - intentions without action"
  },
  {
    "objectID": "posts/2021-11-17-system-programming-week-11/2021-11-17-system-programming-week-11.html",
    "href": "posts/2021-11-17-system-programming-week-11/2021-11-17-system-programming-week-11.html",
    "title": "System Programming Week 11",
    "section": "",
    "text": "image-20211117132356697\n\n\n\n\n\n\n\n\nimage-20211117132408746\n\n\nobject file은 machine level까지 complie되었지만, 아직 executable file은 아닌 상황\n\n\n\n\n\n\nimage-20211117132521952\n\n\n\n\n\nimage-20211117132527826\n\n\n\n\n\n\n\n\nimage-20211117132540562\n\n\n각종 symbol들의 위치를 결정하는 과정을 symbol resolution이라고 함\nlinker가 symbol resolution을 할 때, 각 symbol은 딱 하나만 있어야 함 -> 여러개가 있는 경우 error\n\n\n\nimage-20211117132548432\n\n\nexecutable file을 만들 때, 각 symbol의 위치는 virtual address\ncode는 code끼리, data는 data끼리 모아서 merge\n\n\n\n\n\n\nimage-20211117132604418\n\n\nmachine language로 바뀌면 일단 전부 object file\nShared object file - dynamic하게 linking되고, 실행될 때 memory로 load됨\n\n\n\n\n\n\nimage-20211117132620767\n\n\n하나의 format을 가지고 여러개의 object file을 모두 지원해야 함\n\n\n\n\n\n\nimage-20211117132636156\n\n\n\n\n\nimage-20211117132648282\n\n\n\n\n\n\n\n\nimage-20211117132700684\n\n\nlocal symbol로 사용하고자 한다면 이름이 겹치는 symbol 앞에 static이라고 붙이면 된다. 그러면 exclusive하게 해당 module 내에서만 사용된다. 외부에서 사용이 불가능.\nlocal linker symbol은 local program variable과는 다름\n\n\n\n\n\n\nimage-20211117140234769\n\n\n\n\n\n\n\n\nimage-20211117140244731\n\n\n\n\n\n\n\n\nimage-20211117140300313\n\n\n\n\n\n\n\n\nimage-20211117140311113\n\n\n\n\n\n\n\n\nimage-20211117140320861\n\n\n\n\n\n\n\n\nimage-20211117140924815\n\n\n\n\n\nimage-20211117140933277\n\n\n\n\n\nimage-20211117140940976\n\n\n\n\n\nimage-20211117140953639\n\n\n\n\n\nimage-20211117141046856\n\n\n\n\n\n\n\n\nimage-20211117141057132\n\n\n\n\n\n\n\n\nimage-20211117141107247\n\n\n\n\n\n\n\n\nimage-20211117142256520\n\n\n\n\n\n\n\n\nimage-20211117142311040\n\n\n\n\n\n\n\n\nimage-20211117142445376\n\n\n\n\n\n\n\n\nimage-20211117142931293\n\n\n\n\n\n\n\n\nimage-20211117143311349\n\n\n\n\n\n\n\n\nimage-20211117143322683\n\n\n\n\n\n\n\n\nimage-20211117143333799\n\n\n\n\n\n\n\n\nimage-20211117143349136\n\n\n\n\n\nimage-20211117143750075\n\n\n\n\n\n\n\n\nimage-20211117143800698\n\n\n\n\n\nimage-20211117143831338\n\n\n\n\n\n\n\n\nimage-20211117144259279\n\n\n\n\n\nimage-20211117144309121\n\n\n\n\n\n\n\n\nimage-20211117144323427\n\n\n\n\n\n\n\n\nimage-20211117144732217\n\n\n\n\n\nimage-20211117144804414\n\n\n\n\n\nimage-20211117144814265\n\n\n\n\n\n\n\n\nimage-20211117144910902\n\n\n\n\n\n\n\n\nimage-20211117145052736\n\n\n\n\n\n\n\n\nimage-20211117145104441\n\n\n\n\n\n\n\n\nimage-20211117145432982\n\n\n\n\n\nimage-20211117145517364\n\n\n\n\n\n\n\n\nimage-20211117145442495\n\n\n\n\n\n\n\n\nimage-20211117145457838"
  },
  {
    "objectID": "posts/2021-10-14-algorithms-week-7/2021-10-14-algorithms-week-7.html",
    "href": "posts/2021-10-14-algorithms-week-7/2021-10-14-algorithms-week-7.html",
    "title": "Algorithms Week 7",
    "section": "",
    "text": "image-20211013235723717\n\n\n\n\n\nimage-20211013235805633\n\n\nTurns out that’s not a great idea. Instead we turn to…\n\n\n\n\n\nimage-20211013235957906\n\n\n\n\n\nimage-20211014000228750\n\n\nDoes this work?\nWhenever something seems unbalanced, do rotations until it’s okay gain. -> Even for me this is pretty vague. What do we mean by “seems unbalanced”? What’s “okay”?\n\n\n\n\n\n\nimage-20211014000513219\n\n\n\n\n\nimage-20211014000525049\n\n\n\n\n\n\n\n\n\nimage-20211014000607957\n\n\n\n\n\nimage-20211014000937523\n\n\nThis is pretty balanced.\n\nThe black nodes are balanced.\nThe red nodes are “spread out” so they don’t mess things up too much.\n\nWe can maintain this property as we insert/delete nodes, by using rotations.\nThe Red-Black structure is a proxy for balance. It’s just a little weaker than perfect balance, but we can actually maintain it.\n\n\n\nimage-20211014001326840\n\n\n\n\n\nimage-20211014001404332\n\n\nOkay, so it’s balanced, but can we maintain it? -> Yes\n\n\n\nimage-20211014001805278\n\n\n\n\n\nimage-20211014001830521\n\n\n\n\n\nimage-20211014001901422\n\n\n\n\n\nimage-20211014001923388\n\n\n\n\n\nimage-20211014002046260\n\n\n일단 red로 insert하고, grandparent와 parent의 색을 바꿈\n\n\n\nimage-20211014002127476\n\n\n\n\n\nimage-20211014002150066\n\n\nroot까지 recursive하게 색을 바꿔주면서 올라가면 됨\n\n\n\nimage-20211014002328862\n\n\n\n\n\nimage-20211014002445454\n\n\n\n\n\nimage-20211014002529027\n\n\n\n\n\nimage-20211014003022520\n\n\n딱 하나의 case만 고치는 예제 코드\n\n\n\n\n\nimage-20211014003555547\n\n\n\n\n\nimage-20211014003626363\n\n\n\n\n\nimage-20211014003718968\n\n\n\n\n\nimage-20211014003723654\n\n\n\n\n\nimage-20211014003749633\n\n\n\n\n\nimage-20211014003913792"
  },
  {
    "objectID": "posts/2021-12-07-digital-system-circuits-week-14/2021-12-07-digital-system-circuits-week-14.html",
    "href": "posts/2021-12-07-digital-system-circuits-week-14/2021-12-07-digital-system-circuits-week-14.html",
    "title": "Digital System Circuits Week 14",
    "section": "",
    "text": "image-20211207051327335\n\n\n\n\n\n\n\n\nimage-20211207051152693\n\n\n\n\n\n\n\n\nimage-20211207051554631\n\n\n\n\n\nimage-20211207051740005\n\n\n\n\n\n\n\n\nimage-20211207051757544\n\n\n\n\n\n\n\n\nimage-20211207052240660\n\n\n\n\n\n\n\n\nimage-20211207052536841\n\n\n\n\n\n\n\n\nimage-20211207052804575\n\n\n\n\n\n\n\n\nimage-20211207053140669\n\n\n\n\n\nimage-20211207053213060\n\n\n\n\n\nimage-20211207053314134\n\n\n\n\n\nimage-20211207053340470\n\n\n\n\n\n\n\n\nimage-20211207053508556\n\n\n\n\n\n\n\n\nimage-20211207053526117\n\n\n\n\n\nimage-20211207053612644\n\n\n\n\n\n\n\n\nimage-20211207062234193\n\n\n\n\n\n\n\n\nimage-20211207062601075\n\n\n\n\n\nimage-20211207063009141\n\n\n\n\n\nimage-20211207063016384\n\n\n\n\n\nimage-20211207063024573"
  },
  {
    "objectID": "posts/2021-12-04-system-programming-week-11/2021-12-04-system-programming-week-11.html",
    "href": "posts/2021-12-04-system-programming-week-11/2021-12-04-system-programming-week-11.html",
    "title": "System Programming Week 11",
    "section": "",
    "text": "여러 소스파일에 코드를 작성해서 프로그램 하나를 작성하게 되는데, 여러 파일을 합쳐주는 역할\n\n\n\n\n\n\nimage-20211204192625609\n\n\n\n\n\n\n\n\nimage-20211204192708349\n\n\n별도의 object file로 일단 컴파일, linker로 연결해서 하나의 executable file로 만듦\ngcc는 compiler, optimizer, linker가 다 합쳐진 무언가라고 생각하면 됨\nobject file은 machine level language지만, executable하지는 않음\n\n\n\n\n\n\nimage-20211204192851729\n\n\n\n\n\nimage-20211204193044469\n\n\n\n\n\n\n\n\nimage-20211204193225173\n\n\n\n\n\nimage-20211204193234653\n\n\nmemory <-> storage가 있음\nstorage에 작성한 source code들이 있고, 이를 활용해서 executable file을 만들어서 실행하면 해당 코드가 memory로 올라간다.\n실행파일은 어디서 뭘 해야하는지 address가 다 적혀있다. - symbol이 아니라 address로 function을 호출\n그런데 이 address가 virtual address임\n\n예전에는 프로그램이 1개만 실행됨\n그런데 최근에는 많은 프로그램이 동시에 실행되는데, virtual address space 덕분임\n\n언제 어떤 메모리에 올라갈지 모르겠지만, address를 virtual address space에 입력해둠\n\n그래서 executable file은 virtual address를 기준으로 작성됨\n\n\n\n\n\n\n\nimage-20211204194156251\n\n\n\n\n\n\n\n\nimage-20211204194405029\n\n\n\n\n\n\n\n\nimage-20211204194422403\n\n\n\n\n\nimage-20211204194644393\n\n\n\n\n\n\n\n\nimage-20211204194901990\n\n\n\n\n\n\n\n\nimage-20211204195203815\n\n\n\n\n\n\n\n\nimage-20211204195340780\n\n\n\n\n\n\n\n\nimage-20211204195523378\n\n\n\n\n\n\n\n\nimage-20211204195621051\n\n\nstrong symbol 2개 선언 -> compile error (multiple definition)\n\n\n\nimage-20211204195629237\n\n\nstrong symbol 2개 선언 -> compile error (multiple definition)\n\n\n\nimage-20211204195638979\n\n\nweak type global variable < strong type global variable: symbol resolution은 strong type에게 일어난다.\n\n\n\nimage-20211204195648876\n\n\n둘 다 weak면, random으로 뭘 건드리게 될지 모른다.\n\n\n\nimage-20211204195658726\n\n\ndouble은 8bytes, int는 4bytes인데, foo5.c에서는 둘 다 strong으로 선언되어 있기 때문에 bar5.c에서 weak로 선언된 x에 -0.0을 넣으면 strong x에 접근하게 되서 다른 메모리 공간을 침범하게 됨\n\n\n\n\n\n\nimage-20211204200217464\n\n\n\n\n\n\n\n\nimage-20211204200251181\n\n\n\n\n\n\n\n\nimage-20211204200345974\n\n\n\n\n\n\n\n\nimage-20211204200353973\n\n\n\n\n\n\n\n\nimage-20211204200409981\n\n\n\n\n\n\n\n\nimage-20211204200536909\n\n\n\n\n\n\n\n\nimage-20211204200835179\n\n\n\n\n\n\n\n\nimage-20211204200928964\n\n\n\n\n\n\n\n\nimage-20211205025733688\n\n\nlinking time에 합쳐져서 executable file에 함께 들어가는 object flies\n\n\n\n\n\n\nimage-20211205025958072\n\n\n\n\n\n\n\n\nimage-20211205030055092\n\n\n\n\n\n\n\n\nimage-20211205030144119\n\n\n\n\n\nimage-20211205030156445\n\n\n\n\n\n\n\n\nimage-20211205030243106\n\n\n\n\n\nimage-20211205030257834\n\n\n-L. <- 현재 directory에 libfun이 있는지 찾도록\n-lmine <- archive에서 찾도록\n순서만 바꿨더니 undefined reference error가 뜸 - linker는 command line order로 scan을 하게 됨\n그래서 -lmine에서 먼저 resolution을 검사하고 libtest.o를 검사하게되니 undefined reference error가 발생하게 된 것\n\n\n\n\n\n\nimage-20211205030716879\n\n\ndynamic link library - linking time에 library를 합치는 것이 아니라 load time에 dynamic하게 연동되는 library\n\n\n\nimage-20211205030854708\n\n\n\n\n\n\n\n\nimage-20211205031059780\n\n\n\n\n\n\n\n\nimage-20211205031115640\n\n\n\n\n\nimage-20211205031220986\n\n\n\n\n\nimage-20211205031230567\n\n\n\n\n\n\n\n\nimage-20211205031323720"
  },
  {
    "objectID": "posts/2021-09-30-algorithms-week-5/2021-09-30-algorithms-week-5.html",
    "href": "posts/2021-09-30-algorithms-week-5/2021-09-30-algorithms-week-5.html",
    "title": "Algorithms Week 5",
    "section": "",
    "text": "Comparison-based sorting에 성능적인 한계가 있음\n이러한 단점을 해결하기 위한 linear-time sorting 기법들이 있음\n\n\n\n\nIt behaves as follows:\n\nIf the list has 0 or 1 elements it’s sorted.\nOtherwise, choose a pivot and partition around it.\nRecursively apply quicksort to the sublists to the left and right of the pivot.\n\n\n\n\nimage-20210930183827208\n\n\nvoid qucikSort (int array[], int l, int r) {\n    if (l < r) {\n    int pivot = array[r];\n    int pos = partition(array, l, r, pivot);\n    \n    quickSort(array, l, pos - 1);\n    quickSort(array, pos + 1, r);\n    }\n}\nWorst-case runtime - \\(O(n^{2})\\): When pivot = array[r]\n\n\nvoid qucikSort (int array[], int l, int r) {\n    if (l < r) {\n    int pivot = array[l + rand() % n];\n    int pos = partition(array, l, r, pivot);\n    \n    quickSort(array, l, pos - 1);\n    quickSort(array, pos + 1, r);\n    }\n}\nWorst-case - \\(O(n^{2})\\): Think of this as the adversary chooses the randomness.\nExpected - \\(O(n\\log{n})\\)\n\n\n\nThere’s a really good case, in which partition always picks the median element as the pivot.\n\n\n\nimage-20210930184713329\n\n\nThere’s a really bad case, in which partition always picks the smallest or largest element as the pivot.\n\n\n\nimage-20210930184746442\n\n\n\n\n\nThe expected runtime of auicksort is \\(O(n\\log{n})\\)\nWe can provie it through counting the number of times two elements get compared.\n\n\n\n\n\n\nWe’ve seen a few sorting algorithms\n\nInsertion sort is worst-case \\(O(n^{2})\\)-time.\nMergesort is worst-case \\(O(n\\log{n})\\)-time.\nQuicksort is worst-case \\(O(n\\log{n})\\)-time.\n\nComparison-based algorithms use “comparisons” to achieve their output.\n\nLinear-time select is a comparison-based algorithms.\nquicksort, mergesort, insertionsort\n\nTheorem [Lower bound of \\(\\ohm(n\\log{n})\\)]: Any deterministic comparison-based sorting algorithm requires \\(\\ohm(n\\log{n})\\)-time?\n\nArgue that all comparison-based sorting algorithms produce a decision tree. Then analyze decision trees.\n\nProof:\n\nAny deterministic comparison-based algorithm can be represented as a decision tree with \\(n!\\) leaves.\nThe worst-case running time is the depth of the decision tree.\nAll decision trees with \\(n!\\) leaves have depth at least \\(\\ohm(n\\log{n})\\)\n\nSo any comparison-based sorting algorithm must have worst-case running time at least \\(\\ohm({n\\log{n}})\\)\n\n\n\n\n\n\n\nimage-20210930185525497\n\n\n\n\n\nimage-20210930185537302\n\n\nThe leaves of this tree are all possible orderings of the items: when we reach a leaf we return it.\nRunning the algorithm on a given input corresponds to taking a particular path through the tree.\n\n\n\nimage-20210930185728867\n\n\n\n\n\nimage-20210930185812594\n\n\n\n\n\nimage-20210930191734418\n\n\n\n\n\nIf any deterministic comparison-based sorting algorithm requires \\(\\ohm(n\\log{n})\\)-time, then what’s this nonsense about linear-time sorting algorithms?\n\nWe can achieve \\(O(n)\\) worst-case runtime if we make assumptions about the input. (e.g. They are integers ranging from 0 to k - 1)\nCounting sort, Bucket sort, Radix sort\n\n\n\nvoid countingsort(int array[], int n) {\n    int count [k + 1];\n    for (int i = 0; i < k + 1; i++) {\n        count[i] = 0;\n    }\n    \n    for (int i = 0, j = 0; i <= k; i++) {\n        while(count[i] > 0) {\n            array[j] = i;\n            j++;\n            count[i]--;\n        }\n    }\n}\nAssumption: \\(\\operatorname{input value rance} = [0, k - 1]\\)\n\n\n\nimage-20210930192504411\n\n\n\n\n\nimage-20210930192658163\n\n\n\n\n\nsimilar to the counting sort, but\nMight be multiple keys per bucket, so buckets need another stable_sort to be sorted.\n\n\n\nimage-20210930192817360\n\n\nstatic void bucketsort(int array[], int n, int k) {\n    vector<int> bucket[bucket_number];\n    \n    for (int i = 0; i < n; i++) {\n        int b = array[i] / ceil(k / bucket_number);\n        bucket[b].push_back(array[i]);\n    }\n    \n    if (bucket_number < k) {\n        for (int i = 0; i < bucket_number; i++) {\n            sort(bucket[i].begin(), bucket[i].end());\n        }\n    }\n    \n    int m = 0;\n    for (int i = 0; i < bucket_number; i++) {\n        for (int j = 0; j < bucket[i].size(); j++) {\n            array[m] = bucket[i][j];\n            m++\n        }\n    }\n}\nworst-case runtime: \\(\\O(max(n\\log{n}, n + k))\\)\nTwo cases for um_buckets and k:\n\n\\(k <= \\operatorname{num\\_buckets}\\): At most one key per bucket, so buckets don’t need another stable_sort to be sorted (similar to counting_sort).\n\\(k > \\operatorname{num\\_buckets}\\): Might be multiple keys per buckets, so buckets need another stable_sort to be sorted.\n\n\n\n\nimage-20210930193654631\n\n\n\n\n\n기수정렬\n\n\n\nimage-20210930193810934\n\n\n자릿수로 정렬\nvoid radixsort(int array[], int size) {\n    int max = getMax(array, size);\n    \n    for (int place = 1; max / place > 0; place *= 10) {\n        countingSort(array, size, place);\n    }\n}\nvoid countingSort(int array[], int size, int place) {\n    count int max = 10;\n    int* output = new int[size];\n    int* count = new int[max];\n    \n    for (int i = 0; i < max; ++i) {\n        count[i] = 0\n    }\n    \n    for (int i = 0; i < size; i++) {\n        key = (array[i] / place) % 10;\n        count[key]++;\n    }\n    \n    for (int i = 1; i < max; i++) {\n        count[i] += count[i - 1];\n    }\n    \n    for (int i = size - 1; i >= 0; i--) {\n        key = (array[i] / place) % 10;\n        output[count[key] - 1] = array[i];\n        count[key]--;\n    }\n    \n    for (int i = 0; i < size; i++) {\n        array[i] = output[i];\n    }\n}"
  },
  {
    "objectID": "posts/2021-10-20-algorithms-week-7/2021-10-20-algorithms-week-7.html",
    "href": "posts/2021-10-20-algorithms-week-7/2021-10-20-algorithms-week-7.html",
    "title": "Algorithms Week 7",
    "section": "",
    "text": "Graph basics\nDFS - depth-first search\nBFS - breadth-first search\nShortest path finding\nmore graphs with advanced graph algorithms\n\n\n\n\n\n\n\nimage-20211021031858005\n\n\n\n\n\n\n\n\nimage-20211021032121783\n\n\n\n\n\nimage-20211021032151472\n\n\n\n\n\n\n\n\nimage-20211021032247498\n\n\n\n\n\nimage-20211021032307328\n\n\n\n\n\n\n\n\n\n\nimage-20211021032414202\n\n\n\n\n\nimage-20211021032502567\n\n\n\n\n\n\n\n\nimage-20211021032543944\n\n\n\n\n\n\n\n\n\nimage-20211021032718757\n\n\n\n\n\n\n\n\nimage-20211021032854670\n\n\nlinked lists - generally better for sparse graphs\n\n\n\n\n\n\n\n\nimage-20211021032942687\n\n\n\n\n\nimage-20211021033056311\n\n\n\n\n\nimage-20211021033116054\n\n\n\n\n\nimage-20211021034158183\n\n\n\n\n\n\n\n\n\nimage-20211021034212281\n\n\n\n\n\nimage-20211021034244808\n\n\n\n\n\n\n\n\nimage-20211021034303896\n\n\n\n\n\n\n\n\nimage-20211021034314871\n\n\n\n\n\nimage-20211021034324626\n\n\n\n\n\nimage-20211021034331060"
  },
  {
    "objectID": "posts/2021-11-16-algorithms-week-11/2021-11-16-algorithms-week-11.html",
    "href": "posts/2021-11-16-algorithms-week-11/2021-11-16-algorithms-week-11.html",
    "title": "Algorithms Week 11",
    "section": "",
    "text": "Bellman-Ford algorithm\nFibonacci Numbers\nFloyd-Warshall algorithm\n\nAnother problems\n\nLongest common subsequence\nknapsack\nIndependent sets\n\n\n\n\n\n\n\nimage-20211116154308897\n\n\n\n\n\nimage-20211116154448448\n\n\n\n\n\n\n\n\nimage-20211116154501152\n\n\n\n\n\n\n\n\nimage-20211116154518038\n\n\n\n\n\nimage-20211116154858372\n\n\n\n\n\nimage-20211116154908832\n\n\n\n\n\nimage-20211116155451181\n\n\n\n\n\nimage-20211116155458680\n\n\n\n\n\nimage-20211116155506398\n\n\n\n\n\nimage-20211116155513536\n\n\n\n\n\n\n\nimage-20211116155534589\n\n\n\n\n\n\n\n\n\nimage-20211116155547256\n\n\n\n\n\n\n\nimage-20211116155601590\n\n\n\n\n\n\n\n\n\nimage-20211116160310811\n\n\n\n\n\n\n\n\nimage-20211116160408418\n\n\n\n\n\n\n\n\nimage-20211116160420769\n\n\n\n\n\n\n\n\nimage-20211116160435312\n\n\n\n\n\n\n\n\nimage-20211116163954629\n\n\n\n\n\n\n\n\nimage-20211116164016489\n\n\n\n\n\n\n\nimage-20211116164227743\n\n\n\n\n\n\n\n\n\nimage-20211116164240307\n\n\n\n\n\nimage-20211116164415290\n\n\n\n\n\n\n\n\nimage-20211116164427531\n\n\n\n\n\nimage-20211116164711518\n\n\n\n\n\n\n\n\nimage-20211116164722207\n\n\n\n\n\n\n\n\nimage-20211116164928672\n\n\n\n\n\nimage-20211116164935415\n\n\n\n\n\nimage-20211116164944044\n\n\n\n\n\n\nTop down\nBottom up\n\n\n\n\n\n\n\nimage-20211116165425927\n\n\n\n\n\nimage-20211116165432333\n\n\n\n\n\nimage-20211116165438534\n\n\n\n\n\nimage-20211116165447382\n\n\n\n\n\n\n\n\nimage-20211116165458926\n\n\n\n\n\n\n\nimage-20211116165514624\n\n\n\n\n\n\n\n\n\nimage-20211116165640104\n\n\n\n\n\nimage-20211116165648879\n\n\n\n\n\n\n\n\nimage-20211116170421334\n\n\n\n\n\n\n\n\nimage-20211116170436913\n\n\n\n\n\nimage-20211116172910024\n\n\n\n\n\n\n\n\nimage-20211116173401233\n\n\n\n\n\nimage-20211116173412503\n\n\n\n\n\nimage-20211116173422232\n\n\n\n\n\nimage-20211116173433795\n\n\n\n\n\nimage-20211116173444781\n\n\n\n\n\n\n\n\nimage-20211116173941190\n\n\n\n\n\n\n\n\nimage-20211116173951578\n\n\n\n\n\nimage-20211116174044238\n\n\n\n\n\nimage-20211116174050402\n\n\n\n\n\n\n\n\nimage-20211116174102136\n\n\n\n\n\n\n\n\nimage-20211116185859072\n\n\n\n\n\n\n\n\nimage-20211116185913330\n\n\n\n\n\nimage-20211116185920257\n\n\n\n\n\n\n\n\nimage-20211116185933063\n\n\n\n\n\n\n\n\nimage-20211116190732161"
  },
  {
    "objectID": "posts/2021-11-30-digital-system-circuits-week-13/2021-11-30-digital-system-circuits-week-13.html",
    "href": "posts/2021-11-30-digital-system-circuits-week-13/2021-11-30-digital-system-circuits-week-13.html",
    "title": "Digital System Circuits Week 13",
    "section": "",
    "text": "image-20211130082001213\n\n\n\n\n\n\n\n\nimage-20211130082252959\n\n\n\n\n\nimage-20211130082306677\n\n\n\n\n\n\n\n\nimage-20211130082634417\n\n\n\n\n\n\n\n\nimage-20211130083251074\n\n\n\n\n\n\n\n\nimage-20211130083403163\n\n\n\n\n\n\n\n\nimage-20211130083827060\n\n\n\n\n\n\n\n\nimage-20211130084539154\n\n\n\n\n\n\n\n\nimage-20211130084556792\n\n\n\n\n\n\n\n\nimage-20211130084613846\n\n\n\n\n\n\n\n\nimage-20211130085517868\n\n\n\n\n\n\n\n\nimage-20211130085529349\n\n\n\n\n\n\n\n\nimage-20211130085746028\n\n\n\n\n\n\n\n\nimage-20211130085805136\n\n\n\n\n\n\n\n\nimage-20211130090030853\n\n\n\n\n\nimage-20211130090118533\n\n\n\n\n\n\n\n\nimage-20211130090216696\n\n\n\n\n\n\n\n\nimage-20211130090341262\n\n\n\n\n\n\n\n\nimage-20211130090353718\n\n\n\n\n\n\n\n\nimage-20211130090952730"
  },
  {
    "objectID": "posts/2021-09-30-system-programming-week-5/2021-09-30-system-programming-week-5.html",
    "href": "posts/2021-09-30-system-programming-week-5/2021-09-30-system-programming-week-5.html",
    "title": "System Programming Week 5",
    "section": "",
    "text": "image-20210930170733781\n\n\n\n\n\n\n\naddq Src, Dest\nt = a + b\n\nCF (Carry Flag) for unsigned\n\nCF set if carry out from most significant bit (unsigned overflow, 비트연산으로 인해 most significant bit의 overflow가 일어났을 때)\n\nZF (Zero Flag)\n\nZF set if t == 0\n\nSF (Sign Flag) for signed\n\nSF set if t < 0 (as signed)\n\nOF (Overflow Flag) for signed\n\nset if two’s-complement (signed) overflow: (a > 0 && b > 0 && t < 0) || (a < 0 && b < 0 && t  >= 0)\n\n\nNot set by leaq instruction\n\n\n\n\ncmpq Src2, Src1\ncmpq b, a like computing a - b without setting destination. (빼기 연산으로 compare 연산을 함)\n\nCF set if carry out from most significant bit (used for unsigned comparisions)\nZF set if a == b\nSF set if (a - b) < 0 (as signed)\nOF set if two’s-complement (signed) overflow (a > 0 && b < 0 && (a - b) < 0) || (a < 0 && b < 0 && (a - b) >=0)\n\n\n\n\n\n\ntestq Src2, Src1\n\ntestq b, a like computing a & b without setting destination.\n\nbit by bit으로 comparison 연산\n\nSets condition codes based on value of Src1 & Src2\nUseful to have one of the operands be a mask\nZF set when a & b == 0\nSF set when a & b < 0\n\n\n\n\n\n\n\n\nimage-20210930172123796\n\n\n\n\n\nimage-20210930172302038\n\n\n\n\n\nimage-20210930172404709\n\n\n\n\n\n\n\n\njX Instructions - jump to different part of code depending on condition codes\n\n\n\nimage-20210930172738658\n\n\n\n\n\nimage-20210930172901805\n\n\n\n\n\nC allows goto statement\nJump to position designated by label.\n\n\n\nimage-20210930173048877\n\n\n\n\n\nimage-20210930173152631\n\n\n\n\n\n\n\n\ninstruction supports: if (Test) Dest <- Src\nGCC tries to use them\n\nbut, only when known to be safe.\n\nBranch의 모든 계산을 다 끝내놓고, 조건에 맞는 결과만 가져와서 씀\n\n\n\n\n\nBranches are very disruptive to instruction flow through pipelines.\nConditional moves do not require control transfers.\n\n\n\n\nimage-20210930173601313\n\n\n\n\n\nimage-20210930173718105\n\n\n\n\n\n\n\n\n\nimage-20210930173841921\n\n\n\n\n\n\n\n\n\n\n\nimage-20210930174209931\n\n\nDo-While statement가 assembly에서 가장 직관적으로 변환됨\n\n\n\nimage-20210930174459270\n\n\n\n\n\n\n\n\nimage-20210930174738332\n\n\n\n\n\n\n\n\nimage-20210930174805078\n\n\n-Og는 optimization을 끄는 것\n\n\n\nimage-20210930174900564\n\n\n\n\n\nimage-20210930174936578\n\n\n\n\n\nimage-20210930174949988\n\n\n\n\n\n\n\n\nimage-20210930175111562\n\n\n\n\n\nimage-20210930175427826\n\n\n\n\n\nimage-20210930175439525\n\n\n\n\n\nimage-20210930175457916\n\n\n\n\n\n왜 if statement보다 switch statement가 더 성능이 좋은지 이해하기\n\n\n\n\n\nimage-20210930175632717\n\n\n\n\n\n\n\n\nimage-20210930175743399\n\n\nJump table을 작성해서 argument가 들어오자 마자 조건에 맞는 Code block으로 바로 jump 할 수 있는 code block의 address를 저장하고 있는 jump table을 활용함. -> 계산을 하지 않아도 됨\nif statement로 many cases를 사용하면 하나하나 conditions들을 연산해야 하기 때문에 switch statement가 더 빠름\n\n\n\n\n\n\nimage-20210930175957830\n\n\n\n\n\nimage-20210930180142350\n\n\n\n\n\n\n\n\n\nimage-20210930180235632\n\n\n\n\n\nEach targe requires 8 bytes\nBase address .L4\n\n\n\n\n\nDirect: jmp .L8\nJump target is denoted by label .L8\nIndirect: jmp *.L4(, %rdi, 8)\nMust scale by factor of 8 (addresses are 8 bytes)\nFetch target from effective Address .L4 + x * 8\n\nonly for \\(0<=x<=6\\)\n\n\n\n\n\nimage-20210930180427390\n\n\n\n\n\n\n\n\nimage-20210930180437329\n\n\n\n\n\n\n\n\nimage-20210930180520109\n\n\n\n\n\n\n\n\nimage-20210930180714933\n\n\n\n\n\n\n\n\nimage-20210930180748232"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#intercorporate-investments",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#intercorporate-investments",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Intercorporate investments",
    "text": "Intercorporate investments\n\nInvestments in financial assets\n\nWhen the investing firm has no significant influence and control over the operations of the investee firm\n\nInvestments in associates\n\nWhen the investing firm has significant influence over the operations of the investee firm, but not control\n\nBusiness combination\n\nWhen the investing firm has control over the operations of the investee firm"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investments-in-financial-assets",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investments-in-financial-assets",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Investments in financial assets",
    "text": "Investments in financial assets\nAn ownership interest of less than 20% is usually considered a passive investment. cannot significantly influence or control the investee.\n\nUS GAAP\n\nDebt securities\n\nHeld-for-trading (HFT)\nAvailable-for-sale (AFS)\nHeld-to-maturity (HTM)\n\n\n\nEquity securities\n\nHFT\nAFS\n\n\n\n\n\n\nValuation\n\nHFT – Fair value -> Net Income -> Retained Earnings\nAFS – Fair value -> OCI -> AOCI\nAmortized cost\n\n\n\n\nIFRS9\nCash flow characteristics test (= SPPI test, Solely Payment of Principal and Interests test)\n\nPass SPPI test – ex) Bond\nBusiness model test\n\nFVTPL (Fair Value Through Profit and Loss) securities\nFVOCI (Fair Value through Other Comprehensive Income) securities\nAmortized cost securities\n\n\n\nFail to pass SPPI test – ex) Stock\nFVTPL securities\n- Optionally, FVOCI 선택 가능\n- 선택 후 irrevocable\n\n\n\nAmortized cost (for debt securities only)\nCriteria for amortized cost accounting:\n\nBusiness model test – debt securities are being held to collect contractual cash flows.\nCash flow characteristic test – the contractual cash flows are either principal, or interest on principal, only.\n\n통과하면, amortized cost로 해당 debt security 평가 – 채권 발행자의 discount, premium, par 발행 후 평가하는 것과 동일함.\nInterest income (coupon cash flow adjusted for amortization of premium or discount) is recognized in the income statement, but subsequent changes in fair value are ignored.\n\n\nFair value through profit or loss (for debt and equity securities)\n두 경우 해당됨\n\nIf held for trading\nIf accounting for those securities at amortized cost results in an accounting mismatch\no Hedge 목적 derivatives는 fair value평가를 하게 되어 있고, hedge 목적물 자산은 amortized cost 평가를 하게 되면 accounting 상 mismatch가 발생한다. 이를 해소하기 위해 둘 다 fair value 평가를 하는 경우\n\nEquity securities that held for trading must be classified as FVPL.\nOther equity securities may be classified as either fair value through profit or loss, or fair value through OCI. Once classified, the choice is irrevocable.\nDerivatives that are not used for hedging are always carried at FVPL.\nIf an asset has an embedded derivatives (e.g., CB), the asset as a whole is valued at FVPL.\nFVPL securities are reported on the balance sheet at fair value. The changes in fair value, both realized and unrealized, are recognized in the income statement along with any dividend or interest income.\n\n\nFair value through OCI (for debt and equity securities)\nFVOCI로 분류되면 unrealized gain or loss is reported in OCI. Realized gain or loss, dividend, and interest income are reported in the income statement.\n\n\nHFT\n\n\n\nDiagram, schematic Description automatically generated\n\n\n\n\nAFS\n\n\n\nText Description automatically generated\n\n\n\n\nHMT\n\n\n\nDiagram, schematic Description automatically generated\n\n\n\n\n\nA blackboard with yellow writing Description automatically generated with low confidence\n\n\n\n\n\nText Description automatically generated\n\n\n\n\n\nText, whiteboard Description automatically generated\n\n\n결국 팔고 나면 순익에 반영되는 결과는 다 같다. 그러나, 중간에 평가이익이 변하는 과정을 인식하는 방식이 서로 다름."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investments-in-associates",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investments-in-associates",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Investments in associates",
    "text": "Investments in associates\nAn ownership interests between 20% and 50% is typically a noncontrolling investment; however, the investor can usually significantly influence the investee’s business operations.\nUnder the equity method, the initial investment is recorded at cost and reported on the balance sheet as a noncurrent asset.\nIn subsequent periods, the proportionate share of the investee’s earnings increases the investment account on the investor’s balance sheet and is recognized in the investor’s income statement. Dividends received from the investee are treated as a return of capital and thus, reduce the investment amount. Dividends received from the investee are not recognized in the investor’s income statement.\nIf the investee reports a loss, the investor’s proportionate share of the loss reduces the investment account and also lower earnings in the investor’s income statement. If the investee’s losses reduce the investment account to zero, the investor usually discontinues use of the equity method.\n- 손실을 보다가 balance sheet의 investment 자산을 다 까먹으면 더 이상 손실을 인식하지 않음. (유한책임 반영) 다만, 주석에 반영\nThe equity method is resumed once the proportionate shares of the investee’s earnings exceed the share of losses the were not recognized during the suspension period.\n- 여태 본 손실 다 회복하고 나서 balance sheet에 다시 equity method로 복귀\n\nSignificant influence\n- Board of directors representation\n- Involvement in policy making\n- Material intercompany transactions\n- Interchange of managerial personnel\n- Dependence on technology\n20% 이하여도 may be possible to have significant influence.\nEquity method – Mark to Market approach (X), fair value와는 전혀 무관하다고 생각해도 됨.\n\n\nExcess of purchase price over book value acquired\nAt the acquisition date, the excess of the purchase price over the proportionate share of the investee’s book value is allocated to the investee’s identifiable assets and liabilities based on their fair values. Any remainder is considered goodwill.\n- 투자자 입장에서 평가된 피투자사 balance sheet의 fair value of asset – fair value of liability를 통해 fair value of equity를 구하고, purchase price – fair value of equity = goodwill의 방식으로 계산\nIn subsequent periods, the investor recognizes expense based on the excess amounts assigned to the investee’s assets and liabilities.\nUnder the equity method of accounting, the investor must adjust its balance sheet investment account and the proportionate share of the income reported from the investee for this additional expense.\n- 비용도 fair value를 기준으로 계산해야 함. Inventory, PPE의 fair value를 높게 잡아줬다면, 비용도 그에 맞춰서 태우고, 이를 기준으로 산출한 NI의 proportionate shares를 equity method로 income statement에 equity income으로 가져옴.\n\n\nImpairment of investments in associates\nAnnually impairment test해야 함.\n\nUS GAAP\n2 steps\n\nRecovery test\n\nThe fair value of the investment < the carrying value of investment\nThe decline is considered other-than-temporary\n\nImpairment\n\nWritten-down to fair value and a loss is recognized on the income statement\n\n\n\n\nIFRS\n1 step\n\nLoss = the book value of the investment – recoverable amount\n\n- Recoverable amount = max(net selling price, value in use)\nUnder both IFRS and US GAAP, if there is a recovery in value in the future, the asset cannot be written-up.\n\n\n\nEquity Method\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nA picture containing text, device, meter, night sky Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\nExcess of purchase price over book value acquired\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\nIntercorporate transactions\n내부거래 미실현이익 제거\nProfit from intercorporate transactions must be deferred until the profit is confirmed through use or sale to a third party.\nFor profit that is unconfirmed (goods have not been used or sold by the investor), the investor must eliminate its proportionate shares of the profit from the equity income of the investee.\n- Upstream (investee to investor)\n- Downstream (investor to investee)\n\n\n\nDiagram, schematic Description automatically generated\n\n\n3rd party에게 inventory 판매되기 전까지 해당 부분은 이익으로 잡으면 안됨.\n\n\nAnalytical issues for investments in associates\nFair MV 변화 없을 경우, the equity method usually results in higher earnings as compared to the accounting methods used for minority passive investments.\nThe investor simply reports its proportionate share of the investee’s equity in one line on the balance sheet. By ignoring the investee’s debt, leverage is lower. In addition, the margin ratios are higher since the investee’s revenues are ignored.\nThe proportionate share of the investee’s earnings is recognized in the investor’s income statement, but the earnings may not be available to the investor in the form of cash flow (dividends). That is, the investee’s earnings may be permanently reinvested."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#business-combinations",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#business-combinations",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Business combinations",
    "text": "Business combinations\nOwnership 50% 이상\nConsolidation by acquisition method\n- 피투자사의 자산과 부채를 전부 투자자의 balance sheet에 엎는다. 그리고 나서 equity를 계산하고, NCI (non-controlling interest)라는 항목으로 보유하지 않은 지분을 equity에 표시한다.\n- Income statement에서도 revenue와 expense를 모두 엎어온다. 그리고 나서 NI산출 전에 minority interest라는 항목으로 보유하지 않은 지분만큼의 earnings를 빼준다.\n\n100% Acquisition\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\n\n\n80% Acquisition\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\nProportionate consolidation\n\n\n\nCompany name Description automatically generated with low confidence\n\n\n\n\n\nGoodwill\nUnder the acquisition method, the purchase price is allocated to the identifiable assets and liabilities of the acquired firm on the basis of fair value. Any remainder is reported on the balance sheet as goodwill. Goodwill is said to be an unidentifiable asset that cannot be separated from the business.\n- 구매한 금액을 피투자사의 balance sheet에서 pro forma하게 나눠서 allocation하는데, 이 때 identifiable asset에만 allocation을 해야한다. 즉, 피투자사가 goodwill과 같이 unidentifiable asset을 가지고 있으면 해당 부분에는 allocation하지 않는다. Allocation을 다 하고 산출된 fair value of equity를 활용해서 purchase price – fair value of equity = goodwill을 계산한다.\nThe full goodwill method results in higher total assets and higher total equity than the partial goodwill method. Thus, ROA and ROE will be lower if the full goodwill method is used.\nGoodwill is not amortized. Instead, it is tested for impairment at least annually. Impairment occurs when the carrying value exceed the fair value. Because of its inseparability, goodwill is valued at the reporting unit level.\n\n\n\nA picture containing map Description automatically generated\n\n\n\nUS GAAP\nGoodwill is the amount by which the fair value of the subsidiary is greater than the fair value of the subsidiary’s identifiable net assets (full goodwill).\n- Goodwill = fair value of 피투자사 - 피투자사의 fair value of equity\n- 둘은 엄연히 다르며, fair value of equity는 투자자가 바라보는 피투자사의 fair value of asset and liability를 통해 산출된 값을 의미하며, fair value of 피투자사는 투자자가 지불한 투자사의 equity의 market price로 이해하면 된다. 즉, purchase price.\nUS GAAP는 full goodwill만 허용한다.\n- Full goodwill은 goodwill을 산출할 때 내가 보유하지 않은 지분까지 전부 포함해서 산출하는 개념을 의미한다. 즉, NCI (non-controlling interest)도 fair value 평가를 해주는 개념.\nGoodwill impairment potentially involves two steps.\n\nThe carrying value of the reporting unit (including the good will) > the fair value of the reporting unit\nImpairment loss = difference between the carrying value of the goodwill and the implied fair value of the goodwill.\no The impairment loss is recognized in the income statement as a part of continuing operations.\n\nThe implied fair value of the goodwill is calculated in the same manner as goodwill at the acquisition date. That is, the fair value of the reporting unit is allocated to the identifiable assets and liabilities as if they were acquired on the impairment measurement date. Any excess is considered the implied fair value of the goodwill.\n\n\nIFRS\nGoodwill is the excess of the purchase price over the fair value of the acquiring company’s proportion of the acquired company’s identifiable net assets (partial goodwill). However, IFRS permits the use of the full goodwill approach also.\nTesting for impairment involves a single step approach.\n\nThe carrying amount of the cash generating unit (where the goodwill is assigned) > the recoverable amount -> impairment loss is recognized.\n\n- Recoverable amount = max(net selling price, value in use)\n\n\n\nImpairment of goodwill\n\n\n\nText Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nUnder IFRS and US GAAP, recovery is not allowed.\n\n\nBargain purchase\nIn rare cases, acquisition purchase price is less than the fair value of net assets acquired. Both IFRS and US GAAP requires that the difference between fair value of net assets and purchase price be recognized as a gain in the income statement.\n\n\n\nText Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#joint-ventures",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#joint-ventures",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Joint ventures",
    "text": "Joint ventures\nInvestment in associates로 취급.\n개정됐으나 시험목적 상 In rare cases, IFRS and US GAAP allow proportionate consolidation as opposed to the equity method. Proportionate consolidation is similar to a business acquisition, except the investor (venturer) only reports the proportionate share of the assets, liabilities, revenues, and expenses of the joint ventures. Since only the proportionate share is reported, no minority owners’ interest is necessary.\nJoint venture is an entity in which control is shared by two or more investors."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#special-purpose-and-variable-interest-entities",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#special-purpose-and-variable-interest-entities",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Special purpose and variable interest entities",
    "text": "Special purpose and variable interest entities\nA SPE is a legal structure created to isolate certain assets and liabilities of the sponsor. SPEs are often structured such that the sponsor company has control over the SPE’s finances or operating activities while third parties have controlling interest in the SPE’s equity.\nConsolidation, a VIE is an entity that has one or both of the following characteristics:\n\\1. At-risk equity that is insufficient to finance the entity’s activities without additional financial support.\n\\2. Equity investors that lac any one of the followings:\no Decision making rights\no The obligation to absorb expected losses\no The right to receive expected residual returns\nIf an SPE is considered a VIE, it must be consolidated by the primary beneficiary. The primary beneficiary is the entity that absorbs the majority of the risks or receives the majority of the rewards.\n\nUS GAAP\nVIE condition을 마족하면 consolidation\n\n\nIFRS\n실질적인 지배력 (de facto control)이 있으면 consolidation\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#dc-defined-contribution",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#dc-defined-contribution",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "DC (Defined Contribution)",
    "text": "DC (Defined Contribution)\nThe firm contributes a certain sum each period to the employee’s retirement account. The firm’s contribution can be based on any number of factors including years of service, the employee’s age, compensation, profitability, or even a percentage of the employee’s contribution. In any event, the firm makes no promise to the employee regarding the future value of the plan assets. The investment decisions are left to the employee, who assumes all of the investment risk.\nPension expense is simply equal to the employer’s contribution. There is no future obligation to report on the balance sheet.\n회사는 지불한 cash paid = pension expense. 그러므로 no obligation if fully funded."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#db-defined-benefit",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#db-defined-benefit",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "DB (Defined Benefit)",
    "text": "DB (Defined Benefit)\n\n\n\nGraphical user interface Description automatically generated\n\n\n\n\n\nDiagram Description automatically generated\n\n\nThe firm promises to make periodic payments to the employee after retirement.\nSince the employee’s future benefit is defined, the employer assumes the investment risk.\n굉장히 많은 assumption이 들어감.\nUnder US GAAP, Projected Benefit Obligation (PBO)\nUnder IFRS, Present Value of Defined Benefit Obligation (PV of DBO)\n\nFunded Status of the plan\nThe difference in the benefit obligation and the plan assets\nFunded Status = Plan Asset – PBO\nPlan Asset > PBO: Over funded\nPlan Asset < PBO: Under Funded"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-post-employment-benefits",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-post-employment-benefits",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Other post-employment benefits",
    "text": "Other post-employment benefits\n\nHealth care benefits\nSimilar to DB pension plan\nFuture benefit is defined today but is based on a number of unknown variables. The employer must forecast health care costs that are expected once the employee retires.\nPension plans are typically funded at some level, while other post-employment benefit plans are usually unfunded. In the case of an unfunded plan, the employer recognizes expense in the income statement as the benefits are earned; however, the employer’s cash flow is not affected until the benefits are actually paid to the employee."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#analytical-purpose",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#analytical-purpose",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Analytical purpose",
    "text": "Analytical purpose\n\nType I – Adjustment of Financial statement\nBalance Sheet – no adjustment\nIncome Statement – Pension Expense smoothing adjustment\n\nNot smoothing\n\nTPPC = Current service cost + Income cost – Actual Return +/- Change in Assumption + Plan amendment\nTPPC = contribution – (Beginning funded status – Ending funded status)\n\nReclassification of TPPC\n\nOperating\nNon-operating – Interest cost, Actual Return\n\n\nCash flow Statement\nTPPC만큼은 Pension Fund에 불입했어야 한다.\n원래는\nCFO (Actual Contribution)\nCFI\nCFF\nAnalytical adjustment\nCFO ((Actual Contribution) + (TPPC – Actual Contribution) * (1 – t))\nCFI\nCFF (TPPC – Actual Contribution) * (1 – t)\n세금효과 고려해서 TPPC보다 부족한 부분을 차입했다고 가정하고 CF Statement를 조정\nTPPC가 Actual Contribution보다 낮으면 CFF에 +로 처리\n\nCurrent Service Cost\nThe present value of benefits earned by the employees during the current period. Current service cost is immediately recognized in the income statement.\n\n\nInterest Cost\nInterest cost = Beginning PBO * Discount Rate\nInterest Cost is immediately recognized as a component of pension expense.\n\n\nExpected Return on Plan Assets\nThe return on the plan assets has no effect on the PBO. Expected return instead of actual return) on plan assets is used for the computation of reported pensions expense. The difference in the expected return and the actual return is combined with other items related to changes in actuarial assumptions into the “actuarial gains and losses” account. The difference in the expected return and the actual return is combined with other items related to changes in actuarial assumptions into the “actuarial gains and losses” account.\n\n\nNet Interest Expense/Income (Under IFRS)\nNet Interest Expense (Income) = Beginning Funded Status * Discount Rate\nUnder IFRS, the expected rate of return on plan assets is implicitly assumed to be the same as the discount rate used for computation of PBO and a net interest expense/income is reported.\n\n\nActuarial Gains and Losses\nActuarial gains/losses as changes in PBO due to changes in actuarial assumptions.\nUnder IFRS, actuarial gains and losses are not amortized.\nUnder US GAAP, actuarial gains and losses are amortized using the corridor approach.\n\nCorridor Approach (US GAAP only)\nOnce the beginning balance of actuarial gains and losses exceed 10% of the greater of the beginning PBO or plan assets, amortization is required. The excess amount over the “corridor” is amortized as a component of periodic pension cost in P&L over the remaining service life of the employees.\n\n\n\nPast (prior) Service Cost\nWhen a firm adopts or amends its pension plan, the PBO is immediately increased.\nUnder US GAAP, it is reported as a part of other comprehensive income and amortized over the remaining service life of the affected employee.\nUnder IFRS, the past service costs are recognized in periodic pension cost in P&L immediately.\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\n\n\n\n\n\n\n\n\nComponent\nUS GAAP\nIFRS\n\n\n\n\nCurrent service cost\nIS\nIS\n\n\nPast service cost\nOCI, amortized over service life\nIS\n\n\nInterest cost\nIS\nIS\n\n\nExpected return\nIS\nIS*\n\n\nActuarial gains/losses\nAmortized portion in IS. Unamortized in OCI.\nAll in OCI – not amortized (called remeasurements)\n\n\n\n*Under IFRS, the expected rate of return on PA equals the discount rate and net interest expense/income is reported.\n\n\n\nPresentation\n\n\n\nGraphical user interface Description automatically generated with medium confidence\n\n\nUnder US GAAP, all components of periodic pension cost that are reported in the income statement are aggregated and presented as a single line item.\nUnder IFRS, components may be presented separately.\nBoth US GAAP and IFRS require disclosure of total periodic pension cost in the notes to financial statements."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#ultimate-health-care-trend-rate",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#ultimate-health-care-trend-rate",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Ultimate health care trend rate",
    "text": "Ultimate health care trend rate\n\n\n\nText, whiteboard Description automatically generated\n\n\nThe assumptions are similar for other post-employment benefits except the compensation growth rate is replaced by a health care inflation rate. Generally, the presumption is the inflation rate will taper off and eventually become constant. This constant rate is known as the ultimate health care trend rate.\nAll else equal, firms can reduce the post-employment benefit obligation and periodic expense by decreasing the near term health care inflation rate, by decreasing the ultimate health care trend rate, or by reducing the time needed to reach the ultimate health care trend rate."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-allocation",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-allocation",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Asset allocation",
    "text": "Asset allocation\nUnder US GAAP, the assumed expected rate of return should be consistent with plan’s asset allocation."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#share-based-compensation",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#share-based-compensation",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Share-based Compensation",
    "text": "Share-based Compensation\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nStock options\nOutright share grants\nThey have the advantages of serving to motivate and retain employees as well as being a way to reward employees with no additional outlay of cash.\nIf the shares are not publicly traded, an estimate of value must be used for stock grants. Unless the market price of the options is available, the value of stock options must be estimated using an options valuation model.\nThe compensation expense recorded is spread over the period of time from the grant date until the date on which they can be sold by the employee. The overall principle here is the compensation expense should be spread over the period for which they reward the employee, referred to as the service period."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-option",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-option",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Stock option",
    "text": "Stock option\n\n\n\nA blackboard with writing on it Description automatically generated with low confidence\n\n\n\n\n\nDiagram Description automatically generated with medium confidence\n\n\nAdditional Paid-In Capital에 Stock Option 명목으로 올리고, Income statement에 compensation expense로 태우고, Retained Earnings가 낮아지면서 대차가 맞아지는 과정\n실제 exercise되면 CFF를 통해 현금이 들어오고, 대차를 맞춤\nCompensation expense is based on the fair value of the options on the grand date based on the number of options that are expected to vest. The vesting date is the first date the employee can actually exercise the options. The compensation expense is allocated in the income statement over the service period.\nRecognition of compensation expense will decrease net income and retained earnings; however, paid-in capital is increased by an identical amount. This results in no change to total equity.\nOption pricing models\n\nExercise price\nStock price at the grant date\nExpected term\nExpected volatility\nExpected dividends\nRisk-free rate\n\nLower volatility, a shorter term or a lower risk-free rate, will typically decrease the estimated fair value of the options, decreasing compensation expense. A higher expected dividend yield will also decrease the estimated fair value and compensation expense."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-appreciation-rights",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-appreciation-rights",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Stock appreciation rights",
    "text": "Stock appreciation rights\n\n\n\nA screenshot of a computer Description automatically generated with medium confidence\n\n\n현금만큼 지급하는 개념이라 Liability에 태우고, 실제 지급할 때 cash가 나가면서 liability가 사라짐\nThe difference between a stock appreciation right and an option is the form of payment. A stock appreciation award gives the employee the right to receive compensation based on the increase in the price of the firm’s stock over a predetermined amount. With stock appreciation rights, employees have limited downside risk and unlimited upside potential. Also, since no shares are actually issued, there is no dilution to existing shareholders. A disadvantage of stock appreciation rights is that they require current period cash outlay."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-grants",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#stock-grants",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Stock grants",
    "text": "Stock grants\nCompensation expense for stock granted to an employee is based on the fair value of the stock on the grant date. The compensation expense is allocated over the employee’s service period.\nRestricted stock, performance stock\nWith restricted stock, the transferred stock cannot be sold by the employee until vesting has occurred.\nPerformance stock is contingent on meeting performance goals, such as accounting earnings or other financial reporting metrics like return on assets or return on equity."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#current-rate-method",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#current-rate-method",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Current rate method",
    "text": "Current rate method\n\n\n\nDiagram Description automatically generated with medium confidence\n\n\nAll income statement accounts are translated at the average rate.\nAll balance sheet accounts are translated at the current rate except for common stock, which is translated at the historical (actual) rate that applied when the stock was issued.\nDividends are translated at the rate that applied when they were declared.\nTranslation gain or loss is reported in shareholders’ equity as a part of the cumulative translation adjustment (CTA).\nThe CTA is simply a “plug” figure that forces the basic accounting equation (A = L + E) to balance.\n\n\n\nText Description automatically generated\n\n\nMonetary assets and liabilities are remeasured using the current exchange rate. Monetary assets and liabilities are fixed in the amount of currency to be received or paid and include: cash, receivables, payables, and short-term and long-term debt.\nAll other assets and liabilities are considered nonmonetary and are remeasured at the historical (actual) rate. The most common nonmonetary assets include inventory, fixed assets, and intangible assets.\nJust life the current rate method, common stock and dividends paid are remeasured at the historical (actual) rate.\nExpenses related to nonmonetary assets such as COGS, depreciation expense, and amortization expense are remeasured based on the historical rates prevailing at the time of purchase.\nRemeasurement gain or loss is recognized in the income statement. This results in more volatile net income as compared to the current rate method whereby the translation gain or loss is reported in shareholders’ equity.\nRecall the ending inventory under FIFO consists of the costs from the most recently purchased goods. Thus, FIFO ending inventory is remeasured based on more recent exchange rates. On the other hand, FIFO COGS consists of costs that are older; thus, the exchange rates used to remeasure COGS are older.\nUnder LIFO, ending inventory consists of older costs; thus, the inventory is remeasured at older exchange rates. LIFO COGS, however, consists of costs from the most recently purchased goods; thus, COGS is remeasured based on more recent exchange rates.\n\n\n\n\nLocal currency\n\n\n\n\n\nExposure\nappreciating\ndepreciating\n\n\nCurrent rate method\n\n\n\n\nNet assets\ngain\nLoss\n\n\nNet liabilities\nloss\nGain\n\n\nTemporal method\n\n\n\n\nNet monetary assets\ngain\nLoss\n\n\nNet monetary liabilities\nloss\nGain\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal method\nCurrent rate method\n\n\n\n\nMonetary assets and liabilities\nCurrent rate\nCurrent rate\n\n\nNonmonetary assets and liabilities\nHistorical rate\nCurrent rate\n\n\nCommon stock\nHistorical rate\nHistorical rate\n\n\nEquity (taken as a whole)\nMixed\nCurrent rate\n\n\nRevenues and SG&A\nAverage rate\nAverage rate\n\n\nCost of goods sold\nHistorical rate\nAverage rate\n\n\nDepreciation and amortization\nHistorical rate\nAverage rate\n\n\nNet income\nMixed\nAverage rate\n\n\nExposure\nNet monetary assets\nNet assets\n\n\nExchange rate gain or loss\nIncome statement\nEquity\n\n\n\nUnder the temporal method, firms can eliminate their exposure to changing exchange rates by balancing monetary assets and monetary liabilities. When balanced, no gain or loss is recognized."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pure-balance-sheet-and-pure-income-statement-ratios",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pure-balance-sheet-and-pure-income-statement-ratios",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Pure balance sheet and pure income statement ratios",
    "text": "Pure balance sheet and pure income statement ratios\nPure income statement and pure balance sheet ratios are unaffected by the application of the current rate method."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#mixed-balance-sheetincome-statement-ratios",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#mixed-balance-sheetincome-statement-ratios",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Mixed balance sheet/income statement ratios",
    "text": "Mixed balance sheet/income statement ratios\n다를 수 있음\n하나하나 잘 따져보면서 ratio에서 적용되는 exchange rate 구별해서 계산"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#basel-iii",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#basel-iii",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Basel III",
    "text": "Basel III\nThe three pillars of the Basel III framework are the maintenance of minimum levels of capital, liquidity, and stable funding.\n- Minimum required capital for a bank is based on the risk of the bank’s assets. The riskier a bank’s assets are, the higher its required capital.\n- Basel III specifies that a bank should hold enough liquid assets to meet demands under a 30-day liquidity stress scenario.\n- The Basel III framework requires stable funding relative to a bank’s liquidity needs over a one-year time horizon. Stability in funding is proportional to the tenor of the bank’s deposits; longer-term deposits are more stable than shorter-term deposits. Stability also depends on the type of deposit."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#global-organizations",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#global-organizations",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Global organizations",
    "text": "Global organizations\n\nFinancial Stability Board\nInternational Association of Deposit Insurers – seeks to improve the effectiveness of deposit insurance systems.\nInternational Organization of Securities Commissions (IOSCO) – seeks to promote fair and efficient security markets.\nInternational Association of Insurance Supervisors (IAIS) – seeks to improve supervision of the insurance industry."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-adequacy",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-adequacy",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Capital Adequacy",
    "text": "Capital Adequacy\n\n\n\nA blackboard with yellow writing Description automatically generated with low confidence\n\n\nCapital adequacy is based on risk-weighted assets (RWA); more risky assets require a higher level of capital.\nBasel III defines a bank’s capital in a tiered, hierarchical approach:\n\nTier 1 capital:\n\nCommon equity Tier 1 capital (the most important component): Common stock, additional paid-in capital, retained earnings and OCI less intangibles and deferred tax assets.\nOther Tier 1 capital: subordinated instruments with no specified maturity and no contractual dividends (e.g., preferred stock with discretionary dividends).\n\nTier 2 capital: subordinated instruments with original (i.e., when issued) maturity of more than five years.\n\nIndividual jurisdictions specify the minimum capital requirements. Basel III guidelines specify a minimum Common Equity Tier 1 capital of 4.5% of RWA, minimum total Tier 1 capital of 6% of RWA, and minimum total capital of 8% of RWA.\n\n\n\nA picture containing text, blackboard Description automatically generated"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-quality",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-quality",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Asset Quality",
    "text": "Asset Quality\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nEvaluation of asset quality includes analysis of current and potential credit risk associated with the bank’s assets.\nBank assets include loans (the largest component) and investments in securities. While loans are generally carried on the balance sheet at amortized cost (net of allowances), the accounting treatment for investments in securities differs between US GAAP and IFRS."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#credit-risk-analysis",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#credit-risk-analysis",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Credit Risk Analysis",
    "text": "Credit Risk Analysis\nCredit risk is present in debt securities that the bank invests in, loans the bank makes, as well as in the bank’s off-balance-sheet liabilities. Analyzing the credit quality of a bank’s assets can provide an analyst key insights into the bank’s solvency and future profitability.\n\nKey loans\n- Past due but not yet impaired\n- Impaired\n- Seriously impaired"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#loan-loss-provisions",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#loan-loss-provisions",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Loan Loss Provisions",
    "text": "Loan Loss Provisions\nThe credit quality of loans and loss provisions are critical in evaluating the bank’s financial position and performance. Allowance for loan losses is a contra asset account to loans and is the result of provision for loan losses, an expense subject to management discretion. Analysts need to evaluate the bank’s policy of setting aside adequate provisions relative to actual loan performance. Actual losses (net of recoveries) are then written off against these provisions.\n\nRatios\n\nRatio of allowance for loan losses to nonperforming loans.\nRatio of allowance for loan losses to net loan charge-offs.\nRatio of provision for loan losses to net loan charge-offs."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#management-capabilities",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#management-capabilities",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Management capabilities",
    "text": "Management capabilities"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#earnings",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#earnings",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Earnings",
    "text": "Earnings\n\n\n\nA blackboard with yellow writing Description automatically generated with low confidence\n\n\nEarnings are considered high quality if they are adequate as well as sustainable. Ideally, the trend in earnings should be positive and the underlying accounting estimates should be unbiased. Finally, earnings should ideally be derived from recurring sources.\nA major source of earnings of a bank is from investment in securities. Estimates used in the valuation of these securities may lead to biased earnings.\nLevel 1 inputs are quoted market prices of identical assets.\nLevel 2 inputs are observable but not quoted prices of identical assets.\nLevel 3 inputs are non-observable and hence subjective.\nIn practice, banks often use the fair value hierarchy to label their assets or to label their valuation methodology.\nFor a typical bank, major sources of earnings are (1) net interest income, (2) service income, and (3) trading income. Of these, trading income is the most volatile year-to-year, and, hence, on a relative basis, banks with proportionally higher net interest income and service income would have more sustainable earnings."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#liquidity-position",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#liquidity-position",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Liquidity Position",
    "text": "Liquidity Position\n\n\n\nText Description automatically generated with medium confidence\n\n\n\nLiquidity coverage ratio (LCR)\nLCR = highly liquid assets / expected cash outflows\nHighly liquid assets are those that are easily convertiable into cash, while expected cash flows are the estimated on-month liquidity needs in a stress scenario. The standards recommend a minimum LCR of 100%.\n\n\nNet stable funding ratio (NSFR)\nNSFR = available stable funding / required stable funding\nAvailable stable funding is a function of the composition and maturity distribution of a bank’s funding sources. Required stable funding is a function of the composition and maturity distribution of the bank’s asset base.\n\n\n\n\n\n\n\n\nFunding Component\nASF Factor\n\n\n\n\n\nRegulatory capital minus Tier 2 instruments maturing in a year\n100%\nEquity\n\n\nOther capital instruments and liabilities with maturity > 1 year\n95%\n\n\n\nLess-stable demand deposits and term deposits (maturity < 1 year) from retail and small business customers\n90%\n\n\n\nFunding from nonfinancial corporates (maturity < 1 year), operational deposits, funding from sovereigns, public sector (maturity < 1 year), multilateral and national development banks\n50%\n\n\n\nAll other liabilities\n0%\nShort-term\n\n\n\nNSFR relates the liquidity needs of a bank’s assets to the liquidity provided by the bank’s liabilities. Longer-dated liabilities are considered more stable and hence would be suitable to fund assets with longer maturities. Deposits from retail and small business clients are considered more stable than deposits from corporate clients. The standards recommend a minimum NSFR of 100%.\nOther liquidity monitoring metrics recommended by Basel III include concentration of funding and maturity mismatch."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#sensitivity-to-market-risk",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#sensitivity-to-market-risk",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Sensitivity to Market Risk",
    "text": "Sensitivity to Market Risk\nThe most critical of various market risks is interest rate risk. A bank’s interest rate risk is the result of differences in maturity, rates, and repricing frequency between the bank’s assets and its liabilities.\nThe impact of market risk can be captured by value at risk (VaR)."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-factors",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-factors",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Other Factors",
    "text": "Other Factors\n- Government support\n- Government ownership\n- Bank missions\n- Culture"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pc-insurance-companies",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pc-insurance-companies",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "P&C Insurance Companies",
    "text": "P&C Insurance Companies\nPremium income is usually the highest source of income for a P&C insurer. Keys to the profitability of an insurer are prudence in underwriting, pricing of adequate premiums for bearing risk, and diversification of risk. To diversify their risks, insurers often reinsure some risks. The policy period is often very short, with premiums received at the beginning of the period and invested during the float period. Claim events are clearly defined but may take a long time to emerge.\nProperty insurance covers specific assets against loss due to insured events. Casualty insurance protects against a legal liability due to the occurrence of a covered event. Sometimes a poly, known as multiple peril policy, may cover both property and casualty losses occurring during a covered event."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pc-profitability",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#pc-profitability",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "P&C Profitability",
    "text": "P&C Profitability\nP&C margins are cyclical. During periods of heightened competition, price cutting to obtain new business leads to slim or negative margins (soft pricing period). This soft pricing period leads to losses and a shrinking capital base for many insurers, who either leave the industry or stop underwriting new policies. The resulting reduction in competition leads to a healthier pricing environment (hard pricing period), which in turn results in fatter margins. Higher margins during the hard pricing period attract new competition, perpetuating the cycle.\nMajor expenses for P&C insurers include claims expense and the expense of obtaining new policy business. The cost of writing new policies depends on whether the insurer uses a direct-to-customer model (in which case it would bear the fixed cost of staffing) or the agency model (in which case it would pay variable commissions). Soft or hard pricing is driven by the industry’s combined ratio (total insurance expenses / net premiums earned). When the ratio is low (high), it is a hard (soft) market.\nFor a single insurer, a combined ratio in excess of 100% indicates an underwriting loss.\nUnderwriting loss ratio (loss and loss adjustment expense ratio) = (claims paid + delta of loss reverse) / net premium earned\nExpense ratio (underwriting expense ratio) = underwriting expenses including commissions / net premium written\nThe underwriting loss ratio measures the relative efficiency of the company’s underwriting standards (whether the policies are priced appropriately relative to risks borne), while the expense ratio measures the efficiency of the company’s operations.\nFor reporting purposes, sometimes insurers use US GAAP, which calls for net premium earned as the denominator for both ratios.\nThe loss reverse is an estimated value of unpaid claims (based on estimated losses incurred during the reporting period). Subject to management discretion in measurement, the loss reserve is a highly material amount. Insurers revise their estimate of the loss reserve as more information becomes available. Downward revisions indicate that the company was conservative in estimating their losses in the first place. Upward revisions indicate aggressive profit booking, a warning sign for analysts."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-profitability-ratios",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#other-profitability-ratios",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Other profitability ratios",
    "text": "Other profitability ratios\n\nLoss and loss adjustment expense ratio\nLoss and loss adjustment expense ratio = (loss expense + loss adjustment expense) / net premiums earned\nLoss and loss adjustment expense ratio measures the relative success in estimation of risks insured (lower is more successful).\n\n\nDividends to policyholders (shareholders) ratio\nDividends to policyholders ratio = dividends to policy holders (shareholders) / net premiums earned\nDividends to policyholders (shareholders) ratio is a liquidity measure measuring cash outflow on account of dividends relative to premium income.\nCombined ratio = loss and loss adjustment expense ratio + underwriting expense ratio\n\n\nCombined ratio after dividends (CRAD)\nCRAD measures total efficiency and is more comprehensive than the combined ratio.\nCRAD = combined ratio + dividends to policyholders ratio"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investment-returns",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#investment-returns",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Investment returns",
    "text": "Investment returns\nTotal investment return ratio = total investment income / invested assets\nTotal investment return ratio is used to evaluate the performance of an insurer’s investment operations."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#liquidity",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#liquidity",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Liquidity",
    "text": "Liquidity\nLiquidity is an important consideration for P&C insurers as they stand ready to meet their claim obligations. One way to gauge the liquidity of the investment portfolio is to look at their fair value hierarchy reporting."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capitalization",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capitalization",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Capitalization",
    "text": "Capitalization\nThere are no global risk-based capital requirement standards for insurers. Regionally, the EU has adopted the Solvency II standards, while NAIC in the United States has specified minimum capital levels based on size and risk (including asset, credit. Liquidity, underwriting, and other relevant risks)."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#lh-insurance-companies",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#lh-insurance-companies",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "L&H Insurance companies",
    "text": "L&H Insurance companies\nL&H insurers derive their revenue primarily from premiums, while investment income is the secondary source.\n\nRevenue diversification\nEarnings characteristics\nInvestment returns\nLiquidity\nCapitalization"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#financial-reports-quality-high-to-low",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#financial-reports-quality-high-to-low",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Financial reports quality (high to low)",
    "text": "Financial reports quality (high to low)\n\nGAAP compliant and decision-useful, high-quality earnings\nGAAP compliant and decision-useful, low-quality earnings\nGAAP compliant but not decision-useful, (biased choices)\nNon-compliant accounting\nFraudulent accounting"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#measurement-and-timing-issues",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#measurement-and-timing-issues",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Measurement and timing issues",
    "text": "Measurement and timing issues\nAggressive revenue recognition / conservative revenue recognition"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#classification-issues",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#classification-issues",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Classification issues",
    "text": "Classification issues\nMisclassification\n\nRemoving accounts receivable by selling or transferring receivables to a related entity or by treating them as long-term receivables.\nReclassifying non-core revenues as revenues from core continuing operations\nReclassifying expenses as non-operating\nTreating investing cash flows as operating cash flows"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#biased-accounting",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#biased-accounting",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Biased accounting",
    "text": "Biased accounting\n\nMechanisms to misstate profitability\n- Aggressive revenue recognition, including channel stuffing (aggressively selling products to distributors on generous terms such as lax return policies), bill and hold sales (where economic title to goods may not truly pass to customers), and outright fake sales.\n- Lessor use of finance lease classification – sales-type lease\n- Classifying non-operating revenue/income as operating, and operating expenses as non-operating.\n- Channeling gains through net income and losses through OCI.\n\n\nWarnings signs of misstated profitability\n- Revenue growth higher than peers’\n- Receivables growth higher than revenue growth\n- High rate of customer returns\n- High proportion of revenue is received in final quarter\n- Unexplained boost to operating margin\n- Operating cash flow lower than operating income\n- Inconsistency in operating versus non-operating classification over time\n- Aggressive accounting assumptions – high estimated useful lives\n- Executive compensation largely tied to financial results.\n\n\nMechanisms to misstate assets/liabilities\n- Choosing inappropriate models and/or model inputs and thus affecting estimated values of financial statement elements – estimated useful lives for long-lived assets\n- Reclassification from current to non-current\n- Over-or understating allowances and reserves\n- Understating identifiable assets (and overstating goodwill) in acquisition method accounting for business combinations\n\n\nWarnings signs of misstated assets/liabilities\n- Inconsistency in model inputs for valuation of assets versus valuation of liabilities\n- Typical current assets being classified as non-current\n- Allowances and reserves differ from those of peers and fluctuate over time\n- High goodwill relative to total assets\n- Use of special purpose entities\n- Large fluctuations in deferred tax assets/liabilities\n- Large off-balance-sheet liabilities\n\n\nMechanisms to overstate operating cash flows\n- Managing activities to affect cash flow from operations – working capital management, stretching payables\n- Misclassifying investing cash flow as cash flow from operations\n\n\nWarnings signs of overstated operating cash flows\n- Increase in payables combined with decreases in inventory and receivables.\n- Capitalized expenditures which flow through investing activities.\n- Increases in bank overdraft"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#business-combinations-acquisition-method-accounting",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#business-combinations-acquisition-method-accounting",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Business combinations-acquisition method accounting",
    "text": "Business combinations-acquisition method accounting"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#gaap-accounting-but-not-economic-reality",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#gaap-accounting-but-not-economic-reality",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "GAAP accounting but not economic reality",
    "text": "GAAP accounting but not economic reality"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#quantitative-tools",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#quantitative-tools",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Quantitative tools",
    "text": "Quantitative tools\n\nThe Beneish Model\n\n\n\nCalendar Description automatically generated with low confidence\n\n\nThe Beneish model is a probit regression model that estimates the probability of earnings manipulation using eight dependent variables. The M-score determines the probability of earnings manipulation – higher values indicate high probabilities.\nM-score > -1.78 indicates a higher-than-acceptable probability of earnings manipulation.\n- DSRI (Days sales receivable index) – ratio of days’ sales receivables in year t relative to year t – 1. A large increase in DSRI could be indicative of revenue inflation.\n- GMI (Gross margin index) – ratio of gross margin in year t – 1 to that in year t. a firm with declining margins is more likely to manipulate earnings.\n- AQI (Asset quality index) – ratio of non-current assets other than plant, property, and equipment to total assets in year t relative to year t – 1. Increases in AQI could indicate excessive capitalization of expenses.\n- SGI (Sales growth index) – ratio of sales in year t relative to year t – 1. While not a measure of manipulation by itself, growth companies tend to find themselves under pressure to manipulate earnings to meet ongoing expectations.\n- DEPI (Depreciation index) – ratio of depreciation rate in year t – 1 to the corresponding rate in year t. the depreciation rate is depreciation expense divided by depreciation plus PPE. A DEPI greater than 1 suggests that assets are being depreciated at a slower rate in order to manipulate earnings.\n- SGAI (Sales, general and administrative expenses index) – ratio of SGA expenses (as a % of sales) in year t relative to year t – 1. Increases in SGA expenses might predispose companies to manipulate.\n- Accruals = (income before extraordinary items – cash flow from operations) / total assets\n- LEVI (Leverage index) – ratio of total debt to total assets in year t relative to year t – 1.\nThe Beneish model relies on accounting data, which may not reflect economic reality. Additionally, as managers become aware of the use of specific quantitative tools, they may begin to game the measures used."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#altman-model",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#altman-model",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Altman Model",
    "text": "Altman Model\n\n\n\nDiagram Description automatically generated\n\n\nAltman’s Z-score model was developed to assess the probability that a firm will file for bankruptcy.\n- Net working capital as a proportion of total assets\n- Retained earnings as a proportion of total assets\n- Operating profit as a proportion of total assets\n- Market value of equity relative to book value of liabilities\n- Sales relative to total assets\nEach variable is positively related to the Z-scores, and a higher Z-score is better (less likelihood of bankruptcy)."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#accruals",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#accruals",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Accruals",
    "text": "Accruals\n\n\n\nA picture containing text, blackboard Description automatically generated\n\n\nAccrual accounting requires considerable subjectivity because of the many estimates and judgments involved with assigning revenues and expenses to appropriate periods. Due to this subjectivity in revenue and expense recognition, disaggregating income into its two major components, cash and accruals, further enhances its quality as an input for forecasting future earnings. The accrual component of income is less persistent than the cash component."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#revenue-recognition-issues",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#revenue-recognition-issues",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Revenue recognition issues",
    "text": "Revenue recognition issues\nRevenues generated via deliberate channel-stuffing or as a result of bill-and-hold arrangements should be considered spurious and inferior.\n\nCash versus accruals\nCompare financials with physical data provided by the company\nEvaluate revenue trends and compare with peers\nCheck for related party transactions"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#expense-capitalization",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#expense-capitalization",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Expense capitalization",
    "text": "Expense capitalization\nOne way to boost reported performance is to under-report an operating expense by capitalizing it."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#completeness",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#completeness",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Completeness",
    "text": "Completeness\nIn the case of intercorporate investments, the equity method of accounting allows one-line consolidation for investments in associates. The equity method of accounting would result in certain profitability ratios being higher than under the acquisition method."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#unbiased-measurement",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#unbiased-measurement",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Unbiased measurement",
    "text": "Unbiased measurement\n- Value of the pension liability (based on several actuarial assumptions)\n- Value of investment in debt or equity of other companies for which a market value is not readily available.\n- Goodwill value (subjectivity in impairment testing)\n- Inventory valuation (subjectivity in testing for impairment)\n- Impairment of PP&E and other assets"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#sources-of-earnings-and-return-on-equity",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#sources-of-earnings-and-return-on-equity",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Sources of earnings and return on equity",
    "text": "Sources of earnings and return on equity\nROE = Tax Burden * Interest Burden * EBIT margin * Total Asset Turnover * Financial Leverage\n\n\n\nText Description automatically generated\n\n\nWe must also consider the firm’s sources of income and whether the income is generated internally from operations or externally.\nEliminating the equity income from the investor’s earnings permits analysis of the investor’s performance resulting exclusively from its own asset base. Since, under the equity method, the firm’s investment is reported as a balance sheet asset, total assets should be reduced by the carrying value of investment.\nAnother common adjustment made by analysts is to remove the effects of any unusual items (e.g., provisions for restructuring and litigation, goodwill impairment, etc.) from reported operating earnings (EBIT) before computing the EBIT margin and the tax burden ratios."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-base",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#asset-base",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Asset base",
    "text": "Asset base\nAnalysis of the asset base requires an examination of changes in the composition of balance sheet assets over time.\nSince goodwill is no longer amortized through the income statement, we must consider the possibility of losses in the future if goodwill is determined to have been impaired."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-structure",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-structure",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Capital structure",
    "text": "Capital structure\nA firm’s capital structure must be able to support management’s strategic objectives as well as to allow the firm to honor its future obligations."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-allocation-decisions",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#capital-allocation-decisions",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Capital allocation decisions",
    "text": "Capital allocation decisions\nConsolidated financial statements can hide the individual characteristics of dissimilar subsidiaries. As a result, firms are required to disaggregate financial information by segments to assists users.\nAccrual-based measures such as EBIT may not be a good indicator of an entity’s ability to generate cash flow. We would rather evaluate segmental capital allocation decisions based on cash flows generated by each segment. However, segmental cash flow data is generally not reported. We can, however, approximate cash flow as EBIT plus depreciation and amortization."
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#accruals-ratio",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#accruals-ratio",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Accruals ratio",
    "text": "Accruals ratio\n\nBalance sheet approach\nAccruals BS = ending NOA – beginning NOA\nNet Operating Assets (NOA) = Operating assets – Operating liabilities\nOperating assets = total assets – cash, equivalents to cash, marketable securities\nOperating liabilities = total liabilities – total debt (both short term and long term)\nAccruals ratio BS = (ending NOA – beginning NOA) / ((ending NOA + beginning NOA) / 2)\n\n\nCash flow statement approach\nAccruals CF = NI – CFO – CFI\nAccruals ratio CF = (NI – CFO – CFI) / ((ending NOA – beginning NOA) / 2)\nIn order to compare the two measures, it is necessary to eliminate cash paid for interest and taxes from operating cash flow by adding them back. This adjusted figure is the cash generated from operations (CGO).\nCGO = EBIT + non-cash charges – increase in working capital\nCGO / operating income ratio 사용함"
  },
  {
    "objectID": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#market-value-decomposition",
    "href": "posts/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis/2021-09-12-CFA-Level-2-Financial-Reporting-and-Analysis.html#market-value-decomposition",
    "title": "CFA Level 2 Financial Reporting and Analysis",
    "section": "Market value decomposition",
    "text": "Market value decomposition\nWhen a parent company has an ownership interest in an associate (subsidiary or affiliate), it may be beneficial to determine the standalone value of the parent; that is, the implied value of the parent without regard to the value of the associate. The implied value is equal to the parent’s market value less the parent’s pro-rata share of the associate’s market value. If the associate’s stock is traded on a foreign stock exchange, it may be necessary to convert the market value of the associate to the parent’s reporting currency."
  },
  {
    "objectID": "posts/2021-11-06-digital-system-circuits-week-9/2021-11-06-digital-system-circuits-week-9.html",
    "href": "posts/2021-11-06-digital-system-circuits-week-9/2021-11-06-digital-system-circuits-week-9.html",
    "title": "Digital System Circuits Week 9",
    "section": "",
    "text": "image-20211106052440812\n\n\n\n\n\n\n\n\nimage-20211106053105389\n\n\nfeedback은 안정하거나, 진동하는 결과 2개를 가진다.\n\n\n\nimage-20211106053518574\n\n\n짝수 inverter - 중간에서 일정한 값을 계속 추출 할 수 있음 / 홀수 inverter - 오실레이터, 진동\nor gate는 1 bit storage를 만들 수 있음 / and gate는 1회용 bit storage를 만들 수 있음\n\n\n\n\n\n\nimage-20211106054204566\n\n\ncross coupled NOR gates\n둘 다 1인 상태만 피하면 됨\n\n\n\nimage-20211106054213870\n\n\n\n\n\n\n\n\nimage-20211106063404373\n\n\n\n\n\n\n\n\nimage-20211106063346175\n\n\n\n\n\n\n\n\nimage-20211106063510324\n\n\n\n\n\n\n\n\nimage-20211106063703927\n\n\n\n\n\n\n\n\nimage-20211106064317364\n\n\nDataflow - RTL\n\n\n\n\n\n\nimage-20211106064327637\n\n\n\n\n\n\n\n\nimage-20211106064338310\n\n\n시간제한이 있다거나 하는 특별한 상황에서 사용\n\n\n\n\n\n\nimage-20211106064350967\n\n\n논리식\n\n\n\n\n\n\nimage-20211106064400540\n\n\n\n\n\n\n\n\nimage-20211106064413101"
  },
  {
    "objectID": "posts/2021-10-23-probability-and-inferential-statistics-week8/2021-10-23-probability-and-inferential-statistics-week8.html",
    "href": "posts/2021-10-23-probability-and-inferential-statistics-week8/2021-10-23-probability-and-inferential-statistics-week8.html",
    "title": "Probability and Inferential Statistics Week 8",
    "section": "",
    "text": "probability_and_inferential_statistics_week_8_Page_1\n\n\n\n\n\nprobability_and_inferential_statistics_week_8_Page_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_8_Page_3\n\n\n\n\n\nprobability_and_inferential_statistics_week_8_Page_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_8_Page_5\n\n\n\n\n\nprobability_and_inferential_statistics_week_8_Page_6"
  },
  {
    "objectID": "posts/2021-10-06-tiny-python-project-chapter-9~10/2021-10-06-tiny-python-project-chapter-9~10.html",
    "href": "posts/2021-10-06-tiny-python-project-chapter-9~10/2021-10-06-tiny-python-project-chapter-9~10.html",
    "title": "Tiny Python Project Chapter 9-10",
    "section": "",
    "text": "image-20211006195135195\n\n\n\n\n\nimage-20211006195142583\n\n\n\n\n\nimage-20211006195526288\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@9c4e508fd6f6>\nDate   : 2021-10-06\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Heap abuse',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-a',\n                        '--adjectives',\n                        help='Number of adjectives',\n                        metavar='adjectives',\n                        type=int,\n                        default=2)\n\n    parser.add_argument('-n',\n                        '--number',\n                        help='Number of insults',\n                        metavar='insults',\n                        type=int,\n                        default=3)\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211006195554974\n\n\n\n\n\nimage-20211006195618918\n\n\n\n\n\nimage-20211006195628839\n\n\n\n\n\nimage-20211006195904804\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@9c4e508fd6f6>\nDate   : 2021-10-06\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport random\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Heap abuse',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-a',\n                        '--adjectives',\n                        help='Number of adjectives',\n                        metavar='adjectives',\n                        type=int,\n                        default=2)\n\n    parser.add_argument('-n',\n                        '--number',\n                        help='Number of insults',\n                        metavar='insults',\n                        type=int,\n                        default=3)\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    args = parser.parse_args()\n\n    if args.adjectives < 1:\n        parser.error(f'--adjectives \"{args.adjectives}\" must be > 0')\n    \n    if args.insults < 1:\n        parser.error(f'--number \"{args.insults}\" must be > 0')\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211006200901482\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@9c4e508fd6f6>\nDate   : 2021-10-06\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport random\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Heap abuse',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-a',\n                        '--adjectives',\n                        help='Number of adjectives',\n                        metavar='adjectives',\n                        type=int,\n                        default=2)\n\n    parser.add_argument('-n',\n                        '--number',\n                        help='Number of insults',\n                        metavar='insults',\n                        type=int,\n                        default=3)\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    args = parser.parse_args()\n\n    if args.adjectives < 1:\n        parser.error(f'--adjectives \"{args.adjectives}\" must be > 0')\n    elif args.number < 1:\n        parser.error(f'--number \"{args.number}\" must be > 0')\n    \n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n\n    adjectives = 'bankrupt base caterwauling corrupt cullionly detestable dishonest false filthsome filthy foolish foul gross heedless indistinguishable infected insatiate irksome lascivious lecherous loathsome lubbery old peevish rascaly rotten ruinous scurilous scurvy slanderous sodden-witted thin-faced toad-spotted unmannered vile wall-eyed'.split()\n\n    nouns = 'Judas Satan ape ass barbermonger beggar block boy braggart butt carbuncle coward coxcomb cur dandy degenerate fiend fishmonger fool gull harpy jack jolthead knave liar lunatic maw milksop minion ratcatcher recreant rogue scold slave swine traitor varlet villain worm'.split()\n\n    num_adjs = args.adjectives\n    repeat_num = args.number\n\n    for i in range(repeat_num):\n        selected_adjs = random.sample(adjectives, num_adjs)\n        selected_noun = random.choice(nouns)\n        print('You ' + ', '.join(selected_adjs) + ' {}!'.format(selected_noun))\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211006202153543\n\n\n\n\n\nimage-20211006202206263\n\n\n\n\n\nimage-20211006202222975\n\n\n\n\n\nimage-20211006202300289\n\n\n\n\n\nimage-20211006202325055\n\n\n\n\n\nimage-20211006202347087\n\n\n\n\n\nimage-20211006202416727\n\n\n\n\n\nimage-20211006202435474\n\n\n\n\n\nimage-20211006202442993\n\n\n\n\n\nimage-20211006202450670\n\n\n\n\n\nimage-20211006202458488\n\n\n\n\n\n\n\n\nimage-20211006215241567\n\n\n\n\n\nimage-20211006220047127\n\n\n\n\n\nimage-20211006224257293\n\n\n\n\n\nimage-20211006224409652\n\n\n\n\n\nimage-20211006224437868\n\n\n\n\n\nimage-20211006224512935\n\n\n\n\n\nimage-20211006224647292\n\n\n\n\n\nimage-20211006224656619\n\n\n\n\n\nimage-20211006224705174\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@d7a07f2f85b2>\nDate   : 2021-10-06\nPurpose: Rock the Casbah\n\"\"\"\n\nimport string\nimport argparse\nimport random\nimport os\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Telephone',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='str',\n                        type=str,\n                        help='Input text or file')\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    parser.add_argument('-m',\n                        '--mutations',\n                        help='Percent mutations',\n                        metavar='mutations',\n                        type=float,\n                        default=0.1)\n\n    args = parser.parse_args()\n\n    if not (0 <= args.mutations <= 1 ):\n        parser.error(f'--mutations \"{args.mutations}\" must be between 0 and 1')\n\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read().rstrip()\n    \n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n    t = args.text\n    alpha = ''.join(sorted(string.ascii_letters + string.punctuation))\n    t_len = len(t)\n    num_mutations = round(t_len * args.mutations)\n    new_text = t\n\n    for i in random.sample(range(t_len), num_mutations):\n        new_text = new_text[:i] + random.choice(alpha.replace(new_text[i], '')) + new_text[i + 1:]\n    \n    print(f'You said: \"{t}\"\\nI heard : \"{new_text}\"')\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211006231938092"
  },
  {
    "objectID": "posts/2021-09-29-CFA-Level-2-Equity/2021-09-29-CFA-Level-2-Equity.html",
    "href": "posts/2021-09-29-CFA-Level-2-Equity/2021-09-29-CFA-Level-2-Equity.html",
    "title": "CFA Level 2 Equity",
    "section": "",
    "text": "the valuation of an asset or security by someone who has complete understanding of the characteristics of the asset or issuing firm.\n\n\n\nEfficient market theory: Intrinsic value = Market value\n\n\n\nimage-20210928150009224\n\n\n\n\n\nimage-20210928150038904\n\n\n\n\n\n\nThe going concern assumption is simply the assumption that a company will continue to operate as a business.\nThe liquidation value is the estimate of what the assets of the firm would bring if sold separately, net of the company’s liabilities.\ngoing concern value > liquidation value: value added from asset synergy\n\n\n\n\n\nFair market value is the price at which a hypothetical willing, informed, and able seller would trade an asset to a willing, informed, and able buyer.\nwell-informed, willing buyer and seller\n팔거나 살 수 있는 willingness가 중요, ability도 있어야 함\n\n\n\nfair value != fair market value, but 같을 수 있음\nvalue for financial reporting\n\n\n\nInvestment value is the value of a stock to a particular buyer. Investment value may depend on the buyer’s specific needs and expectations, as well as perceived synergies with existing buyer assets.\nvalue to specific buyer: synergy 고려\n\n\n\n\n\nstock selection - is the stock under- or overvalued?\ninferring market expectations - what does the security price say about expectations?\nEvaluating corporate events - what is the effect on firm value from a merger?\nfairness opinions - is the value paid for the firm fair?\nevaluating business strategies - what is the effect on firm value of a new strategy?\ncommunicating with analysis and shareholders - how is firm value being affected?\nappraising private businesses - what is the value of a private firm?\ncompensation - what is the value of equity compensation?\n\n\n\n\n\n\n\nunderstanding the business\nforecasting company performance\nselecting the appropriate valuation model - absolute or relative\nusing forecasts in a valuation\napplying the valuation conclusions - over or under valued?\n\n\n\n\n\nthreat of new entrants in the industry\nthreat of substitutes\nbargaining power of buyers\nbargaining power of supplier\nrivalry among existing competitors\n\n\n\n\n\ncost leadership\nproduct differentiation\nfocus - employing one of the previous strategies within a particular segment of the industry in order to gain a competitive advantage.\n\n\n\n\nimage-20210928151955656\n\n\n\n\n\n\naccelerating or premature recognition of income\n\nrecognizes revenue early using bill-and-hold sales.\n\nreclassifying gains and nonoperating income\n\ncapitalizes product development expenses\n\nexpense recognition and losses\n\nrevenue: non-operating -> operating\nexpense: operating -> non-operating\n\namortization, depreciation, and discount rates\n\nliabilities are understated\n\noff-balance-sheet issues\n\nincreases loan-loss reserves\n\n\n\n\n\n\n\n\n\n\n\nimage-20210928152423281\n\n\nOne absolute valuation approach is to determine the value of a firm today as the discounted or present value of all the cash flows expected in the future.\nAnother absolute approach to valuation is represented by asset-based models. This approach estimates a firm’s value as the sum of the market value of the assets it owns or controls. This approach is commonly used to value firms that own or control natural resources, such as oil fields, coal deposits, and other mineral claims.\n\n\n\n\n\n\nimage-20210928152430326\n\n\nAnother very common approach to valuation is to determine the value of an asset in relation to the values of other assets.\n\n\n\n\nRather than valuing a company as a single entity, an analysts can value individual parts of the firm and add them up to determine the value of the company as a whole. The value obtained is called the sum-of-the-parts value.\n\n\n\ninternal capital inefficiency\nendogenous (internal) factors\nresearch measurement errors\n\n\n\n\n\n\nWhat are the characteristics of the company?\nWhat is the availability and quality of data?\nWhat is the purpose of the valuation?\n\nThe purpose of the analysis may be valuation for making a purchase offer for a controlling interest in the company. In this case, a model based on cash flow may be more appropriate than one based on dividends because a controlling interest would allow the purchaser to set dividend policy.\n\n\n\n\n\n\n\n\nimage-20210928153839545\n\n\n\n\n\nIf the expected return is not equal to required return, there can be a “return from convergence of price to intrinsic value”.\n\n\n\nimage-20210928153936586\n\n\n만약 expected return = 20%, required rate of return = 10%이면, 이 차이에서 발생하는 convergence에서 오는 return을 price convergence라고 함\n\n\n\n\nequity risk premium = required return on equity index - risk-free rate\n\n\nA historical estimate of the equity risk premium consists of the difference between the historical mean return for a broad-based equity-market index and a risk-free rate over a given time period.\nA weakness of the approach is the assumption that the mean and variance of the returns are constant over time. In fact, the premium actually appears to be countercyclical-it is low during good times and high during bad times.\nThe historical estimate can also be upward biased if only firms that have survived during the period of measurement (called survivorship bias) are included in the sample.\nThe risk premium is always be lower when the geometric mean is used instead of the arithmetic mean. If the yield curve is upward sloping, the use of longer-term bonds rather than shorter-term bonds to estimate the risk-free rate will cause the estimated risk premium to be smaller.\n\n\n\n\n\n\n\n\nimage-20210928155312172\n\n\nA weakness of the approach is that the forward-looking estimates will change through time and need to be updated.\nAnother weakness is the assumption of a stable growth rate, which is often not appropriate in rapidly growing economies.\n\n\n\nimage-20210928161607976\n\n\n\n\n\n\n\n\nimage-20210928155423600\n\n\nMacroeconomic model estimates of the equity risk premium are based on the relationships between macroeconomic variables and financial variables.\nbreak-even inflation rate = (1 + YTM of 20-year T-bonds) / (1 + YTM of 20-year TIPS) - 1\nExpected real growth rate in EPS should be approximately equal to the real GDP growth rate. Growth in GDP can be estimated as the sum of labor productivity growth and growth in the labor supply.\n\n\n\nimage-20210928155637456\n\n\nIf the market is believed to be overvalued (\\(\\hat{PEg} < 0\\)), P/E ratios would be expected to decrease and the opposite would be true if the market were believed to be undervalued (\\(\\hat{PEg} > 0\\)). If the market is correctly priced, \\(\\hat{PEg} = 0\\). The \\(\\hat{Y}\\) can be estimated using estimated dividends on the index including reinvestment return.\n\n\n\nimage-20210928161620305\n\n\n\n\n\n\n\n\n\n\\[\n\\\\\\operatorname{required rate on stock} j = \\operatorname{risk-free\\, rate} + \\operatorname{equity risk premium} * \\operatorname{beta of} j\n\\]\n\n\n\n\\[\n\\\\\\operatorname{required return of stock}j = RF + \\beta_{mkt, j} * (R_{mkt} - RF) + \\beta_{SMB, j} * (R_{small} - R_{big}) + \\beta_{HML, j} * (R_{HBM} - R_{LBM})\n\\]\n\\(R_{mkt} - RF\\) = return on a value-weighted market index minus the risk-free rate\n\\(R_{small} - R_{big}\\) = a small-cap return premium equal to the average return on small-cap portfolios minus the average return on large-cap portfolios\n\\(R_{HBM} - R_{LBM}\\) = a value return premium equal to the average return on high book-to-market portfolios minus the average return on low book-to-market portfolios\nThe baseline value (i.e., the expected value for the variable) for \\(\\beta_{mkt, j}\\) is one, and the baseline values for \\(\\beta_{SMB, j}\\) and \\(\\beta_{HML, j}\\) are zero.\n\n\n\nThe Pastor-Stambaugh model adds a liquidity factor to the Fama-French model. The baseline value for the liquidity factor beta is zero. \\[\n\\\\\\operatorname{required return of stock}j = RF + \\beta_{mkt, j} * (R_{mkt} - RF) + \\beta_{SMB, j} * (R_{small} - R_{big}) + \\beta_{HML, j} * (R_{HBM} - R_{LBM}) + \\beta_{liq, j} * (R_{illiq} - R_{liq})\n\\]\n\n\n\nMacroeconomic multifactor models use factors associated with economic variables that can be reasonably believed to affect cash flows and/or appropriate discount rates. The Burmeister, Roll, and Ross model incorporates the following five factors:\n\nConfidence risk - unexpected changes in the difference between the return of risky corporate bonds and government bonds.\nTime horizon risk - unexpected change in the difference between the return of long-term government bonds and Treasury bills.\nInflation risk - unexpected changes in the inflation rate.\nBusiness cycle risk - unexpected change in the level of real business activity.\nMarket timing risk - the equity market return that is not explained by the other four factors.\n\n\n\n\nimage-20210928165331662\n\n\n\n\n\n\\[\n\\\\\\operatorname{required return} = RF + \\operatorname{equity risk premium} + \\operatorname{size premium} + \\operatorname{specific-company\\, premium}\n\\]\nThe formula could have a factor for the level of controlling versus minority interests and a factor for marketability of the equity; however, these latter two factors are usually used to adjust the value of the company directly rather than through the required return.\nprivate company valuation formula 사용\nother risk premium (specific-company premium): marketability / control 반영되는 부분인데 risk premium보다는 Cashflow (value)에 직접 영향 - DCF 할 때 CF에 영향\n\n\n\nThe method simply adds a risk premium to the yield to maturity (YTM) of the company’s long-term debt. The logic here is that the yield to maturity of the company’s bonds includes the effects of inflation, leverage, and the firm’s sensitivity to the business cycle. That value is usually estimated at 3-5%, with the specific estimate based upon some model or simply from experience.\n\n\n\n\n\n\nPopular choices for the index include the S&P 500 and the NYSE Composite. The most common length and frequency are five years of monthly data.\n\n\n\nWhen making forecasts of the equity risk premium, some analysts recommend adjusting the beta for beta drift. Beta drift refers to the observed tendency of an estimated beta to revert to a value of 1.0 over time. To compensate, the Blume method can be used to adjust the beta estimate: \\[\n\\\\\\operatorname{adjusted beta} = (2 / 3 * \\operatorname{regression beta}) + (1 / 3 * 1.0)\n\\]\n\n\n\n\nIdentify a benchmark company, which is publicly traded and similar to target company’s operations.\nEstimate the beta of that benchmark company. This can be done with a regression analysis.\nUnlever the beta estimate \\[\n\\\\\\operatorname{unlevered beta of benchmark company} = \\operatorname{beta of benchmark company} * \\frac{1}{1 + \\frac{\\operatorname{debt of benchmark company}}{\\operatorname{equity of benchmark company}}}\n\\] {b}\nLever up the unlevered beta \\[\n\\\\\\operatorname{estimate of beta for target company} = \\operatorname{unlevered beta of benchmark company} * (1 + \\frac{\\operatorname{debt of target company}}{\\operatorname{equity of target company}})\n\\]\n\n\n\n\n\n\n\n\nimage-20210928165532985\n\n\n\n\n\nInternational investment, if not hedged, exposes the investor to exchange rate risk.\n\n\nOne method for adjusting data from emerging markets is to use a corresponding developed market as a benchmark and add a premium for the emerging market. \\[\n\\\\\\operatorname{Emerging ERP} = \\operatorname{Developed equity risk premium} + \\operatorname{Country premium}\n\\\\\\operatorname{Country premium} = \\operatorname{Emerging market bond yield} - \\operatorname{developed market bond yield}\n\\]\n\n\n\nA second method is the country risk rating model. This model estimates a regression equation using the equity risk premium for developed countries as the depended variables and risk ratings for those countries as the independent variable. The model is then used for predicting the equity risk premium for emerging markets using the emerging markets risk-ratings.\nDeveloped market의 risk rating (independent variable)과 ERP (dependent variable)을 regression analysis -> emerging market의 risk rating을 대입하여 ERP 추정\n\n\n\n\n\\[\n\\\\\\operatorname{WACC} = \\frac{\\operatorname{market value of debt}}{\\operatorname{market value of debt and equity}} * r_{d} * (1 - t) + \\frac{\\operatorname{market value of equity}}{\\operatorname{market value of debt and equity}} * r_{e}\n\\]\nthe tax rate should be the marginal tax rate.\nthe WACC calculation should use the target weights for debt and equity.\n\n\n\n\nCash flows to the firm - WACC\nCash flows to equity - required rate on equity\nNominal cash flows - nominal discount rates\nReal cash flows - real discount rates\n\n\n\n\nBottom-up analysis starts with analysis of an individual company or its reportable segments.\nTop-down analysis begins with expectations about a macroeconomic variable, often the expected growth rate of nominal GDP.\nA hybrid analysis incorporates elements of both top-down and bottom-up analysis. A hybrid analysis can highlights any inconsistencies in assumptions between the top-down and bottom-up approaches. A hybrid analysis is the most common type.\n\n\n\n\n\n\nimage-20210928171050936\n\n\n\n\n\nIf the average cost of production decreases as industry sales increase, we say that the industry exhibits economies of scale.\nEconomies of scale are observed when larger companies in an industry have larger margins.\n\n\n\n\n\nBecause cost of goods sold is closely related to revenue, future COGS is usually estimated as a percentage of future revenue: \\[\n\\\\\\operatorname{forecast COGS} = \\operatorname{historical COGS} / \\operatorname{revenue} * \\operatorname{estimate of future revenue}\n\\\\\\operatorname{or}\n\\\\\\operatorname{forecast COGS} = (1 - \\operatorname{gross margin}) * \\operatorname{estimate of future revenue}\n\\] It can be worthwhile to examine the gross margins of a firm’s competitor in the market as a check of the reasonableness of future gross margin estimates. In some cases, differences between firms’ business models may be the underlying reason for differences in gross margins.\nA closer examination of the volume and price of a firm’s inputs may improve the quality of a forecast of COGS, especially in the short run. An analyst must be aware of the proportion of future input costs hedged in this way or, at a minimum, whether the firm has historically hedged these costs and over what horizon.\nEstimates of a firm’s COGS may also be improved by forecasting COGS for the firm’s various product categories and business segments separately.\n\n\n\nCompared to COGS, SG&A operating expenses are less sensitive to changes in sales volume; SG&A fixed cost component is generally greater than its variable cost component.\nSelling and distribution costs, on the other hand, may be more directly related to sales volumes, because it is likely that more salespeople will be hired to support higher firm sales. If a firm’s financial statements break out the components of SG&A separately, the different components can be considered separately to improve the overall forecast of SG&A expenses.\n\n\n\nThe primary determinants of gross interest expense are the level of (gross) debt and market interest rates.\nNet debt is gross debt minus cash, cash equivalents, and short-term securities.\nNet interest expense is gross interest expense minus interest income on cash and short-term debt securities.\n\n\n\n\nstatutory rate\neffective tax rate\ncash tax rate\n\n\n\n\nimage-20210928172318121\n\n\n\n\n\n\nNet income less dividends declared will flow through to retained earnings. Working capital items can be forecast based on their historical relationship with income statement items.\nThe forecasted annual COGS divided by the inventory turnover ratio can be used to forecast an inventory value for the balance sheet that is consistent with income statement projections of COGS. \\[\n\\\\\\operatorname{projected accounts receivable} = \\operatorname{days sales outstanding} * \\operatorname{forecasted sales} / 365\n\\] Working capital items will increase at the same rate as revenues.\nProperty, plant and equipment (PP&E) on the balance sheet is determined by depreciation and capital expenditures (capex).\nForecasts may also be improved by analyzing capital expenditures for maintenance separately from capital expenditure for growth. Historical depreciation should be increased by the inflation rate when estimating capital expenditure for maintenance because replacement cost can be expected to increase with inflation.\n\n\n\nimage-20210928172921226\n\n\n\n\n\nimage-20210928172926084\n\n\n\n\n\nOnce financial projections are completed, the return on invested capital (ROIC) can be calculated. While analysis use varying definitions of ROIC, it can be thought of as net operating profit projected for taxes (NOPLAT) divided by invested capital (operating assets minus operating liabilities). ROIC is a return to both equity and debt and is preferable to return on equity (ROE) in some contexts because it allows (relative to their peers) are likely exploiting some competitive advantage in the production and/or sale of their products.\nA related measure, return on capital employed, is similar to ROIC but uses pretax operating earnings in the numerator to facilitate comparison between companies that face different tax rates.\n\n\n\nimage-20210928173304640\n\n\n\n\n\n\n\n\nA firm’s future competitive success is possibly the most important factor in determining future revenue and profitability.\n\n\n\nthreat of substitute products\nintensity of industry rivalry\nbargaining power of suppliers\nbargaining power of customers\nthreat of new entrants\n\n\n\n\n\n\n\n\nimage-20210928173651908\n\n\n가격 인상 시 demand elasticity에 대한 고려 필요 - 탄력적이면 price 상승 시 revenue 하락, 비탄력적이면 price 상승 시 revenue 하락이 덜함\nprice-volume trade-off: inflation 상황에서 가격 인상이 늦으면 profit margin이 낮아짐, 빠르면 profit margin은 유지되지만 volume이 내려갈 수 있음\nCompanies with commodity-type inputs can hedge their exposure to changes in input prices through derivatives or, more simply, fixed-price contracts for future delivery. - hedge\nCompanies that are vertically integrated (and are in effect their own suppliers) will be less subject to the effects of variations in input prices. - vertical integration\nFor a company that neither hedges input price exposure nor is vertically integrated the issue for the analyst is to determine how rapidly, and to what extent, the increase in costs can be passed on to customers, as well as the expected effect of price increases on sales volume and sales revenue.\nThe effects of increasing a product’s price depend on the product’s elasticity of demand. For most firms, product demand is relatively elastic. With elastic demand, the percentage reduction in unit sales is greater than the percentage increase in price, and a price increase will decrease total sales revenue.\nThe elasticity of demand is most affected by the availability of substitute products. In a competitive industry, the pricing decisions of other firms in the industry can affect the market shares of all firms in an industry.\nFirms that are too quick to increase prices will experience declining sales volumes, though firms that are slow to increase prices will experience declining gross margins.\n\n\n\nSome advances in technology decrease costs of production, which will increase profit margins (at least for early adopters), and, over time, increase industry supply and unit sales as well.\nOther advances in technology will result in either improved substitutes or wholly new products. Some technological advances can disrupt not only markets but entire industries. One way for an analyst to model the introduction of new substitutes for a company’s products is to estimate a cannibalization factor, which is the percentage of new product sales that will replace existing product sales. \\[\n\\\\\\operatorname{cannibalization rate} = \\frac{\\operatorname{new product sales that replace existing product sales}}{\\operatorname{total new product sales}}\n\\]\n\n\n\n\n\n\nimage-20210928175154104\n\n\n\n\n\nAn analyst will typically value a stock using the earnings or some measures of cash flow over a forecast period, along with the stock’s terminal value at the end of the forecast horizon. This terminal value is usually estimated using either a relative valuation approach or a discounted cash flow approach.\nWhen using a multiples approach, an analyst must ensure that the multiple used is consistent with the estimate of the company’s growth rate and required rate of return.\nWhen using a discounted cash flow approach to estimate the terminal value, two key inputs are a cash flow or earnings measure and an expected future growth rate.\n\n\n\nSteps in developing a sales-based pro forma model:\n\nEstimate revenue growth and future expected revenue\nEstimate COGS\nEstimate SG&A\nEstimate financing costs\nEstimate income tax expense and cash taxes\nEstimate cash taxes, taking into account changes in deferred tax items\nModel the balance sheet based on items that flow from the income statement\nUse depreciation and capital expenditures (for maintenance and for growth) to estimate capital expenditures and net PP&E for the balance sheet\nUse the completed pro forma income statement and balance sheet to construct a pro forma cash flow statement\n\n\n\n\n \\[\n\\\\FCFF = NI + NCC + Interest * (1 - t) - WC_{I} - FC_{I}\n\\\\FCFE = FCFF - Interest * (1 - t) + \\operatorname{Net Borrowing}\n\\]\n\n\n\nDividend discount model\n\nhistory of dividend payments\ndividends related to earnings\nnoncontrolling perspective\n\nFree cash flow model\n\nsmall or zero dividends\npositive cash flow related to earning\ncontrolling perspective\n\nResidual income model\n\nsmall or zero dividends\nnegative free cash flow\nhigh quality accounting disclosures\n\n\n\n\n\n\n\n\n\\[\n\\\\V_{0} = \\frac{D_{1} + P_{1}}{1 + r}\n\\]\n\n\n\n\\[\n\\\\V_{0} = \\frac{D_{1}}{(1 + r)} + \\frac{D_{2} + P_{2}}{(1 + r)^{2}}\n\\]\n\n\n\n\\[\n\\\\V_{0} = \\sum_{t = 1}^{\\infty}\\frac{D_{t}}{(1 + r)^{t}}\n\\]\n\n\n\n\nThe GGM assumes that dividends increase at a constant rate indefinitely.\n\n\n\\[\n\\\\V_{0} = \\frac{D_{1}}{r - g}\n\\]\nThe model assumes that:\n\nThe firm expects to pay a dividend, \\(D_{1}\\), in one year.\nDividends grow indefinitely at a constant rate, g (which may be less than zero).\nThe growth rate, g, is less than the required return, r.\n\nIt is unrealistic to assume that any firm can continue to grow indefinitely at a rate higher than the long-term growth rate in real gross domestic product (GDP) plus the long-term inflation rate.\n\n\n\n\n\n\n\nimage-20210928195238185\n\n\n\n\n\nstock value - 1) no growth rate, 2) PVGO \\[\n\\\\V_{0} = \\frac{E_{1}}{r} + PVGO\n\\] This means the value of a firm’s equity has two components:\n\nThe value of its assets in place \\((E_{1}/r)\\), which is the present value of a perpetual cash flow of \\(E_{1}\\).\nThe present value of its future investment opportunities (PVGO).\n\n\n\n\n\\[\n\\operatorname{justified\\, leading\\, P/E} = \\frac{P_{0}}{E_{1}} = \\frac{D_{1} / E_{1}}{r - g} = \\frac{1 - b}{r - g}\n\\\\\\\\\\operatorname{justified\\, trailing\\, P/E} = \\frac{P_{0}}{E_{0}} = \\frac{D_{0} * (1 + g) / E_{0}}{r - g} = \\frac{(1 - b) * (1 + g)}{r - g} = \\operatorname{justified\\, leading\\, P/E} * (1 + g)\n\\\\b = \\operatorname{retention ratio}\n\\]\n\n\n\n\\[\n\\\\\\operatorname{value of perpetual preferred shares} = \\frac{D_{p}}{r_{p}}\n\\]\n\n\n\n\n\n\nsimple and applicable to stable, mature firms\ncan be applied to entire markets\ng can be estimated using macro data\ncan be applied to firms that repurchase stock\n\n\n\n\n\nnot applicable to non-dividend-paying firms\ng must be constant\nstock value is very sensitive to \\(r - g\\)\nmost firms have nonconstant growth in dividends.\n\n\n\n\n\n\n\nThe growth rate starts out high and then decline linearly over the high-growth stage until it reaches the long-run average growth rate.\n\n\n\nimage-20210928200508045\n\n\n\n\n\n\n\ninitial growth phase\ntransition phase\nmature phase\n\n\n\n\nimage-20210928200600541\n\n\n\n\n\n\n\n\n\n\n\n\ngrowth phase\n\n\n\n\n\n\nvariable\ninitial growth\ntransition\nmaturity\n\n\nearnings growth\nvery high\nabove average but falling\nstable at long-run level\n\n\ncapital investment\nsignificant requirements\ndecreasing\nstable at long-run level\n\n\nprofit margin\nhigh\nabove average but falling\nstable at long-run level\n\n\nFCFE\nnegative\nmay be positive, and growing\nstable at long-run level\n\n\nROE vs. Required Return\nROE > r\nROE approaching r\nROE = r\n\n\nDividend payout\nlow or zero\nincreasing\nstable at long-run level\n\n\nappropriate model\nthree-stage\ntwo-stage\nGordon growth (single)\n\n\n\n\n\n\n\\[\n\\\\\\operatorname{DCF: }TV_{10} = \\frac{D_{11}}{r - g}\n\\\\\\operatorname{Multiple: }TV_{10} = \\frac{P}{E_{10}} * E_{10}\\, or\\, \\frac{P}{E_{11}} * E_{11}\n\\]\n\n\n\n\n\n\\[\n\\\\V_{0} = \\frac{D_{0} * (1 + g_{L})}{r - g_{L}} + \\frac{D_{0} * H * (g_{S} - g_{L})}{r - g_{L}}\n\\\\H = \\frac{t}{2} = \\operatorname{half-life\\, (in\\, years)\\, of\\, high-growth\\, period}\n\\\\g_{S} = \\operatorname{short-term\\, growth\\, rate}\n\\\\g_{L} = \\operatorname{long-term\\, growth\\, rate}\n\\]\n\n\n\n\n\n\n\nimage-20210928205937750\n\n\n\n\n\nimage-20210928205950664\n\n\n\n\n\n\n\n\n\\[\n\\\\g = b * ROE\n\\\\ROE = \\frac{\\operatorname{Net Income}}{Sales} * \\frac{Sales}{\\operatorname{Total assets}}*\\frac{\\operatorname{Total assets}}{Equity} = \\operatorname{Profit Margin} * \\operatorname{Total asset turnover} * \\operatorname{Leverage ratio}\n\\\\g = \\frac{\\operatorname{Net Income} - Dividends}{\\operatorname{Net Income}} * \\frac{\\operatorname{Net Income}}{Sales} * \\frac{Sales}{\\operatorname{Total assets}}*\\frac{\\operatorname{Total assets}}{Equity} = b * ROA * \\frac{\\operatorname{Total assets}}{Equity}\n\\]\n\n\n\n\nMarket price > Model price -> overvalued\nMarket price = Model price -> fairly valued\nMarket price < Model price -> undervalued\n\n\n\n\n\n\n\nimage-20210928210735515\n\n\n \\[\n\\\\\\operatorname{firm value} = \\operatorname{FCFF discouunted at the WACC}\n\\\\\\operatorname{equity value} = \\operatorname{FCFE discounted at the required return on equity}\n\\\\\\operatorname{equity value} = \\operatorname{firm value} - \\operatorname{market value of debt}\n\\] FCFE is easier and more straightforward to use in cases where the company’s capital structure is not particularly volatile. On the other hand, if a company has negative FCFE and significant debt outstanding, FCFF is generally the best choice.\n\n\n\nThe ownership perspective in the free cash flow approach is that of an acquirer who can change the firm’s dividend policy, which is a control perspective, or for minority shareholders of a company that is in-play. The ownership perspective implicit in the dividend discount approach is that of a minority owner who has no direct control over the firm’s dividend policy.\nAnalysts often prefer to use free cash flow rather than dividend-based valuation for the following reasons:\n\nmany firms pay no, or low, cash dividends\nDividends are paid at the discretion of the board of directors. It may consequently, be poorly aligned with the firm’s long-run profitability.\nIf a company is vied as an acquisition target, free cash flow is a more appropriate measure because the new owner will have discretion over tis distribution (control perspective).\nFree cash flows may be more related to long-run profitability of the firm as compared to dividends.\n\n\n\n\n\n\n\\[\n\\\\FCFF = NI + NCC + Interest * (1 - t) - FCInv - WCInv\n\\]\n\n\n\n\n\nimage-20210928221457064\n\n\nThe most significant noncash charge is usually depreciation.\n\nAmortization of intangibles should be added back to net income, much like depreciation.\nProvisions for restructuring charges and other noncash losses should be added back to net income. However, if the firm is accruing these costs to cover future cash outflows, then the forecast of future free cash flow should be reduced accordingly. Gains or losses on sale of long-term assets are also removed (they would be accounted for under fixed capital investment).\nIncome from restructuring charge reversals and other noncash gains should be subtracted from net income.\nFor a bond issuer, the amortization of a bond discount should be added back to net income, and the accretion of the bond premium should be subtracted from net income to calculate FCFF.\nDeferred taxes, which result from differences in the timing of reporting income and expenses for accounting versus tax purposes, must be carefully analyzed. Over time, differences between book and taxable income should offset each other and have no significant effect on overall cash flows. If, however, the analyst expects deferred tax liabilities to continue to increase, increases in deferred tax liabilities should be added back to net income. Increases in deferred tax assets that are not expected to reverse should be subtracted from net income.\n\n\n\n\n\n\n\n\nimage-20210928224141856\n\n\nFixed capital investment is a net amount: it is equal to the difference between capital expenditures (investments in long-term fixed assets) and the proceeds from the sale of long-term assets: \\[\n\\\\\\text{If no long-term assets were sold during the year,}\n\\\\FCInv = \\operatorname{ending\\, net\\, PP\\&E} - \\operatorname{beginning\\, net\\, PP\\&E} + depreciation\n\\\\\\text{If long-term assets were sold during the year,}\n\\\\FCInv = \\operatorname{ending\\, net\\, PP\\&E} - \\operatorname{beginning\\, net\\, PP\\&E} + depreciation - \\operatorname{gain on sale}\n\\]\n\n\n\nThe investment in net working capital is equal to the change in working capital, excluding cash, cash equivalents, notes payable, and the current portion of long-term debt.\n\n\n\nOnly the after-tax interest cost because paying interest reduces our tax bill.\n\n\n\n\\[\n\\\\FCFF = EBIT * (1 - t) + Dep - FCInv - WCInv\n\\]\nBecause many noncash adjustments occur on the income statement below EBIT, we don’t need to adjust for them when calculating free cash flwo if we start with EBIT.\n\n\n\n\\[\n\\\\FCFF = EBITDA * (1 - t) + Dep * t - FCInv - WCInv\n\\]\n\n\n\n\\[\n\\\\FCFF = CFO + Int * (1 - t) - FCInv\n\\]\n\n\n\n\\[\n\\\\FCFE = FCFF - Int * (1 - t) + \\operatorname{net borrowing}\n\\]\n\n\n\n\\[\n\\\\FCFE = NI + NCC - FCInv - WCInv + \\operatorname{net borrowing}\n\\]\n\n\n\n\\[\n\\\\FCFE = CFO - FCInv + \\operatorname{net borrowing}\n\\]\n\n\n\nRemember to treat preferred stock just like debt, except preferred dividends are not tax deductible.\nSpecifically, any preferred dividends should be added back to the FCFF, just as after-tax interest charges are in the net income approach to generating FCFF. This approach assumes that net income is net income to common shareholders after preferred dividends have been subtracted out. The only adjustment to FCFE would be to modify net borrowing to reflect new debt borrowing and net issuance by the amount of the preferred stock.\n\n\n\n\n\n\n\nThe first method is to calculate historical free cash flow and apply a growth rate under the assumptions that growth will be constant and fundamental factors will be maintained.\nThe second method is to forecast the underlying components of free cash flow and calculate each year separately. \\[\n\\\\FCFE = NI - (1 - DR) * (FC_{I} - Dep + WC_{I})\n\\\\DR =\\frac{Debt}{Debt + Equity} = \\operatorname{Debt Ratio}\n\\]\n\n\n\nThe free cash flow to equity approach takes a control perspective that assumes that recognition of value should be immediate. Dividend discount models take a minority perspective.\n\n\n\nDividends, share repurchases, and share issues have no effect on FCFF and FCFE; changes in leverage have only a minor effect on FCFE and no effect on FCFF.\n\n\n\nimage-20210928224733024\n\n\n\n\n\nNet income is a poor proxy for FCFE. \\[\n\\\\FCFE = NI + NCC - FCInv - WCInv + \\operatorname{net borrowing}\n\\] EBITDA is a poor proxy for FCFF. \\[\n\\\\FCFF = EBITDA * (1 - t) + Dep * t - FCInv - WCInv\n\\]\n\n\n\n\n\n\\[\n\\\\\\operatorname{value of the firm} = \\frac{FCFF_{1}}{WACC - g} = \\frac{FCFF_{0} * (1 + g)}{WACC - g}\\\\\nWACC = W_{e} * r_{e} + W_{d} * r_{d} * (1 - t)\n\\\\\\operatorname{Equity value} = \\operatorname{Firm value} - \\operatorname{MV of Debt}\\\\\n\\]\n\n\n\n\\[\n\\\\\\operatorname{value of equity} = \\frac{FCFE_{1}}{r - g} = \\frac{FCFE_{0} * (1 + g)}{r - g}\n\\]\n\n\n\n\n\n\n\nSensitivity analysis shows how sensitive an analyst’s valuation results are to changes in each of a model’s input.\nThere are two major sources of error in valuation analysis:\n\nEstimating the future growth in FCFF and FCFE.\nThe chosen base years for the FCFF or FCFE growth forecasts.\n\n\n\n\n\\[\n\\\\\\operatorname{terminal value in year n} = \\operatorname{trailing\\, P/E} * \\operatorname{earnings in year n}\n\\\\\\operatorname{terminal value in year n} = \\operatorname{leading\\, P/E} * \\operatorname{forecasted\\, earnings\\, in\\, year\\, n + 1}\n\\]\n\n\n\n\n\n\nThe method of comparables values a stock based on the average price multiple of the stock of similar companies. The economic rationale for the method of comparables is the Law of One Price, which assets that two similar assets should sell at comparable price multiples.\nThe method of forecasted fundamentals values a stock based on the ratio of its value from a discounted cash flow (DCF) model to some fundamental variable. The economic rationale for the method of forecasted fundamentals is that the value used in the numerator of the justified price multiple is derived from a DCF model.\n\n\n\n\n\n\n\n\n\nA justified price multiple is what the multiple should be if the stock is fairly valued. If the actual multiple is greater than the justified price multiple, the stock is overvalued; if the actual multiple is less than the justified multiple, the stock is undervalued (all else equal).\n\n\nThere are a number of rationales for using price-to-earnings (P/E) ratio in valuation:\n\nEarnings power, as measured by earnings per share (EPS), is the primary determinant of investment value.\nThe P/E ratio is popular in the investment community.\nEmpirical research shows that P/E differences are significantly related to long-run average stock returns.\n\nP/E ratio have a number of shortcomings:\n\nEarnings can be negative, which produces a meaningless P/E ratio.\nThe volatile, transitory portion of earnings makes the interpretation of P/Es diffciult for analysts.\nManagement discretion within allowed accounting practices can distort reported earnings, and thereby lessen the comparability of P/Es across firms.\n\n \\[\n\\\\\\operatorname{trailing\\, P/E} = \\frac{\\operatorname{market price per share}}{EPS over previous 12 months}\n\\\\\\operatorname{leading\\, P/E} = \\frac{\\operatorname{market price per share}}{\\operatorname{forecasted EPS over next 12 months}}\n\\] Trailing P/E is not useful for forecasting and valuation if the firm’s business has changed. Leading P/E may not be relevant if earnings are sufficiently volatile so that next year’s earnings are not forecastable with any degree of accuracy.\n\n\n\n\n\n\nimage-20210929011159080\n\n\nAdvantages of using the price-to-book (P/B) ratio include:\n\nBook value is a cumulative amount that is usually positive, even when the firm reports a loss and EPS is negative. Thus, a P/B can typically be used when P/E cannot.\nBook value is more stable than EPS, so it may be more useful than P/E when EPS is particularly high, low, or volatile.\nBook value is an appropriate measure of net asset value for firms that primarily hold liquid assets. Examples include finance, investment, insurance, and banking firms.\nP/B can be useful in valuing companies that are expected to go out of business.\nEmpirical research shows that P/Bs help explain differences in long-run average stock return.\n\nDisadvantages of using P/B include:\n\nP/Bs do not reflect the value of intangible economic assets, such as human capital.\nP/Bs can be misleading when there are significant differences in the asset size of the firms under consideration because in some cases the firm’s business model dictates the size of its asset base.\nDifferent accounting conventions can obscure the true investment in the firm made by shareholders, which reduces the comparability of P/Bs across firms and countries.\nInflation and technological change can cause the book and market values of assets to differ significantly, so book value is not an accurate measure of the value of shareholders’ investment.\n\n\\[\n\\\\\\operatorname{P/B\\, ratio} = \\frac{\\operatorname{market value of equity}}{\\operatorname{book value of equity}} = \\frac{\\operatorname{market price per share}}{\\operatorname{book value per share}}\n\\\\\\operatorname{book value of equity} = \\operatorname{common\\, shareholders'\\, equity} = \\operatorname{total assets} - \\operatorname{total liabilities} - \\operatorname{preferred stock}\\\\\n\\]\nA common adjustment is to use tangible book value, which is equal to book value of equity less intangible assets. Balance sheets should be adjusted for significant off-balance-sheet assets and liabilities and for differences between the fair and recorded value of assets and liabilities. Finally, book values often need to be adjusted to ensure comparability.\n\n\n\nThe advantages of using the price-to-sales (P/S) ratio include:\n\nSales revenue is always positive.\nSales revenue is not as easy to manipulate.\nP/S ratios are not as volatile as P/E multiples.\nP/S ratios are particularly appropriate for valuing stocks in mature or cyclical industries and start-up companies with no record of earnings.\nLike P/E and P/B ratios, empirical research finds that differences in P/S are significantly related to differences in long-run average stock returns.\n\nThe disadvantages of using P/S ratios include:\n\nHigh growth in sales does not necessarily indicate high operating profits as measured by earnings and cash flow.\nP/S ratios do not capture differences in cost structures across companies.\nWhile less subject to distortion, revenue recognition practices can still distort sales forecasts.\n\n\\[\n\\\\\\operatorname{P/S\\, ratio} = \\frac{\\operatorname{market value of equity}}{\\operatorname{total sales}} = \\frac{\\operatorname{market price per share}}{\\operatorname{sales per share}}\n\\]\n\n\n\nAdvantages of using the price-to-cash flow (P/CF) ratio include:\n\nCash flow is harder for managers to manipulate than earnings.\nPrice to cash flow is more stable than price to earnings.\nReliance on cash flow rather than earnings handles the problem of differences in the quality of reported earnings, which is problem for P/E.\nEmpirical evidence indicates that differences in price to cash flow are significantly related to differences in long-run average stock returns.\n\nThere are two drawbacks to the price-to-cash flow ratio:\n\nItems affecting actual cash flow from operations are ignored when the EPs plus noncash charges estimate is used. For example, noncash revenue and net changes in working capital are ignored.\nFrom a theoretical perspective, free cash flow to equity (FCFE) is preferable to operating cash flow. However, FCFE is more volatile than operating cash flow, so it is not necessarily more informative.\n\n\n\n\nAdvantages of the dividend yield approach include:\n\nDividend yield contributes to total investment return.\nDividends are not as risky as the capital appreciation component of total return.\n\nDisadvantages of the dividend yield approach include:\n\nThe focus on dividend yield is incomplete because it ignores capital appreciation.\nThe dividend displacement of earnings concept argues that dividends paid now displace future earnings, which implies a trade-off between current and future cash flows.\n\n\n\n\nimage-20210929011213448\n\n\n\n\n\n\n\n\nUnderlying earnings are earnings that exclude nonrecurring components.\nThe countercyclical tendency to have high P/Es due to lower EPS at the bottom of the cycle and low P/Es due to high EPS at the top of the cycle is known as the Molodovsky effect.\n\n\n\nimage-20210929011500247\n\n\n\n\n\nNormalized (or normal) earnings per share, which is an estimate of EPS in the middle of the business cycle.\n\nUnder the method of historical average EPS, the normalized EPS is estimated as the average EPS over some recent period, usually the most recent business cycle.\nUnder the method of average return on equity, normalized EPS is estimated as the average return on equity (ROE) multiplied by the current book value per share (BVPS). Once again, average ROE is often measured over the most recent business cycle. The reliance on BVPS reflects the effect of firms size changes more accurately than does the method of historical average EPS.\n\nThe method of historical average EPS ignores size effects, so the method of average ROE is preferred.\n\n\n\n\nNegative earnings render P/E ratios meaningless.\n\n\nA high E/P suggests a cheap security, and a low E/P suggests an expensive security.\n\n\n\n\n\n\n\n\\[\n\\\\\\operatorname{Leading\\, P_{0}/E_{1}} = \\frac{D_{1} / E_{1}}{r - g} = \\frac{1 - b}{r - g}\n\\\\\\operatorname{Trailing\\, P_{0}/E_{0}} = \\frac{D_{0} / E_{0} * (1 + g)}{r - g} = \\frac{(1 - b)(1 + g)}{r - g}\n\\\\\\operatorname{Justified\\, P/B} = \\frac{ROE(1 - b)}{r - g} = \\frac{ROE - ROE*b}{r - g} = \\frac{ROE - g}{r - g}\n\\\\\\operatorname{Justified\\, P/S} = \\frac{E_{0} / S_{0}(1 - b)(1 + g)} {r - g} = \\operatorname{profit margin} * \\operatorname{trailing\\, P/E}\n\\\\\\operatorname{Justified\\, P/CF} = \\frac{\\frac{FCFE_{0}(1 + g)}{r - g}}{CF}\n\\\\\\operatorname{Justified\\, EV/EBITDA} = \\frac{\\frac{FCFF_{0}(1 + g)}{WACC - g}}{EBITDA}\n\\\\\\operatorname{Justified\\, D/P} = \\frac{r - g}{1 + g}\n\\]\n\n\n\n\n\n\n\n\n\nimage-20210929013002675\n\n\nFirms with multiples below the benchmark are undervalued, and firms with multiples above the benchmark are overvalued.\n\n\n\nimage-20210929013009757\n\n\nMost analysts use trailing book values in calculating P/Bs. Relative P/B valuation must consider differences in ROE, risk, and expected growth in making comparisons among stocks.\nP/S ratios are usually calculated based on trailing sales. Analysts need to control for profit margin, expected growth, risk, and the quality of accounting data in making comparisons.\n\n\nThe Fed model considers the overall market to be overvalued (undervalued) when the earnings yield on the S&P 500 Index is lower (higher) than the yield on 10-year U.S. Treasury bonds.\nThe Yardeni model includes expected earnings growth rate in the analysis: \\[\n\\\\CEY = CBY - k * LTEG + \\epsilon_{i}\n\\\\CEY = \\text{current earnings yield of the market}\n\\\\CBY = \\text{current Moody's A-rated corporate bond yield}\n\\\\LTEG = \\text{five-year consensus earnings growth rate}\n\\\\k = \\text{constant assigned by the market to earnings growth (about 0.20 in recent years)}\n\\]\n\\[\n\\\\\\frac{P}{E} = \\frac{1}{CBY - k * LTEG}\n\\]\nP/E ratio is negatively related to interest rates and positively related to growth.\n\n\n\n\n\\[\n\\\\\\operatorname{PEG ratio} = \\frac{\\operatorname{P/E\\, ratio}}{g}\n\\]\nThe PEG is interpreted as P/E per unit of expected growth. This implied valuation rule is that stocks with lower PEGs are more attractive than stocks with higher PEGs.\nThere are a number of drawbacks to using the PEG ratio:\n\nThe relationship between P/E and g is not linear, which makes comparisons difficult.\nThe PEG ratio still doesn’t account for risk.\nThe PEG ratio doesn’t reflect the duration of the high-growth period for a multistage valuation model.\n\n\n\n\n\n\n\\[\n\\\\TV_{n} = \\operatorname{Justified}\\frac{P}{E_{n}} * EPS_{n}\\,/ TV_{n} = \\operatorname{Justified}\\frac{P}{E_{n + 1}} * EPS_{n + 1}\n\\]\n\n\n\n\\[\n\\\\TV_{n} = \\operatorname{Benchmark}\\frac{P}{E_{n}} * EPS_{n}\\, / TV_{n} = \\operatorname{Benchmark}\\frac{P}{E_{n + 1}} * EPS_{n + 1}\n\\]\n\n\n\n\nThere are at least four definitions of cash flow available for use\n\nEarnings-plus-non-cash-charges (CF) \\[\n\\\\CF = \\operatorname{net income} + depreciation + amortization\n\\]\n\nIt ignores some items that affect cash flow, such as noncash revenue and changes in net working capital.\n\nAdjusted cash flow (adjusted CFO)\n\nCFO is often adjusted for nonrecurring cash flows. US GAAP requires interest paid, interest received, and dividends received to be classified as operating cash flows. IFRS, however, is more flexible: interest paid may be classified as either an operating or financing cash flow, while interest and dividends received can be classified as either operating or investing cash flows.\n\nFree cash flow to equity (FCFE)\n\nTheory suggests that FCFE is the preferred way to define cash flow, but it is more volatile than straight cash flow. \\[\n\\\\FCFE = CFO - FCInv + \\operatorname{net borrowing}\n\\]\n\nEarnings before interest, taxes, depreciation, and amortization (EBITDA)\n\nEBITDA is a pretax, pre-interest measure that represents a flow to both equity and debt. Thus, it is better suited as an indicator of total company value than just equity value. Enterprise value-to-EBITDA ratio.\n\n\n\\[\n\\\\\\operatorname{P/CF\\, ratio} = \\frac{market value of equity}{cash flow} = \\frac{\\operatorname{market price per share}}{\\operatorname{cash flow per share}}\n\\]\n\n\n\n\n\n\\[\n\\\\EV = \\operatorname{market value of common stock} + \\operatorname{market value of preferred stock} + \\operatorname{market value of debt} + \\operatorname{minority interest} - \\operatorname{cash and investments}\n\\]\nThe rationale for subtracting cash and investments is that an acquirer’s net price paid for an acquisition target would be lower by the amount of the target’s liquid assets. \\[\n\\\\\\operatorname{EV/EBITDA\\, ratio} = \\frac{\\operatorname{enterprise value}}{EBITDA}\n\\] EV/EBITDA is useful in a number of situations:\n\nThe ratio may be more useful than P/E when comparing firms with different degrees of financial leverage.\nEBITDA is useful for valuing capital-intensive businesses with high levels of depreciation and amortization.\nEBITDA is usually positive even when EPS is not.\n\nEV/EBITDA has a number of drawbacks, however:\n\nIf working capital is growing, EBITDA will overstate CFO.\nBecause FCFF captures the amount of capital expenditures, it is more strongly linked with valuation theory than EBITDA.\n\n\n\n\nimage-20210929092305355\n\n\n\n\n\nsometimes referred to as market value of invested capital. \\[\n\\\\TIC = \\operatorname{market value of equity} + \\operatorname{market value of debt}\n\\] TIC includes cash and short-term investments.\nIn addition to EV/EBITDA and TIC/EBITDA, analysts employ enterprise value ratios with EBIT, FCFF, or other items in the denominator.\n\n\n\n\nUsing relative valuation methods that requires the use of comparable firm is challenging in an alternative context due to differences in accounting methods, cultures, risk , and growth opportunities. Further, benchmarking is difficult because P/Es for individual firms in the same industry vary widely internationally and country market P/Es can vary significantly.\nThe usefulness of all price multiples is affected to some degree by differences in international accounting standards. The lease affected is P/adjusted CFO, P/FCFE, while P/B, P/E, P/EBITDA, and EV/EBITDA will be more seriously affected because they are more influenced by management’s choice of accounting methods and estimates.\n\n\n\n\n\n\\[\n\\\\\\operatorname{earnings surprise} = \\operatorname{reported EPS} - \\operatorname{expected EPS}\n\\]\n\n\n\n\\[\n\\\\\\operatorname{Standardized Unexpected Earnings} = \\frac{\\operatorname{earnings surprise}}{\\operatorname{standard deviation of earnings surprise}}\n\\]\nA given size forecast error is more meaningful the smaller the size of the historical forecast errors.\nRelative strength indicators compare a stock’s price or return performance during a given time period with its own historical performance or with some group of peer stocks. The economic rationale is that patterns of persistence or reversal may exist in stock returns.\n\n\n\n\n\nArithmetic mean - outlier 영향 큼\nHarmonic mean - small value 영향 큼\nWeighted harmonic mean - 가장 ideal\n\n\\[\n\\\\\\operatorname{weighted harmonic mean} = \\frac{1}{\\sum_{i = 1}^{n}\\frac{w_{i}}{X_{i}}}\n\\]\n\n\n\nResidual income (RI), or economic profit, is the net income of a firm less a charge that measures stockholders’ opportunity cost of capital.\n\n\nEconomic value added (EVA) measures the value added for shareholders by management during a given year. \\[\n\\\\EVA = NOPAT - WACC * \\operatorname{total capital} = EBIT * (1 - t) - \\$WACC\\\\\n\\\\NOPAT = \\text{net operating profit after tax}\n\\] The analyst should make the following adjustments (if applicable) to the financial statements before calculating NOPAT and invested capital:\n\nCapitalize and amortize research and development charges (rather than expense them), and add them back to earnings to calculate NOPAT.\nAdd back charges on strategic investments that will generate returns in the future.\nEliminate deferred taxes and consider only cash taxes as an expense.\nTreat operating leases as capital leases and adjust nonrecurring items.\nAdd LIFO reserve to invested capital and add back change in LIFO reserve to NOPAT. (LIFO -> FIFO)\n\nMarket value added (MVA) is the different between the market value of a firm’s long-term debt and equity and the book value of instead capital supplied by investors. It measures the value created by management’s decisions since the firm’s inception. \\[\n\\\\MVA = \\operatorname{MV of firm} - \\operatorname{BV of firm}\n\\]\n\n\n\n\n\nThe measurement of managerial effectiveness\nThe measurement of goodwill impariment\nEquity valuation\n\n\n\n\n\\[\n\\\\RI_{t} = E_{t} - r * B_{t - 1} = (ROE - r) * B_{t - 1} = NI - \\$r\n\\\\\\text{Clean surplus relationship}: B_{t} = B_{t - 1} + E_{t} - D_{t}\n\\]\nThe residual income valuation model breaks the intrinsic value of a stock into two elements: 1) current book value of equity and 2) present value of expected future residual income. \\[\n\\\\V_{0} = B_{0} + (\\frac{RI_{1}}{(1 + r)^{1}} + \\frac{RI_{2}}{(1 + r)^{2}} + \\frac{RI_{3}}{(1 + r)^{3}} + ...)\n\\] Values tends to be recognized earlier in the RI approach than in other present value-based approaches. To see this, recall that with a dividend discount model (DDM) or free cash flow to equity (FCFE) model, a large portion of the estimated intrinsic value comes from the present value of the expected terminal value. Yet the uncertainty of the expected terminal value is usually greater than any of the other forecasted cash flows because it occurs several years in the future. Valuation with residual income models, however, is relatively less sensitive to terminal value estimates, which reduces forecast error.\n주주자본비용 지급 후의 ‘순액’ 개념 (주주자본비용 해당분이 \\(BV_{0}\\))\n\n\n\n\n\n\\[\n\\\\V = B_{0} + \\frac{(ROE - r) * B_{0}}{r - g} = B_{0} + \\frac{RI_{1}}{r - g}\n\\]\n\n\n\n\\[\n\\\\\\operatorname{Tobin's\\, Q} = \\frac{\\operatorname{MV of debt} + \\operatorname{MV of equity}}{\\operatorname{Replacement cost of total assets}} > 1: \\text{economic profit}\n\\]\nFundamental drivers of residual income:\n\nIf return on equity (ROE) is equal to the required return on equity, the justified market value of a share of stock is equal to its book value. When ROE is higher than the required return on equity, the firm will have positive residual income and will be valued at more than book value.\n\\(\\frac{(ROE - r) * B_0}{r - g}\\) is the additional value generated by the firm’s ability to produce returns in excess of the cost of equity and, consequently, is the present value of a firm’s expected economic profits.\n\n\n\n\n\nAmong the various market multiples, residual income models are most closely related to the price-to-book value (P/B) ratio because the justified P/B is directly linked to expected future residual income. If ROE is greater than the required return on equity, the second term (the present value of residual income) will be positive, the market value will be greater than book value, and the justified P/B ratio will be greater than one.\n\n\n\n\n\n\n\n\n\nContinuing residual income is the residual income that is expected over the long term.\nThe projected rate at which residual income is expected to fade over the life cycle of the firm is captured by a persistence factor, \\(\\omega\\), which is between zero and one.\n\nResidual income is expected to persist at its current level forever: \\(\\omega = 1\\)\nResidual income is expected to drop immediately to zero: \\(\\omega = 0\\)\nResidual income is expected to decline over time as ROE falls to the cost of equity (in which case residual income is eventually zero.\nResidual income is expected to decline to a long-run average level consistent with a mature industry.\n\nHigher persistence factors will be associated with the following:\n\nLow dividend payouts.\nHistorically high residual income persistence in the industry.\nPO가 낮으면 RR이 높고, g가 높아짐\n\nLower persistence factors will be associated with the following:\n\nHigh return on equity\nSignificant levels of nonrecurring items.\nHigh accounting accruals.\n지나치게 높으면 장기적으로 ROE가 감소\n\n\\[\n\\\\V_{0} = B_{0} + \\operatorname{PV\\, of\\, interim\\, high-growth\\, RI} + \\operatorname{PV of continuing residual income}\n\\\\\\operatorname{PV\\, of\\, continuing\\, residual\\, income\\, in\\, year\\, T-1} = \\frac{RI_{T}}{1 + r - \\omega}\n\\]\n\n\nIf \\(\\omega=1\\) \\[\n\\\\\\operatorname{PV\\, of\\, continuing\\, residual\\, income\\, in\\, year\\, T - 1} = \\frac{RI_{T}}{1 + r - \\omega} = \\frac{RI_{T}}{r}\n\\]\n\n\n\nIf \\(\\omega=0\\) \\[\n\\\\\\operatorname{PV\\, of\\, coninuing\\, residual\\, income\\, in\\, year\\, T - 1} = \\frac{RI_{T}}{1 + r - \\omega} = \\frac{RI_{T}}{1 + r}\n\\]\n\n\n\n\\[\n\\\\\\operatorname{PV\\, of\\, continuing\\, residual\\, income\\, in\\, year\\, T - 1} = \\frac{RI_{T}}{1 + r - \\omega}\n\\]\n\n\n\n\\[\n\\\\CRI_{T - 1} = \\frac{RI_{t} + (P_{T} - B_{T})}{1 + r}\n\\\\\\operatorname{PV\\, of\\, continuing\\, residual\\, income\\, in\\, year\\, T} = P_{T} - B_{T}\n\\\\P_{T} = B_{T} * \\operatorname{forecasted\\, price-to-book\\, ratio}\n\\]\nLong-run level에 도달하면 MV - BV가 RI일 것이라는 가정\n\n\n\nimage-20210929125935092\n\n\n\n\n\nimage-20210929125926654\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal value does not dominate the intrinsic value estimate, as is the case with dividend discount and free cash flow valuation models.\nResidual income models use accounting data, which is usually easy to find.\nThe models are applicable to firms that do not pay dividends or that do not have positive expected free cash flows in the short run.\nThe models are applicable even when cash flows are volatile.\nThe models focus on economic profitability rather than just on accounting profitability.\n\n\n\n\n\nThe models rely on accounting data that can be manipulated by management.\nReliance on accounting data requires numerous and significant adjustments.\nThe models assume that the clean surplus relation holds or that its failure to hold has been properly taken into account.\n\nResidual income models are appropriate under the following circumstances:\n\nA frim does not pay dividends, or the stream of payments is too volatile to be sufficiently predictabl.e\nExpected free cash flows are negative for the foreseeable future.\nThe terminal value forecast is highly uncertain, which makes dividend discount or free cash flow models less useful.\n\nResidual income models are not appropriate under the following circumstances:\n\nThe clean surplus accounting relation is violated significantly.\nThere is significantly uncertainty concerning the estimates of book value and return on equity.\n\n\n\n\n\n\n\nThe clean surplus relationship (ending book value = beginning book value + net income - dividends) may not hold when items are charged directly to shareholders’ equity and do not go through the income statement. (OCI) Therefore, we have to adjust net income to account for these items if they are not expected to reverse in the future.\n\nForeign currency translation gains and losses that flow directly to retained earnings under the current rate method.\nCertain pension adjustments.\nGains/losses on certain hedging instruments.\nChanges in revaluation surplus (IFRS only) for long-lived assets.\nChanges in the value of certain liabilities due to changes in the liability’s credit risk (IFRS only).\nChanges in the market value of debt and equity securities classified as available-for-sale (unrealized gain or loss).\n\nThe effect of violations of the clean surplus relationship is that net income is not correct, but book value is still correct.\nClean surplus violations (OCI items)의 경우 NI대신 Balance sheet의 Book value를 사용\n\n\n\nCommon adjustments to the balance sheet necessary to reflect fair value include the following:\n\nOperating leases should be capitalized by increasing assets and liabilities by the present value of the expected future operating lease payments.\nSpecial purpose entities (SPEs) whose assets and liabilities are not reflected in the financial statements of the parent company should be consolidated.\nReserves and allowances should be adjusted. For example, the allowance for bad debts, which is an offset to accounts receivables, should reflect the expected loss experience.\nInventory for companies that use LIFO should be adjusted to FIFO by adding the LIFO reserve to inventory and equity, assuming no deferred tax impact.\nThe pension asset or liability should be adjusted to reflect the funded status of the plan, which is equal to the difference between the fair value of the plan assets and the projected benefit obligations (PBO).\nDeferred tax liabilities should be eliminated and reported as equity if the liability is not expected to reserve.\n\n\n\n\n\nTwo intangible asset require special attention: 1) goodwill and 2) R&D expenditures.\nTo remove distortion, the amortization of intangibles capitalized during acquisition should be removed prior to computing the ROE used for residual income valuation.\nThe suggested analytical treatment of R&D expenditures is less definitive, but we can make the general statement that the ROE estimate for a mature company should reflect the long-term productivity of the company’s R&D expenditures: Productive R&D expenditures increase ROE and residual income, and unproductive expenditure reduce ROE and residual income.\n\n\nNonrecurring items should not be included in residual income forecasts because they represent items that are not expected to continue in the future. Items that may need adjustment in measuring recurring earnings include discounted operations, accounting changes, unusual items, extraordinary items, and restructuring charges. -> RI 계산 시 제외\nFirms may adopt other types of aggressive accounting practices that overstate the book value of assets and earnings.\n\n\n\n\nHow reliable are earning forecasts?\nAre there systematic violations of the clean surplus relation?\nDo poor quality accounting rules result in financial statements that bear no resemblance to the economic reality of the business?\n\n\n\n\n\n\n\n\n\n\n\nimage-20210929182136504\n\n\n\n\n\nStage of lifecycle: Private companies are typically less mature than public firms.\nSize: Private firms typically have less capital, fewer assets, and fewer employees than public firms and, as such, can be riskier. Accordingly, private firms are often valued using greater risk premiums and greater required returns compared to public firms.\nQuality and depth of management: Smaller private firms may not be able to attract as many qualified applicants as public firms. This may reduce the depth of management, slow growth, and increase risk at private firm.\nManagement / shareholder overlap: In most private firms, management has a substantial ownership position. In this case, external shareholders have less influence and the firm may be able to take a longer-term perspective.\nShort-term investors: Management of public firm may take a shorter-term view compared to private firms where managers are long-term holders of significant equity interests.\nQuality of financial and other information: A potential creditor or equity investor in a private firm will have less information than is available for a public firm. This leads to greater uncertainty, higher risk, and reduces private firm valuations.\nTaxes: Private firms may be more concerned with taxes than public firms.\n\n\n\n\n\nLiquidity: Private company equity typically has fewer potential owners and is less liquid than publicly traded equity. Thus, a liquidity discount is often applied in valuing privately held shares.\nRestrictions on marketability: Private companies often have agreements that prevent shareholders from selling, reducing the marketability of shares.\nConcentration of control\n\nOverall, company-specific factors can have positive or negative effects on private company valuations, whereas stock-specific factors are usually a negative. Compared to public companies, private companies haver greater heterogeneity so that the appropriate discount rates and methods for valuing them vary widely as well.\n\n\n\n\n\n\n\nimage-20210929183057053\n\n\n\n\n\nVenture capital funding\nInitial public offering (IPO)\nSale in an acquisition\nBankruptcy proceedings\nPerformance-based managerial compensation\n\n\n\n\n\nFinancial reporting: Valuation in this area are often related to goodwill impairment tests in which units of a public firm are valued using private company valuation methods. The reporting of stock-based compensation also required accurate valuation.\nTax purposes: At the firm level, transfer pricing, property taxes, and corporate restructuring may necessitate valuation. For individual equity owners, estate and gift tax issues may necessitate valuations.\n\n\n\n\nLitigation-related valuations may be required for shareholders suits, damage claims, lost profit claims, or divorce settlements.\n\n\n\n\n\n\n\nFair market value: tax reporting\n\nA hypothetical willing and able seller sells the asset to a willing and able buyer.\nAn arm’s length transaction (neither party is compelled to act) in a free market.\nA well-informed buyer and seller.\n\nMarket value: real estate and tangible asset appraisal (감정평가)\n\nAn arm’s length transactions.\nA well-informed buyer and seller.\n\nFair value: financial reporting and litigation\nInvestment value: private company sale\n\nA willing seller and buyer\nAn arm’s length transaction\nAn asset that has been marketed.\nA well-informed and prudent buyer and seller\n\nIntrinsic value: Investment analysis\n\nEffects of value definition - value의 정의가 다르면 자산가치가 다르게 산정\n\nintended purpose - 같은 비상장주식이라도 사용목적에 따라 다른 추정치\n대주주의 FMV는 경영권가치 반영, 소액주주의 Investment value는 minority/marketability discount 고려\n세무목적은 FMV, 재무보고 목적은 Fair value\n\n\n\n\n\n\nIncome approach: Values a firm as the present value of its expected future income.\nMarket approach: Values a frim using the price multiples based on recent sales of comparable assets.\nAsset-based approach: Values a firm’s assets minus its liabilities.\n\nEarly in its life, a firm’s future cash flows may be subject to so much uncertainty that an asset-based approach would be most appropriate. As the firm moves to a high growth phase, it might be appropriately valued using an income approach. A mature firm might be more appropriately valued using the market approach.\nFirm size is also a consideration in choosing a valuation methodology. Price multiples from large public firms should not be used to value a small private firm without some assurance that the risk and growth prospects of the firms are similar.\nNonoperating assets constitute a portion of firm value and must be included when valuing a firm.\n\n\n\nIn valuing a firm, the appropriate earnings definition is normalized earnings: “firm earnings if the firm were acquired.”\n\n\nNormalized earnings should exclude nonrecurring and unusual items. In the case of private firms with a concentrated control, there may be discretionary or tax-motivated expenses that need to be adjusted when calculating normalized earnings.\nArtificially low earnings may also be the result of excessively high owner compensation or of personal expenses charged to the firm. Use of company-owned assets potentially require an adjustment to earnings.\nAny real estate owned by the firm may merit treatment separate from that of firm operations for the following reasons:\n\nThe real estate may have different risk characteristics than firm operations.\nThe real estate may have different growth prospects than firm operations.\nDepreciation is most often based on historical cost and may understate the current cost in the market of the use of the assets.\n\nIf it is used in the firm’s business, a market-estimated rental expense is used in calculating or estimating earnings. The value of real estate is therefore separated from its operations and treated as a nonoperating asset. If the real estate is leased from a related party, the lease rate should be adjusted to a market rate.\nOther adjustments are common to both private and public companies. Additionally, some private firm financial statements are reviewed rather than audited; In any case, the analyst should be prepared to make further adjustments.\n\n\n\nIn a strategic transaction, valuation of the firm is based in part on the perceived synergies with the acquirer’s other assets. A financial transaction assumes no synergies, as when one firm buys another in a dissimilar industry.\n\n\n\nAlso, controlling and noncontrolling equity interests will have quite different values. These differences should be accounted for in cash flow estimates and assumptions.\nWhen there is significant uncertainty about a private company’s future operations, the analyst should examine several scenarios when estimating future cash flows. For development stage firms, scenarios could include a sale of the firm, an IPO, bankruptcy, or continued private operation. For a mature firm, scenarios might include different ranges of cash flows based on different assumed growth rates.\nThe analyst should be aware of the potential bias in management estimates. For example, management may overstate the value of goodwill or understate future capital needs.\nAlthough analysts use FCFF or FCFE depending on the purposes of the valuation, FCFF is usually more appropriate when the significant changes in the firm’s capital structure are anticipated. The reasoning is that the discount rate used for FCFF valuation, the weighted average cost of capital (WACC), is less sensitive to leverage changes than the cost of equity, the discount rate used for FCFE valuation.\n\n\n\nimage-20210929185438842\n\n\n\n\n\n\n\n\nThe free cash flow method here is a two-stage model.\n\n\n\nUnder this method, a single measure of economic benefit is divided by a capitalization rate to arrive at firm value, where the capitalization rate is the required rate of return minus a growth rate. This is a growing perpetuity model that assumes stable growth and is, in effect, a single-stage free cash flow model. It is most often used for small private companies. It may be suitable when no comparables are available, projections are quire uncertain, and stable growth is a reasonable assumption.\nIf growth is non-constant, the capitalized cash flow method (CCM) should be avoided in favor of the free cash flow method. \\[\n\\\\\\operatorname{value of a firm} = \\frac{FCFF_{1}}{WACC - g}\n\\\\\\operatorname{value of equity} = \\frac{FCFE_{1}}{r - g}\n\\]\n\n\n\nExcess earnings are firm earnings minus the earnings required to provide the required rate of return on working capital and fixed assets. The value of intangible asset can be estimated as the present value of the (growing) stream of excess earnings (using the excess earnings and the growing perpetuity formula from the CCM). This value for the intangible assets is added to the values of working capital and fixed assets to arrive at firm value.\nThe excess earnings method (EEM) is used infrequently but can be used for small firms when their intangible assets are significant. However, the required return for working capital and fixed assets is subject to estimation error. \\[\n\\\\\\\\\\\\operatorname{firm value} = \\operatorname{value\\, of\\, FC\\, / WC (tangible)} + \\operatorname{PV\\, of\\, future\\, excess\\, earning\\, (Intangible)}\n\\]\n\n\n\n\n\nSize premiums: Size premiums are often added to the discount rates for small private companies.\nAvailability and cost of debt: A private firm may have less access to debt financing than a public firm. Because equity capital is usually more expensive than debt and because the higher operating risk of smaller private companies results in higher cost of debt as well. WACC will typically be higher for private firms.\nAcquirer versus target: When acquiring a private firm, some acquirer will incorrectly use their own (lower) cost of capital, rather than the higher rate appropriate for the target, and arrive at a value of the target company that is too high.\nProjection risk: Because of the lower availability of information from private firms and managers who are inexperienced at forecasting, that analysts should increase the discount rate used.\nLifecycle stage: It is particularly difficult to estimate the discount rate for firms in an early stage of development. If such firms have unusually high levels of unsystematic risk, the use of the CAPM may be inappropriate.\n\n\n\n\nimage-20210929191029630\n\n\n\n\n\n\nCAPM: Typically, beta is estimated from public firm data, and this may not be appropriate for private firms that have little change of going public or being acquired by a public firm.\nExpanded CAPM: This version of the CAPM includes additional premiums for size and firm-specific (unsystematic) risk.\nBuild-up method: When it is not possible to find comparable public firms for beta estimation, the build-up method can be used. Beginning with the expected return on the market (beta is implicitly assumed to be one), premiums are added for small size, industry factors, and company specific factors.\n\n\n\n\nimage-20210929191347008\n\n\n\n\n\nMarket approaches to valuing private firms use price multiples and data from previous public and private transactions.\n\nlarge private firm: EBIT or EBITDA multiple\nsmall private firm: NI multiple\nExtremely small private firm: Revenue multiple\n\n\n\n\nimage-20210929191604234\n\n\n\n\nThe guideline public company method (GPCM) uses price multiples from trade data for public companies, with adjustments to the multiples to account for differences between the subject firm and the comparables.\nWhen evaluating a controlling equity interest in a private firm, the control premium should be estimated. Most public share trades are for small, noncontrolling interests; therefore, the price multiple does not reflect a control premium.\nWhen estimating a control premium, the following issues should be considered:\n\nTransaction type: strategic or financial (nonstrategic). A financial transactions typically has a smaller price premium.\nIndustry conditions: Periodically, there is a flurry in industry acquisition activity, diving up acquisition prices. In such markets, share prices of public companies may already reflect some premium for control, and adding a standard control premium to such share price may overstate the appropriate premium for control.\nType of consideration: Some historical acquisitions involves the acquirer’s stock rather than cash. Estimates of the control premium when acquisitions are made up shares that are at higher temporary or “bubble” values will be overstated.\nReasonableness: The use of control premium and price multiples can quickly result in significant differences in valuation from historical pricing.\n\n\n\n\n\nWhen using the guideline transactions method (GTM), prior acquisition values for entire (public and private) companies that already reflect any control premiums are used, so no additional adjustment for a controlling interest is necessary.\n\nTransaction type\nContingent consideration: Contingent consideration refers to that part of the acquisition price that is contingent on the achievement of specific company performance targets, such as receiving FDA approval for a drug.\nType of consideration\nAvailability of data\nDate of data\n\n\n\nThe prior transaction method (PTM) uses transactions data from the stock of the actual subject company and is most appropriate when valuing minority (noncontrolling) interests.\nIdeally, the previous transactions would be arm’s-length, of the same motivation (strategic or financial) as the subject transaction, and fairly recent.\n\n\n\n\nThe value of ownership is equivalent to the fair value of its assets less the fair value of its liabilities.\nRarely used for going concerns: difficulty in valuing intangible assets.\nMost appropriate for:\n\nResource firms\nFinancial service firms, investment companies (real estate investment trusts, closed-end investment companies)\nsmall business with limited intangible assets or early stage companies\n\n\n\n\n\n\nMinority shareholders are at a disadvantage relative to controlling shareholders because they have less power to select the directors and management. They cannot determine the investment and payout policies that affect the value of the firm and the distribution of earnings.\nControlling shareholders can also enjoy excessive compensation and other perquisites to the detriment of minority shareholders. \\[\n\\\\DLOC = 1 - \\frac{1}{1 + \\operatorname{control premium}}\n\\\\DLOC = \\text{Discount Lack of Control}\n\\] \n\n\n\nIf an interest in a firm cannot be easily sold, discounts for lack of marketability (DLOM) would be applied (sometimes termed a discount for lack of liquidity). It is often the case that if a DLOC is applied.\nThe DLOM varies with the following:\n\nAn impending IPO or firm sale would decrease the DLOM.\nThe payment of dividends would decrease the DLOM.\nEarlier, higher payments would decrease the DLOM.\nContractual restrictions on selling stock would increase the DLOM.\nA greater pool of buyers would decrease the DLOM.\nGreater risk and value uncertainty would increase the DLOM.\n\nTo estimate the DLOM, an analyst can use one of three methods. The price of the restricted shares is compared to the price of the publicly traded shares.\nA third method would estimate the DLOM as the price of a put option divided by the stock price. \\[\n\\\\\\operatorname{total discount} = 1 - (1 - DLOC)(1 - DLOM)\n\\]\n\n\n\nimage-20210929193918102\n\n\n\n\n\n\nThe Uniform Standards of Professional Appraisal Practice (USPAP) were created by the Appraisal Foundation.\nThe International Valuation Standards Committee (IVSC) has create the International Valuation Standards.\nThere are many challenges involved with the implementation of appraisal standards:\n\nThe compliance with these standards is usually at the discretion of the appraiser because most buyers are still unaware of them.\nBecause most valuation reports are private, it is very difficult for the organizations to ensure compliance to the standards.\nAlthough the organizations provide technical guidance on the use of their standards, it is necessarily limited due to the heterogeneity of valuations."
  },
  {
    "objectID": "posts/2021-10-28-tiny-python-project-chapter-11~21/2021-10-28-tiny-python-project-chapter-11~21.html",
    "href": "posts/2021-10-28-tiny-python-project-chapter-11~21/2021-10-28-tiny-python-project-chapter-11~21.html",
    "title": "Tiny Python Project Chapter 11-21",
    "section": "",
    "text": "image-20211028091059547\n\n\n\n\n\nimage-20211028091113621\n\n\n\n\n\nimage-20211028091118411\n\n\n\n\n\nimage-20211028091450877\n\n\n\n\n\nimage-20211028104605929\n\n\n\n\n\nimage-20211028104616076\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@e5040579c2cf>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Bottles of beer song',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-n',\n                        '--num',\n                        help='How many bottles',\n                        metavar='number',\n                        type=int,\n                        default=10)\n\n    args = parser.parse_args()\n\n    if (args.num < 0):\n        parser.error(f'--num \"{args.num}\" must be greater than 0')\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    num = args.num\n\n    # for i in range(num, 0, -1):\n    #     print(verse(i))\n\n    print('\\n\\n'.join(map(verse, range(num, 0, -1))))\n        \ndef verse(bottle):\n    sing_verse = ''\n    if (bottle == 1):\n        sing_verse = '\\n'.join([f'{bottle} bottle of beer on the wall,', f'{bottle} bottle of beer,', 'Take one down, pass it around,', 'No more bottles of beer on the wall!'])\n    \n    elif (bottle == 2):\n        sing_verse = '\\n'.join([f'{bottle} bottles of beer on the wall,', f'{bottle} bottles of beer,', 'Take one down, pass it around,', f'{bottle - 1} bottle of beer on the wall!'])\n        \n    else:\n        sing_verse = '\\n'.join([f'{bottle} bottles of beer on the wall,', f'{bottle} bottles of beer,', 'Take one down, pass it around,', f'{bottle - 1} bottles of beer on the wall!'])\n        \n\n    return sing_verse\n\ndef test_verse():\n    \"\"\"Test verse\"\"\"\n\n    last_verse = verse(1)\n    assert last_verse == '\\n'.join([\n        '1 bottle of beer on the wall,', '1 bottle of beer,', 'Take one down, pass it around,', 'No more bottles of beer on the wall!'\n    ])\n\n    two_bottles = verse(2)\n    assert two_bottles == '\\n'.join([\n        '2 bottles of beer on the wall,', '2 bottles of beer,', 'Take one down, pass it around,', '1 bottle of beer on the wall!'\n    ])\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211028110723435\n\n\n\n\n\nimage-20211028110728946\n\n\n\n\n\nimage-20211028110735521\n\n\n\n\n\n\n\n\nimage-20211028111110183\n\n\n\n\n\nimage-20211028111119362\n\n\n\n\n\nimage-20211028111126792\n\n\n\n\n\nimage-20211028111750799\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@bc67f35f04dd>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport random\nimport os\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Ransom Note\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Ransom Note',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n    metavar='text',\n                        help='Input text or file')\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='int',\n                        type=int,\n                        default=None)\n\n    args = parser.parse_args()\n\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read().rstrip()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n    original_t = args.text\n    t = ''.join([choose(original_t[i]) for i in range(len(original_t))])\n    print(t)\n\ndef choose(char):\n    return random.choice([char.lower(), char.upper()])\n\ndef test_choose():\n    state = random.getstate()\n    random.seed(1)\n    assert choose('a') == 'a'\n    assert choose('b') == 'b'\n    assert choose('c') == 'c'\n    assert choose('d') == 'd'\n    random.setstate(state)\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211028112436771\n\n\n\n\n\nimage-20211028112519886\n\n\n\n\n\n\n\n\nimage-20211028112633192\n\n\n\n\n\nimage-20211028112642264\n\n\n\n\n\nimage-20211028113012514\n\n\n\n\n\nimage-20211028113019770\n\n\n\n\n\nimage-20211028114129950\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@117e0abe372c>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Twelve Days of christmas',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-n',\n                        '--num',\n                        help='Number of days to sing',\n                        metavar='days',\n                        type=int,\n                        default=12)\n\n    parser.add_argument('-o',\n                        '--outfile',\n                        help='Outfile',\n                        metavar='FILE',\n                        type=argparse.FileType('wt'),\n                        default=sys.stdout)\n\n    args = parser.parse_args()\n\n    if not(1 <= args.num <= 12):\n        parser.error(f'--num \"{args.num}\" must be between 1 and 12')\n        \n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    num = args.num\n    \n    print('\\n\\n'.join([verse(day) for day in range(1, num + 1)]), end='', file=args.outfile)\n\ndef verse(day):\n    ordinal = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth']\n\n    present = ['twelve drummers drumming', 'eleven pipers piping', 'ten lords a leaping', 'nine ladies dancing', 'eight maids a milking', 'seven swans a swimming', 'six geese a laying', 'five gold rings', 'four calling birds', 'three French hens', 'two turtle doves', 'a partridge in a pear tree']\n\n    present_sent = ''\n\n    for i in reversed(range(1, day + 1)):\n        if (day > 1 and i == 1):\n            present_sent += f'And {present[-1 * i]}.'\n        \n        elif (day == 1):\n            present_sent += f'{present[-1 * i].capitalize()}.'\n        \n        else:\n            present_sent += f'{present[-1 * i][0].upper()}{present[-1 * i][1:]},\\n'\n\n    return '\\n'.join([f'On the {ordinal[day - 1]} day of Christmas,', 'My true love gave to me,', present_sent])\n\ndef test_verse():\n    assert verse(1) == '\\n'.join(['On the first day of Christmas,', 'My true love gave to me,', 'A partridge in a pear tree.'])\n    assert verse(2) == '\\n'.join(['On the second day of Christmas,', 'My true love gave to me,', 'A partridge in a pear tree.'])\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211028134238381\n\n\n\n\n\nimage-20211028134245241\n\n\n\n\n\nimage-20211028134315766\n\n\n\n\n\nimage-20211028134324646\n\n\n\n\n\nimage-20211028134340037\n\n\n\n\n\n\n\n\nimage-20211028135454852\n\n\n\n\n\nimage-20211028135506371\n\n\n\n\n\nimage-20211028141608445\n\n\n\n\n\nimage-20211028141602464\n\n\n\n\n\nimage-20211028141702902\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@c05ebd5110ee>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport re\nimport string as s\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Make rhyming \"word\"',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('word',\n                        metavar='word',\n                        help='A word to rhyme')\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    word = args.word\n\n    w_list = list('bcdfghjklmnpqrstvwxyz') + (\n        'bl br ch cl cr dr fl fr gl gr pl pr sc '\n        'sh sk sl sm sn sp st sw th tr tw thw wh wr '\n        'sch scr shr sph spl spr squ str thr').split()\n\n    w_list = sorted(w_list)\n\n    start, rest = stemmer(word)\n\n    if rest == '':\n        print(f'Cannot rhyme \"{start}\"')\n    else:\n        print('\\n'.join([f'{w}{rest}' for w in w_list]))\n\ndef stemmer(word):\n    consonants = ''.join([c for c in s.ascii_lowercase if c not in 'aeiou'])\n    pattern = f'([{consonants}]+)?([aeiou])(.*)'\n\n    match = re.match(pattern, word.lower())\n\n    if match:\n        start = match.group(1) or ''\n        aeiou = match.group(2) or ''\n        rest = match.group(3) or ''\n        return (start, aeiou + rest)\n\n    else:\n        return (word, '')\n\ndef test_stemmer():\n    assert stemmer('') == ('', '')\n    assert stemmer('cake') == ('c', 'ake')\n    assert stemmer('chair') == ('ch', 'air')\n    assert stemmer('APPLE') == ('', 'apple')\n    assert stemmer('RDNZL') == ('rdnzl', '')\n    assert stemmer('123') == ('123', '')\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211028171658450\n\n\n\n\n\nimage-20211028171720475\n\n\n\n\n\nimage-20211028171749321\n\n\n\n\n\nimage-20211028171759009\n\n\n\n\n\n\n\n\nimage-20211028235724282\n\n\n\n\n\nimage-20211028235728792\n\n\n\n\n\nimage-20211028235736730\n\n\n\n\n\nimage-20211028235742184\n\n\n\n\n\nimage-20211028235857880\n\n\n\n\n\nimage-20211029000005376\n\n\n\n\n\nimage-20211029000057073\n\n\n\n\n\nimage-20211029000105580\n\n\n\n\n\nimage-20211029000118664\n\n\n\n\n\nimage-20211029000132586\n\n\n\n\n\nimage-20211029000138399\n\n\n\n\n\nimage-20211029000151272\n\n\n\n\n\nimage-20211029000222752\n\n\n\n\n\nimage-20211029000228575\n\n\n\n\n\nimage-20211029000330568\n\n\n\n\n\nimage-20211029000619591\n\n\n\n\n\nimage-20211029000643386\n\n\n\n\n\nimage-20211029000705435\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@c6ec56f874a6>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\nimport re\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Southern fry text',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='text',\n                        help='Input text or file')\n\n    args = parser.parse_args()\n\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read().rstrip()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    text = args.text\n    \n    for w in re.split(r'(\\W+)', text):\n        print(fry(w), end='')\n\ndef fry(text):\n    word = text.lower()\n    if word == 'you':\n        return text[0] + \"'all\"\n\n    match = re.search(r'(.+)ing$', text)\n    if match:\n        first = match.group(1) or ''\n    else:\n        return text\n\n    if re.search(r'[aeiouy]', first.lower()):\n        return first + \"in'\"\n    else:\n        return text\n\ndef test_fry():\n    assert fry('you') == \"y'all\"\n    assert fry('You') == \"Y'all\"\n    assert fry('fishing') == \"fishin'\"\n    assert fry('Aching') == \"Achin'\"\n    assert fry('swing') == \"swing\"\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029025220915\n\n\n\n\n\n\n\n\nimage-20211029025341877\n\n\n\n\n\nimage-20211029025351361\n\n\n\n\n\nimage-20211029025359920\n\n\n\n\n\nimage-20211029025736393\n\n\n\n\n\nimage-20211029025745219\n\n\n\n\n\nimage-20211029030039936\n\n\n\n\n\nimage-20211029030052811\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@ab92a6669e44>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\nimport re\nimport random\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Scramble the letters of words',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='text',\n                        help='Input text or fileoptional arguments:')\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Randome seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    args = parser.parse_args()\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n    text = args.text\n\n    splitter = re.compile(\"([a-zA-Z](?:[a-zA-Z']*[a-zA-Z])?)\")\n\n    for w in splitter.split(text):\n        if re.match(r\"\\w\", w):\n            print(scramble(w), end='')\n        else:\n            print(w, end='')\n\ndef scramble(word):\n    if len(word) > 3:\n        list_word = list(word[1:-1])\n        random.shuffle(list_word)\n        return ''.join([word[0], *list_word, word[-1]])\n    else:\n        return word\n\ndef test_scramble():\n    state = random.getstate()\n    random.seed(1)\n    assert scramble('a') == 'a'\n    assert scramble('ab') == 'ab'\n    assert scramble('abc') == 'abc'\n    assert scramble('abcd') == 'acbd'\n    assert scramble('abcde') == 'acbde'\n    assert scramble('abcdef') == 'aecbdf'\n    assert scramble(\"abcde'f\") == \"abcd'ef\"\n    random.setstate(state)\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029031630704\n\n\n\n\n\nimage-20211029031649057\n\n\n\n\n\nimage-20211029031658673\n\n\n\n\n\n\n\n\nimage-20211029031832993\n\n\n\n\n\nimage-20211029031838697\n\n\n\n\n\nimage-20211029031951913\n\n\n\n\n\nimage-20211029031940751\n\n\n\n\n\nimage-20211029032358927\n\n\n\n\n\nimage-20211029032427222\n\n\n\n\n\nimage-20211029032518238\n\n\n\n\n\nimage-20211029032603026\n\n\n\n\n\nimage-20211029032617678\n\n\n\n\n\nimage-20211029032707879\n\n\n\n\n\nimage-20211029032823913\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@ab92a6669e44>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport re\nimport sys\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Mad Libs',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('file',\n                        metavar='FILE',\n                        help='Input file',\n                        type=argparse.FileType('rt'))\n\n    parser.add_argument('-i',\n                        '--inputs',\n                        help='Inputs (for testing)',\n                        metavar='input',\n                        type=str,\n                        nargs='*',)\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    inputs = args.inputs\n    text = args.file.read().rstrip()\n    splitter = re.compile(r\"(<([^<>]+)>)\")\n    blanks = re.findall(splitter, text)\n\n    if not blanks:\n        sys.exit(f'\"{args.file.name}\" has no placeholders.')\n\n    tmpl = 'Give me {} {}'\n\n    for placeholder, name in blanks:\n        article = 'an' if name.lower()[0] in 'aeiou' else 'a'\n        answer = inputs.pop(0) if inputs else input(tmpl.format(article, name))\n        text = re.sub(f\"{placeholder}\", answer, text, count=1)\n        \n    print(text)\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029043147498\n\n\n\n\n\nimage-20211029043200995\n\n\n\n\n\n\n\n\nimage-20211029044046784\n\n\n\n\n\nimage-20211029044059191\n\n\n\n\n\nimage-20211029044039899\n\n\n\n\n\nimage-20211029044120790\n\n\n\n\n\nimage-20211029044205266\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@2cf8b1eba529>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\nimport re\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Gematria',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='text',\n                        help='Input text or file')\n\n    args = parser.parse_args()\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    text = args.text\n    \n    for line in text.splitlines():\n        print(' '.join(map(word2num, line.split())))\n\ndef word2num(word):\n    text = re.sub(r'[^A-Za-z0-9]', '', word)\n    num = str(sum(map(ord, re.findall(r'[A-Za-z0-9]', text))))\n    return num\n\ndef test_word2num():\n    assert word2num(\"a\") == \"97\"\n    assert word2num(\"abc\") == \"294\"\n    assert word2num(\"ab'c\") == \"294\"\n    assert word2num(\"4a-b'c,\") == \"346\"\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029050326175\n\n\n\n\n\n\n\n\nimage-20211029050755895\n\n\n\n\n\nimage-20211029050904470\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@68d96a2911e0>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport csv\nfrom tabulate import tabulate\nimport io\nimport re\nimport random\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Create Workout Of (the) Day (WOD)',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-f',\n                        '--file',\n                        help='CSV input file of exercises',\n                        metavar='FILE',\n                        type=argparse.FileType('rt'),\n                        default='inputs/exercises.csv')\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    parser.add_argument('-n',\n                        '--num',\n                        help='Number of exercises',\n                        metavar='exercises',\n                        type=int,\n                        default=4)\n\n    parser.add_argument('-e',\n                        '--easy',\n                        help='Halve the reps',\n                        action='store_true')\n\n    args = parser.parse_args()\n\n    if args.num < 1:\n        parser.error(f'--num \"{args.num}\" must be greater than 0')\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n    wod = []\n    exercises = read_csv(args.file)\n\n    for ex, low, high in random.sample(exercises, k=args.num):\n        rep = random.randint(low, high)\n        if args.easy:\n            rep = int(rep / 2)\n        wod.append((ex, rep))\n    \n    print(tabulate(wod, headers=('Exercise', 'Reps')))\n\ndef read_csv(fh):\n    exercises = []\n    for row in csv.DictReader(fh, delimiter=','):\n        low, high = map(int, row['reps'].split('-'))\n        exercises.append((row['exercise'], low, high))\n\n    return exercises\ndef test_read_csv():\n    text = io.StringIO('exercise,reps\\nBurpees,20-50\\nSitups,40-100')\n    assert read_csv(text) == [('Burpees', 20, 50), ('Situps', 40, 100)]\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029072039151\n\n\n\n\n\n\n\n\nimage-20211029074224073\n\n\n\n\n\nimage-20211029074231364\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@8e6e030411c6>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport re\nimport random\nimport argparse\nimport string\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Rock the Casbah',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('file',\n                        metavar='FILE',\n                        help='Input file(s)',\n                        nargs='+',\n                        type=argparse.FileType('rt'))\n\n    parser.add_argument('-n',\n                        '--num',\n                        help='Number of passwords to generate',\n                        metavar='num_passwords',\n                        type=int,\n                        default=3)\n\n    parser.add_argument('-w',\n                        '--num_words',\n                        help='Number of words to use for password',\n                        metavar='num_words',\n                        type=int,\n                        default=4)\n\n    parser.add_argument('-m',\n                        '--min_word_len',\n                        help='Minimum word length',\n                        metavar='minimum',\n                        type=int,\n                        default=3)\n\n    parser.add_argument('-x',\n                        '--max_word_len',\n                        help='Maximum word length',\n                        metavar='maximum',\n                        type=int,\n                        default=6)\n\n    parser.add_argument('-s',\n                        '--seed',\n                        help='Random seed',\n                        metavar='seed',\n                        type=int,\n                        default=None)\n\n    parser.add_argument('-l',\n    '--l33t',\n    help='Obfuscate letters',\n    action='store_true')\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    random.seed(args.seed)\n    words = set()\n\n    def word_len(word):\n        return args.min_word_len <= len(word) <= args.max_word_len\n\n    for fh in args.file:\n        for line in fh:\n            for word in filter(word_len, map(clean, line.lower().split())):\n                words.add(word.capitalize())\n    \n    words = sorted(words)\n    passwords = [''.join(random.sample(words, args.num_words)) for _ in range(args.num)]\n\n    if args.l33t:\n        passwords = map(l33t, passwords)\n\n    print('\\n'.join(passwords))\n\ndef l33t(word):\n    word = ransom(word)\n    l33t_dict = str.maketrans({'a': '@', 'A': '4', 'O': '0', 't': '+', 'E': '3', 'I': '1', 'S': '5'})\n\n    return word.translate(l33t_dict) + random.choice(string.punctuation)\n\ndef test_l33t():\n    state = random.getstate()\n    random.seed(1)\n    assert l33t('Money') == 'moNeY{'\n    assert l33t('Dollars') == 'D0ll4r5`'\n    random.setstate(state)\n\ndef ransom(word):\n    new_word = ''\n    for w in word.lower():\n        if random.choice([True, False]):\n            new_word += w\n        else:\n            new_word += w.upper()\n    return new_word\n\ndef test_ransom():\n    state = random.getstate()\n    random.seed(1)\n    assert ransom('Money') == 'moNeY'\n    assert ransom('Dollars') == 'DOLlaRs'\n    random.setstate(state)\n\ndef clean(word):\n    word = re.sub(r'[^a-zA-Z]', '', word)\n    return word\n\ndef test_clean():\n    assert clean('') == ''\n    assert clean('states,') == 'states'\n    assert clean(\"Don't\") == 'Dont'\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029082305559\n\n\n\n\n\nimage-20211029082318959\n\n\n\n\n\n\n\n\nimage-20211029084154653\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@ded74145e45a>\nDate   : 2021-10-28\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport re\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Rock the Casbah',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('-b',\n                        '--board',\n                        help='',\n                        metavar='board',\n                        type=str,\n                        default='.' * 9)\n\n    parser.add_argument('-p',\n                        '--player',\n                        help='',\n                        metavar='player',\n                        type=str,\n                        choices=['X', 'O'],\n                        default=None)\n\n    parser.add_argument('-c',\n                        '--cell',\n                        help='',\n                        metavar='cell',\n                        type=int,\n                        choices=[i for i in range(1, 10)],\n                        default=None)\n\n    args = parser.parse_args()\n\n    if (args.player == None and args.cell != None) or (args.cell == None and args.player != None):\n        parser.error('Must provide both --player and --cell')\n\n    if not re.search(r'^[.XO]{9}$', args.board):\n        parser.error(f'--board \"{args.board}\" must be 9 characters of ., X, O')\n\n    if args.player and args.cell and args.board[args.cell - 1] in 'XO':\n        parser.error(f'--cell \"{args.cell}\" already taken')\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    board = args.board\n    cell = args.cell\n    player = args.player\n\n    if None not in (player, cell):\n        board = ''.join([player if (cell - 1) == i else c for i, c in enumerate(board)])\n\n    result = find_winner(board)\n    if result == None:\n        result = 'No winner.'\n    else:\n        result += ' has won!'\n    \n    print(format_board(board) + f'\\n{result}')\n\ndef format_board(board):\n    board = ''.join([c if c in 'XO' else str(i + 1) for i, c in enumerate(board)])\n\n    board = f\"\"\"-------------\n| {board[0]} | {board[1]} | {board[2]} |\n-------------\n| {board[3]} | {board[4]} | {board[5]} |\n-------------\n| {board[6]} | {board[7]} | {board[8]} |\n-------------\"\"\"\n\n    return board\n\ndef find_winner(board):\n    wins = [('PPP......'), ('...PPP...'), ('......PPP'), ('P..P..P..'),\n            ('.P..P..P.'), ('..P..P..P'), ('P...P...P'), ('..P.P.P..')]\n\n    for p in 'XO':\n        other_player = 'O' if p == 'X' else 'X'\n        for b in wins:\n            win_board = b.replace('P', p)\n            comp_board = board.replace(other_player, '.')\n            count = 0\n            for i, c in enumerate(win_board):\n                if comp_board[i] != '.' and comp_board[i] == c:\n                    count += 1\n                if count == 3:\n                    return p\n\n    return None\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029153031020\n\n\n\n\n\n\n\n\nimage-20211029153120042\n\n\n\n\n\nimage-20211029153244622\n\n\n\n\n\nimage-20211029153544734\n\n\n\n\n\nimage-20211029153552623\n\n\n\n\n\nimage-20211029153616572\n\n\n\n\n\nimage-20211029153633936\n\n\n\n\n\nimage-20211029153648410\n\n\n\n\n\nimage-20211029153658883\n\n\n\n\n\nimage-20211029153733124\n\n\n\n\n\nimage-20211029153744777\n\n\n\n\n\nimage-20211029153757951\n\n\n\n\n\nimage-20211029153810373\n\n\n\n\n\nimage-20211029154117355\n\n\n\n\n\nimage-20211029154139143\n\n\n\n\n\nimage-20211029154212666\n\n\n\n\n\nimage-20211029154223367\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@1a4e6e4cffcf>\nDate   : 2021-10-29\nPurpose: Rock the Casbah\n\"\"\"\n\nimport re\nfrom typing import List, NamedTuple, Optional\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n    state = State()\n\n    while not state.quit:\n        print(\"\\033[H\\033[J\")\n        print(format_board(state.board))\n        if state.error:\n            print(state.error)\n            state = state._replace(error=None)\n        val = input(f'Player {state.player}, what is your move? [q to quit] ')\n\n        if val == 'q':\n            state = state._replace(quit=True)\n            print('You lose, loser!')\n            break\n        elif state.draw:\n            print(\"All right, we'll call it a draw.\")\n            break\n        elif not re.search(r'^[1-9]$', val):\n            state = state._replace(error=f'Invaild cell \"{val}\", please use 1-9')\n            continue\n        else:\n            val = int(val)\n        \n        if state.board[int(val) - 1] in 'XO':\n            state = state._replace(error=f'Cell \"{val}\" already taken.')\n            continue\n        \n        state = state._replace(board=[state.player if i == (val - 1) else c for i, c in enumerate(state.board)])\n\n        winner = find_winner(''.join(state.board))\n        next_player = 'O' if state.player == 'X' else 'X'\n\n        if winner:\n            print(format_board(''.join(state.board)))\n            print(f'{state.player} has won!')\n            state = state._replace(quit=True)\n            continue\n        elif not winner and '.' not in state.board:\n            state = state._replace(draw=True)\n        else:\n            state = state._replace(player=next_player)\n\ndef format_board(board):\n    board = ''.join([c if c in 'XO' else str(i + 1) for i, c in enumerate(board)])\n\n    board = f\"\"\"-------------\n| {board[0]} | {board[1]} | {board[2]} |\n-------------\n| {board[3]} | {board[4]} | {board[5]} |\n-------------\n| {board[6]} | {board[7]} | {board[8]} |\n-------------\"\"\"\n\n    return board\n\ndef find_winner(board):\n    wins = [('PPP......'), ('...PPP...'), ('......PPP'), ('P..P..P..'),\n            ('.P..P..P.'), ('..P..P..P'), ('P...P...P'), ('..P.P.P..')]\n\n    for p in 'XO':\n        other_player = 'O' if p == 'X' else 'X'\n        for b in wins:\n            win_board = b.replace('P', p)\n            comp_board = board.replace(other_player, '.')\n            count = 0\n            for i, c in enumerate(win_board):\n                if comp_board[i] != '.' and comp_board[i] == c:\n                    count += 1\n                if count == 3:\n                    return p\n\n    return None\n\nclass State(NamedTuple):\n    board: List[str] = list('.' * 9)\n    player: str = 'X'\n    quit: bool = False\n    draw: bool = False\n    error: Optional[str] = None\n    winner: Optional[str] = None\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211029163151406\n\n\n\n\n\nimage-20211029163156607\n\n\n\n\n\n\n\n\nimage-20211029163213333\n\n\n\n\n\nimage-20211029163234981\n\n\n\n\n\nimage-20211029163238679\n\n\n\n\n\n\n\n\nimage-20211029163319915\n\n\n\n\n\nimage-20211029163445766\n\n\n\n\n\nimage-20211029163506441\n\n\n\n\n\nimage-20211029163520121\n\n\n\n\n\nimage-20211029163528026\n\n\n\n\n\nimage-20211029163536997\n\n\n\n\n\nimage-20211029163544972\n\n\n\n\n\nimage-20211029163555473\n\n\n\n\n\nimage-20211029163609312\n\n\n\n\n\nimage-20211029163625125\n\n\n\n\n\nimage-20211029163634566\n\n\n\n\n\nimage-20211029163650214\n\n\n\n\n\nimage-20211029163715105\n\n\n\n\n\nimage-20211029163722238\n\n\n\n\n\nimage-20211029163732310\n\n\n\n\n\nimage-20211029163746868\n\n\n\n\n\nimage-20211029163758256\n\n\n\n\n\nimage-20211029163803990\n\n\n\n\n\nimage-20211029163812708\n\n\n\n\n\nimage-20211029163840606\n\n\n\n\n\nimage-20211029163851368\n\n\n\n\n\nimage-20211029163855731\n\n\n\n\n\nimage-20211029163901835\n\n\n\n\n\nimage-20211029163913872\n\n\n\n\n\nimage-20211029163924193\n\n\n\n\n\nimage-20211029163933408\n\n\n\n\n\nimage-20211029163941919\n\n\n\n\n\nimage-20211029163953506\n\n\n\n\n\nimage-20211029164006276\n\n\n\n\n\nimage-20211029164024751\n\n\n\n\n\nimage-20211029164037222\n\n\n\n\n\nimage-20211029164052871\n\n\n\n\n\nimage-20211029164101271\n\n\n\n\n\nimage-20211029164131483\n\n\n\n\n\nimage-20211029164136555\n\n\n\n\n\nimage-20211029164145798\n\n\n\n\n\nimage-20211029164150616"
  },
  {
    "objectID": "posts/2021-12-08-probability-and-statistics-week-14/2021-12-08-probability-and-statistics-week-14.html",
    "href": "posts/2021-12-08-probability-and-statistics-week-14/2021-12-08-probability-and-statistics-week-14.html",
    "title": "Probability and Statistics Week 14",
    "section": "",
    "text": "image-20211208032045619\n\n\n\n\n\n\n\n\nimage-20211208032055401\n\n\n\n\n\n\n\n\nimage-20211208032106120\n\n\n\n\n\nimage-20211208032702211\n\n\n\n\n\n\n\n\nimage-20211208032715999\n\n\n\n\n\nimage-20211208032721921\n\n\n\n\n\n\n\n\nimage-20211208042254510\n\n\n\n\n\n\n\n\nimage-20211208042306456\n\n\n\n\n\n\n\n\nimage-20211208042315155\n\n\n\n\n\nimage-20211208042321286\n\n\n\n\n\n\n\n\nimage-20211208042332389\n\n\n\n\n\n\n\n\nimage-20211208043013463\n\n\n\n\n\n\n\n\nimage-20211208043023190\n\n\n\n\n\nimage-20211208043028637\n\n\n\n\n\n\n\n\nimage-20211208043040999\n\n\n\n\n\nimage-20211208043047102\n\n\n\n\n\nimage-20211208043052820\n\n\n\n\n\nimage-20211208043058265\n\n\n\n\n\n\n\n\nimage-20211208043921131\n\n\n\n\n\n\n\n\nimage-20211208043932412\n\n\n\n\n\n\n\n\nimage-20211208054439298\n\n\n\n\n\nimage-20211208054446153\n\n\n\n\n\nimage-20211208054452961"
  },
  {
    "objectID": "posts/2021-11-28-probability-and-inferential-statistics-week-11/2021-11-29-digital-system-circuits-week-10.html",
    "href": "posts/2021-11-28-probability-and-inferential-statistics-week-11/2021-11-29-digital-system-circuits-week-10.html",
    "title": "Digital System Circuits Week 10",
    "section": "",
    "text": "cross-coupled NOR gate\n\n\n\nimage-20211130023147982\n\n\n\n\n\n\n\n\nimage-20211130023155938\n\n\n\n\n\n\n\n\nimage-20211130023229253\n\n\n\n\n\n\n\n\nimage-20211130023241528\n\n\n\n\n\nimage-20211130023247677\n\n\nInvalid condition = race condition\n\n한 번 바뀌면 다시 바뀌지 않는 상태\n\n\n\n\n\n\n\nimage-20211130023300788\n\n\n\n\n\n\n\n\nimage-20211130023313917\n\n\n\n\n\nimage-20211130023323797\n\n\n\n\n\nimage-20211130023333918\n\n\nglitch - unwanted spark caused by logic gate delay\n\n\n\n\n\n\nimage-20211130023109651\n\n\n\n\n\n\n\n\nimage-20211130030411499\n\n\n\n\n\n\n\n\nimage-20211130030536292\n\n\n\n\n\nimage-20211130030546389\n\n\n\n\n\n\n\n\nimage-20211130030558393\n\n\n\n\n\nimage-20211130031700597\n\n\n\n\n\nimage-20211130031707887\n\n\n\n\n\n\n\n\nimage-20211130032715015"
  },
  {
    "objectID": "posts/2021-10-02-kaggle-AI-ethics/2021-10-02-kaggle-AI-ethics.html",
    "href": "posts/2021-10-02-kaggle-AI-ethics/2021-10-02-kaggle-AI-ethics.html",
    "title": "Kaggle - AI Ethics",
    "section": "",
    "text": "These technologies have the potential to harm or help the people that they serve. By applying an ethical lens, we can work toward identifying the harms that these technologies can cause to people and we can design and build them to reduce these harms - or decide not to build them.\nThis course covers several topics:\n\nIn the human-centered design lesson, you’ll learn how to design an AI system to ensure that it serves the needs of the people that it is intended for.\nIn the bias lesson, you’ll determine how AI systems can learn to discriminate against certain groups.\nIn the fairness lesson, you’ll learn to quantify the extent of the bias in AI systems.\nIn the model cards lesson, you’ll learn how to use a popular framework for improving public accountability for AI models.\n\n\n\nBefore selecting data and training models, it is important to carefully consider the human needs an AI system should serve - and if it should be built at all.\nHuman-centered design (HCD) is an approach to designing systems that serve people’s needs.\n6 steps\n\n\nYour entire team – including data scientists and engineers – should be involved in this step, so that every team member gains an understanding of the people they hope to serve. Your team should include and involve people with diverse perspectives and backgrounds, along race, gender, and other characteristics. Sharpen your problem definition and brainstorm creative and inclusive solutions together.\n\n\n\n\nWould people generally agree that what you are trying to achieve is a good outcome?\nWould non-AI systems - such as rule-based solutions, which are easier to create, audit and maintain - be significantly less effective than an AI system?\nIs the task that you are using AI for one that people would find boring, repetitive or otherwise difficult to concentrate on?\nHave AI solutions proven to be better than other solutions for similar use cases in the past?\n\nIf you answered no to any of these questions, an AI solution may not be necessary or appropriate.\n\n\n\nWeigh the benefits of using AI against the potential harms, throughout the design pipeline: from collecting and labeling data, to training a model, to deploying the AI system. Consider the impact on users and on society. Your privacy team can help uncover hidden privacy issues and determine whether privacy-preserving techniques like differential privacy or federated learning may be appropriate. Take steps to reduce harms, including by embedding people - and therefore human judgment - more effectively in data selection, in model training and in the operation of the system. If you estimate that the harms are likely to outweigh the benefits, do not build the system.\n\n\nAn online education company wants to use an AI system to ‘read’ and automatically assign scores to student essays, while redirecting company staff to double-check random essays and to review essays that the AI system has trouble with. The system would enable the company to quickly get scores back to students. The company creates a harms review committee, which recommends that the system not be built. Some of the major harms flagged by the committee include: the potential for the AI system to pick up bias against certain patterns of language from training data and amplify it (harming people in the groups that use those patterns of language), to encourage students to ‘game’ the algorithm rather than improve their essays and to reduce the classroom role of education experts while increasing the role of technology experts.\n\n\n\n\nDevelop a non-AI prototype of your AI system quickly to see how people interact with it. This makes prototyping easier, faster and less expensive. It also gives you early information about what users expect from your system and how to make their interactions more rewarding and meaningful.\nThe people giving feedback should have diverse backgrounds – including along race, gender, expertise and other characteristics. They should also understand and consent to what they are helping with and how.\n\n\n\nPeople who use your AI system once it is live should be able to challenge its recommendations or easily opt out of using it. Put systems and tools in place to accept, monitor and address challenges.\nTalk to users and think from the perspective of a user: if you are curious or dissatisfied with the system’s recommendations, would you want to challenge it by:\n\nRequesting an explanation of how it arrived at its recommendation?\nRequesting a change in the information you input?\nTurning off certain features?\nReaching out to the product team on social media?\nTaking some other action?\n\n\n\n\nSafety measures protect users against harm. They seek to limit unintended behavior and accidents, by ensuring that a system reliably delivers high-quality outcomes. This can only be achieved through extensive and continuous evaluation and testing. Design processes around your AI system to continuously monitor performance, delivery of intended benefits, reduction of harms, fairness metrics and any changes in how people are actually using it.\nHuman oversight of your AI system is crucial:\n\nCreate a human ‘red team’ to play the role of a person trying to manipulate your system into unintended behavior. Then, strengthen your system against any such manipulation.\nDetermine how people in your organization can best monitor the system’s safety once it is live.\nExplore ways for your AI system to quickly alert a human when it is faced with a challenging case.\nCreate ways for users and others to flag potential safety issues.\n\n\n\n\n\nML applications have discriminated against individuals on the basis of race, sex, religion, socioeconomic status, and other categories.\n\n\nBias in data is complex. Flawed data can also result in representation bias (covered later in this tutorial), if a group is underrepresented in the training data. For instance, when training a facial detection system, if the training data contains mostly individuals with lighter skin tones, it will fail to perform well for users with darker skin tones. A third type of bias that can arise from the training data is called measurement bias, which you’ll learn about below.\nbias can also result from the way in which the ML model is defined, from the way the model is compared to other models, and from the way that everyday users interpret the final results of the model. Harm can come from anywhere in the ML process.\n\n\n\n\n\nHistorical bias occurs when the state of the world in which the data was generated is flawed.\n\n\n\nMeasurement bias occurs when the accuracy of the data varies across groups. This can happen when working with proxy variables (variables that take the place of a variable that cannot be directly measured), if the quality of the proxy varies in different groups.\n\n\n\nAggregation bias occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group or only performs well for the majority group. (This is often not an issue, but most commonly arises in medical applications.)\n\n\n\nEvaluation bias occurs when evaluating a model, if the benchmark data (used to compare the model to other models that perform similar tasks) does not represent the population that the model will serve.\n\n\n\nDeployment bias occurs when the problem the model is intended to solve is different from the way it is actually used. If the end users don’t use the model in the way it is intended, there is no guarantee that the model will perform well.\nNote that these are not mutually exclusive: that is, an ML application can easily suffer from more than one type of bias.\n\n\n\nimage-20211002021354944\n\n\n\n\n\nimage-20211002021457078\n\n\n\n\n\nimage-20211002021549954\n\n\n\n\n\n\n\n\n\n\n\nDemographic parity says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants.\n\nA nonprofit is organizing an international conference, and 20,000 people have signed up to attend. The organizers write a ML model to select 100 attendees who could potentially give interesting talks at the conference. Since 50% of the attendees will be women (10,000 out of 20,000), they design the model so that 50% of the selected speaker candidates are women.\n\n\n\n\nEqual opportunity fairness ensures that the proportion of people who should be selected by the model (“positives”) that are correctly selected by the model is the same for each group. We refer to this proportion as the true positive rate (TPR) or sensitivity of the model.\n\n\n\nAlternatively, we could check that the model has equal accuracy for each group. That is, the percentage of correct classifications (people who should be denied and are denied, and people who should be approved who are approved) should be the same for each group.\n\n\n\nGroup unaware fairness removes all group membership information from the dataset. For instance, we can remove gender data to try to make the model fair to different gender groups. Similarly, we can remove information about race or age.\n\nOne difficulty of applying this approach in practice is that one has to be careful to identify and remove proxies for the group membership data. For instance, in cities that are racially segregated, zip code is a strong proxy for race. That is, when the race data is removed, the zip code data should also be removed, or else the ML application may still be able to infer an individual’s race from the data. Additionally, group unaware fairness is unlikely to be a good solution for historical bias.\n\n\n\n\n\n\n\n\nimg\n\n\nNote that *group unaware* fairness cannot be detected from the confusion matrix, and is more concerned with removing group membership information from the dataset.\nAlso note that none of the examples satisfy more than one type of fairness. For instance, the demographic parity example does not satisfy equal accuracy or equal opportunity. Take the time to verify this now. In practice, it is not possible to optimize a model for more than one type of fairness: to read more about this, explore the Impossibility Theorem of Machine Fairness. So which fairness criterion should you select, if you can only satisfy one? As with most ethical questions, the correct answer is usually not straightforward, and picking a criterion should be a long conversation involving everyone on your team.\nWhen working with a real project, the data will be much, much larger. In this case, confusion matrices are still a useful tool for analyzing model performance. One important thing to note, however, is that real-world models typically cannot be expected to satisfy any fairness definition perfectly.\n\n\n\n\n\n\nimage-20211002023258614\n\n\n\n\n\nimage-20211002023452065\n\n\n\n\n\nimage-20211002023622243\n\n\n\n\n\n\nA model card is a short document that provides key information about a machine learning model. Model cards increase transparency by communicating information about trained models to broad audiences.\nAI researchers are exploring many ways to communicate key information about models to inform people who use AI systems, people who are affected by AI systems and others.\nModel cards - introduced in a 2019 paper - are one way for teams to communicate key information about their AI system to a broad audience. This information generally includes intended uses for the model, how the model works, and how the model performs in different situations.\n\n\nA model card should strike a balance between being easy-to-understand and communicating important technical information. When writing a model card, you should consider your audience: the groups of people who are most likely to read your model card. These groups will vary according to the AI system’s purpose.\n\n\n\n\n\n\nInclude background information, such as developer and model version.\n\n\n\n\n\nWhat use cases are in scope?\nWho are your intended users?\nWhat use cases are out of scope?\n\n\n\n\n\nWhat factors affect the impact of the model?\n\n\n\n\n\nWhat metrics are you using to measure the performance of the model? Why did you pick those metrics?\n\nFor classification systems – in which the output is a class label – potential error types include false positive rate, false negative rate, false discovery rate, and false omission rate. The relative importance of each of these depends on the use case.\nFor score-based analyses – in which the output is a score or price – consider reporting model performance across groups.\n\n\n\n\n\n\nWhich datasets did you use to evaluate model performance? Provide the datasets if you can.\nWhy did you choose these datasets for evaluation?\nAre the datasets representative of typical use cases, anticipated test cases and/or challenging cases?\n\n\n\n\n\nWhich data was the model trained on?\n\n\n\n\n\nHow did the model perform on the metrics you chose? Break down performance by important factors and their intersections.\n\n\n\n\n\nDescribe ethical considerations related to the model, such as sensitive data used to train the model, whether the model has implications for human life, health, or safety, how risk was mitigated, and what harms may be present in model usage.\n\n\n\n\n\nAdd anything important that you have not covered elsewhere in the model card.\n\n\n\n\n\nThe use of detailed model cards can often be challenging because an organization may not want to reveal its processes, proprietary data or trade secrets. In such cases, the developer team should think about how model cards can be useful and empowering, without including sensitive information.\n\n\n\n\nTo dive deeper into the application of HCD to AI, check out these resources:\n\nLex Fridman’s introductory lecture on Human-Centered Artificial Intelligence\nGoogle’s People + AI Research (PAIR) Guidebook\nStanford Human-Centered Artificial Intelligence (HAI) research\n\nTo continue learning about bias, check out the Jigsaw Unintended Bias in Toxicity Classification competition that was introduced in this exercise.\n\nKaggler Dieter has written a helpful two-part series that teaches you how to preprocess the data and train a neural network to make a competition submission. Get started here.\nMany Kagglers have written helpful notebooks that you can use to get started. Check them out on the competition page.\n\nAnother Kaggle competition that you can use to learn about bias is the Inclusive Images Challenge, which you can read more about in this blog post. The competition focuses on evaluation bias in computer vision.\n\nExplore different types of fairness with an interactive tool.\nYou can read more about equal opportunity in this blog post.\nAnalyze ML fairness with this walkthrough of the What-If Tool, created by the People and AI Research (PAIR) team at Google. This tool allows you to quickly amend an ML model, once you’ve picked the fairness criterion that is best for your use case."
  },
  {
    "objectID": "posts/2021-10-23-probability-and-inferential-statistics-week7/2021-10-23-probability-and-inferential-statistics-week7.html",
    "href": "posts/2021-10-23-probability-and-inferential-statistics-week7/2021-10-23-probability-and-inferential-statistics-week7.html",
    "title": "Probability and Inferenctial Statistics Week 7",
    "section": "",
    "text": "probability_and_inferential_statistics_week_7_Page_1\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_3\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_5\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_6\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_7\n\n\n\n\n\nprobability_and_inferential_statistics_week_7_Page_8\n\n\n\n\n\n\n\n\n20170361_7주차 숙제_Page_1\n\n\n\n\n\n20170361_7주차 숙제_Page_2\n\n\n\n\n\n20170361_7주차 숙제_Page_3\n\n\n\n\n\n20170361_7주차 숙제_Page_4\n\n\n\n\n\n20170361_7주차 숙제_Page_5\n\n\n\n\n\n20170361_7주차 숙제_Page_6\n\n\n\n\n\n20170361_7주차 숙제_Page_7\n\n\n\n\n\n20170361_7주차 숙제_Page_8"
  },
  {
    "objectID": "posts/2021-11-05-probability-and-inferential-statistics-week-9/2021-11-05-probability-and-inferential-statistics-week-9.html",
    "href": "posts/2021-11-05-probability-and-inferential-statistics-week-9/2021-11-05-probability-and-inferential-statistics-week-9.html",
    "title": "Probability and Inferential Statistics Week 9",
    "section": "",
    "text": "image-20211105232132539\n\n\n\n\n\nimage-20211105234953000\n\n\n\n\n\nimage-20211105235018040\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_4\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_5\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_6\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_1\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_2\n\n\n\n\n\nprobability_and_inferential_statistics_week_9_3"
  },
  {
    "objectID": "posts/2021-10-06-kaggle-Intro-to-Deep-Learning/2021-10-06-kaggle-Intro-to-Deep-Learning.html",
    "href": "posts/2021-10-06-kaggle-Intro-to-Deep-Learning/2021-10-06-kaggle-Intro-to-Deep-Learning.html",
    "title": "Kaggle - Intro do Deep Learning",
    "section": "",
    "text": "Using Keras and Tensorflow you’ll learn how to:\n\ncreate a fully-connected neural network architecture\napply neural nets to two classic ML problems: regression and classification\ntrain neural nets with stochastic gradient descent, and\nimprove performance with dropout, batch normalization, and other techniques\n\n\n\nDeep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.\nThrough their power and scalability neural networks have become the defining model of deep learning. Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form.\n\n\n\nAs a diagram, a neuron (or unit) with one input looks like:\n\n\n\nDiagram of a linear unit.\n\n\nThe input is x. Its connection to the neuron has a weight which is w. Whenever a value flows through a connection, you multiply the value by the connection’s weight. For the input x, what reaches the neuron is w * x. A neural network “learns” by modifying its weights.\nThe b is a special kind of weight we call the bias. The bias doesn’t have any input data associated with it; instead, we put a 1 in the diagram so that the value that reaches the neuron is just b (since 1 * b = b). The bias enables the neuron to modify the output independently of its inputs.\nThe y is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron’s activation is y = w * x + b, or as a formula \\(y=wx+b\\).\nThough individual neurons will usually only function as part of a larger network, it’s often useful to start with a single neuron model as a baseline. Single neuron models are linear models.\n\n\n\nComputing with the linear unit.\n\n\n\n\n\nWe can just add more input connections to the neuron, one for each additional feature. To find the output, we would multiply each input to its connection weight and then add them all together.\n\n\n\nThree input connections: x0, x1, and x2, along with the bias.\n\n\nThe formula for this neuron would be \\(y=w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+b\\). A linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane.\n\n\n\nThe easiest way to create a model in Keras is through keras.Sequential, which creates a neural network as a stack of layers. We can create models like those above using a dense layer (which we’ll learn more about in the next lesson).\nWe could define a linear model accepting three input features ('sugars', 'fiber', and 'protein') and producing a single output ('calories') like so:\n\n\n\nimage-20211006183116710\n\n\nWith the first argument, units, we define how many outputs we want. In this case we are just predicting 'calories', so we’ll use units=1.\nWith the second argument, input_shape, we tell Keras the dimensions of the inputs. Setting input_shape=[3] ensures the model will accept three features as input ('sugars', 'fiber', and 'protein').\nWhy is input_shape a Python list? The data we’ll use in this course will be tabular data, like in a Pandas dataframe. We’ll have one input for each feature in the dataset. The features are arranged by column, so we’ll always have input_shape=[num_columns]. The reason Keras uses a list here is to permit use of more complex datasets. Image data, for instance, might need three dimensions: [height, width, channels].\n\n\n\n\nIn this lesson we’re going to see how we can build neural networks capable of learning the complex kinds of relationships deep neural nets are famous for.\nThe key idea here is modularity, building up a complex network from simpler functional units. We’ve seen how a linear unit computes a linear function – now we’ll see how to combine and modify these single units to model more complex relationships.\n\n\nNeural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.\n\n\n\nA stack of three circles in an input layer connected to two circles in a dense layer.\n\n\nYou could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution.\nMany Kinds of Layers A “layer” in Keras is a very general kind of thing. A layer can be, essentially, any kind of data transformation. Many layers, like the convolutional and recurrent layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for feature engineering or just simple arithmetic. There’s a whole world of layers to discover – check them out!\n\n\n\nIt turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.\nWithout activation functions, neural networks can only learn linear relationships. In order to fit curves, we’ll need to use activation functions.\nAn activation function is simply some function we apply to each of a layer’s outputs (its activations). The most common is the rectifier function max(0,x)max(0,x).\n\n\n\nA graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a ‘hinge’ shape like ’_/’.\n\n\nThe rectifier function has a graph that’s a line with the negative part “rectified” to zero. Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.\nWhen we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. (For this reason, it’s common to call the rectifier function the “ReLU function”.) Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b), which we might draw in a diagram like:\n\n\n\nDiagram of a single ReLU. Like a linear unit, but instead of a ‘+’ symbol we now have a hinge ’_/’.\n\n\n\n\n\nNow that we have some nonlinearity, let’s see how we can stack layers to get complex data transformations.\n\n\n\nAn input layer, two hidden layers, and a final linear layer.\n\n\nThe layers before the output layer are sometimes called hidden since we never see their outputs directly.\nNow, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n\n\n\n\n\n\nimage-20211006184240869\n\n\nBe sure to pass all the layers together in a list, like [layer, layer, layer, ...], instead of as separate arguments. To add an activation function to a layer, just give its name in the activation argument.\n\n\n\n\nAs with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal’s 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal’s 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\nIn addition to the training data, we need two more things:\n\nA “loss function” that measures how good the network’s predictions are.\nAn “optimizer” that can tell the network how to change its weights.\n\n\n\nWe’ve seen how to design an architecture for a network, but we haven’t seen how to tell a network what problem to solve. This is the job of the loss function.\nThe loss function measures the disparity between the the target’s true value and the value the model predicts.\nDifferent problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value – calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\nA common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\nThe total MAE loss on a dataset is the mean of all these absolute differences.\n\n\n\nA graph depicting error bars from data points to the fitted line..\n\n\nBesides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\nDuring training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\n\n\n\nWe’ve described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\nVirtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n\nSample some training data and run it through the network to make predictions.\nMeasure the loss between the predictions and the true values.\nFinally, adjust the weights in a direction that makes the loss smaller.\n\nThen just do this over and over until the loss is as small as you like (or until it won’t decrease any further.)\n\n\n\nFitting a line batch by batch. The loss decreases and the weights approach their true values.\n\n\nEach iteration’s sample of training data is called a minibatch (or often just “batch”), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n\n\n\nNotice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn’t always obvious.\nFortunately, for most work it won’t be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is “self tuning”, in a sense). Adam is a great general-purpose optimizer.\n\n\n\nAfter defining a model, you can add a loss function and optimizer with the model’s compile method:\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\nNotice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API – if you wanted to tune parameters, for instance – but for us, the defaults will work fine.\nWhat’s In a Name? The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means “determined by chance.” Our training is stochastic because the minibatches are random samples from the dataset. And that’s why it’s called SGD!\nOne thing you might note for now though is that we’ve rescaled each feature to lie in the interval [0,1][0,1]. As we’ll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.\n\n\n\nimage-20211006185813175\n\n\n\n\n\nimage-20211006185829299\n\n\n\n\n\n\n\n\nimage-20211006190149421\n\n\n\n\n\nimage-20211006190231572\n\n\n\n\n\n\nIn this lesson, we’re going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we’ll examine at the learning curves for evidence of underfitting and overfitting and look at a couple of strategies for correcting it.\n\n\nYou might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can’t actually help the model make predictions. The noise is the part might look useful but really isn’t.\nWe train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model’s performance, we need to evaluate it on a new set of data, the validation data.\nWhen we train a model we’ve been plotting the loss on the training set epoch by epoch. To this we’ll add a plot the validation data too. These plots we call the learning curves. To train deep learning models effectively, we need to be able to interpret them.\n\n\n\nA graph of training and validation loss.\n\n\nNow, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won’t generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.\nIdeally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.\n\n\n\nTwo graphs. On the left, a line through a few data points with the true fit a parabola. On the right, a curve running through each datapoint with the true fit a parabola.\n\n\nThis trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn’t learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.\n\n\n\nA model’s capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\nYou can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones.\n\n\n\nWe mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn’t decreasing anymore. Interrupting the training this way is called early stopping.\n\n\n\nA graph of the learning curves with early stopping at the minimum validation loss, underfitting to the left of it and overfitting to the right.\n\n\nOnce we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won’t continue to learn noise and overfit the data.\nTraining with early stopping also means we’re in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number (more than you’ll need), and early stopping will take care of the rest.\n\n\n\nIn Keras, we include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has a variety of useful callbacks pre-defined, but you can define your own, too.)\n\n\n\nimage-20211006191222318\n\n\n\n\n\nimage-20211006191311344\n\n\n\n\n\n\nThere’s more to the world of deep learning than just dense layers. There are dozens of kinds of layers you might add to a model. (Try browsing through the Keras docs for a sample!) Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts.\nIn this lesson, we’ll learn about a two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways. Both are commonly used in modern architectures.\n\n\nThe first of these is the “dropout layer”, which can help correct overfitting.\nIn the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of “conspiracy” of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.\nThis is the idea behind dropout. To break up these conspiracies, we randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n\n\n\nAn animation of a network cycling through various random dropout configurations.\n\n\nYou could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you’re familiar with random forests as an ensemble of decision trees, it’s the same idea.)\nkeras.Sequential([\n    # ...\n    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n    layers.Dense(16),\n    # ...\n])\n\n\n\nThe next special layer we’ll look at performs “batch normalization” (or “batchnorm”), which can help correct training that is slow or unstable.\nWith neural networks, it’s generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn’s StandardScaler or MinMaxScaler. The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\nNow, if it’s good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\nMost often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get “stuck”. Consider adding batch normalization to your models, especially if you’re having trouble during training.\nlayers.Dense(16),\nlayers.BatchNormalization(),\nlayers.Activation('relu'),\nAnd if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn’s StandardScaler.\n\n\n\n\nNow we’re going to apply neural networks to another common machine learning problem: classification.\n\n\nClassification into one of two classes is a common machine learning problem. These are all binary classification problems.\nBefore using this data we’ll assign a class label: one class will be 0 and the other will be 1. Assigning numeric labels puts the data in a form a neural network can use.\n\n\n\nAccuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: accuracy = number_correct / total.\nThe problem with accuracy (and most other classification metrics) is that it can’t be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in “jumps”. So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.\nNow, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.\nFor classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides. Cross-entropy is a sort of measure for the distance from one probability distribution to another.\n\n\n\nGraphs of accuracy and cross-entropy.\n\n\nThe technical reasons we use cross-entropy are a bit subtle, but the main thing to take away from this section is just this: use cross-entropy for a classification loss; other metrics you might care about (like accuracy) will tend to improve along with it.\n\n\n\nThe cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation.\n\n\n\nThe sigmoid graph is an ‘S’ shape with horizontal asymptotes at 0 to the left and 1 to the right.\n\n\nTo get the final class prediction, we define a threshold probability. Typically this will be 0.5, so that rounding will give us the correct class: below 0.5 means the class with label 0 and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras uses by default with its accuracy metric.\n\n\n\nimage-20211006193905258"
  },
  {
    "objectID": "posts/2021-10-01-tiny-python-project-chapter-5~8/2021-10-01-tiny-python-project-chapter-5~8.html",
    "href": "posts/2021-10-01-tiny-python-project-chapter-5~8/2021-10-01-tiny-python-project-chapter-5~8.html",
    "title": "Tiny Python Project Chapter 5-8",
    "section": "",
    "text": "Chapter 5\n\n\n\nimage-20211001233148710\n\n\n\n\n\nimage-20211001233209804\n\n\n\n\n\nimage-20211001233219993\n\n\n\n\n\nimage-20211001233250181\n\n\n\n\n\nimage-20211001233300367\n\n\n\n\n\nimage-20211001233326520\n\n\n\n\n\nimage-20211001233427568\n\n\n\n\n\nimage-20211001233646494\n\n\n\n\n\nimage-20211001233630917\n\n\n\n\n\nimage-20211001233747120\n\n\n\n\n\nimage-20211001233752815\n\n\n\n\n\nimage-20211001233805801\n\n\n\n\n\nimage-20211001233821350\n\n\n\n\n\nimage-20211001233830874\n\n\n\n\n\nimage-20211001234035570\n\n\n\n\n\nimage-20211001234055468\n\n\n\n\n\nimage-20211001234152805\n\n\n\n\n\nimage-20211001234202552\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@9762e84df2b1>\nDate   : 2021-10-01\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Rock the Casbah',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='text',\n                        type=str,\n                        help='Input string or file')\n\n    parser.add_argument('-o',\n                        '--outfile',\n                        help='Output filename',\n                        metavar='str',\n                        type=str,\n                        default='')\n\n    args = parser.parse_args()\n\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read().rstrip()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    text_arg = args.text\n    filename_arg = args.outfile\n\n    out_fh = open(filename_arg, 'wt') if filename_arg else sys.stdout\n    out_fh.write(text_arg.upper() + '\\n')\n    out_fh.close()\n\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211002001109223\n\n\n\n\n\nimage-20211002001143460\n\n\n\n\n\nimage-20211002001224039\n\n\n\n\n\nimage-20211002001230174\n\n\n\n\n\nimage-20211002001240260\n\n\n\n\n\nimage-20211002001248275\n\n\n\n\nChapter 6\n\n\n\nimage-20211002001500256\n\n\n\n\n\nimage-20211002002046258\n\n\n\n\n\nimage-20211002002135867\n\n\n\n\n\nimage-20211002002202839\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@9762e84df2b1>\nDate   : 2021-10-01\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport sys\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Rock the Casbah',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('file',\n                        metavar='FILE',\n                        nargs='*',\n                        type=argparse.FileType('rt'),\n                        default=[sys.stdin],\n                        help='Input file(s)')\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n\n    totalLineCounts, totalWordsCounts, totalByteCounts = 0, 0, 0\n\n    for fh in args.file:\n        lineCounts, wordsCounts, byteCounts = 0, 0, 0\n\n        for l in fh:      \n            lineCounts += 1;\n            wordsCounts += len(l.split())\n            byteCounts += len(l)\n        \n        totalLineCounts += lineCounts\n        totalWordsCounts += wordsCounts\n        totalByteCounts += byteCounts\n\n        print(f'{lineCounts:8}{wordsCounts:8}{byteCounts:8} {fh.name}')\n    \n    if len(args.file) > 1:\n        print(f'{totalLineCounts:8}{totalWordsCounts:8}{totalByteCounts:8} total')\n        \n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211002004150107\n\n\n\n\n\nimage-20211002004205286\n\n\n\n\nChapter 7\n\n\n\nimage-20211002004318306\n\n\n\n\n\nimage-20211002004347239\n\n\n\n\n\nimage-20211002005659600\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@f139ec3a85b3>\nDate   : 2021-10-01\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Rock the Casbah',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('letter',\n                        metavar='letter',\n                        nargs='+',\n                        help='Letter(s)',)\n\n    parser.add_argument('-f',\n                        '--file',\n                        help='Input file',\n                        metavar='FILE',\n                        type=argparse.FileType('rt'),\n                        default='gashlycrumb.txt')\n\n    return parser.parse_args()\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n\n    sentenceDict = {v[0]: v for v in args.file}\n\n    for letter in args.letter:\n        if (letter.upper() not in sentenceDict.keys()):\n            print('I do not know \\\"{}\\\".'.format(letter))\n            continue\n        print(sentenceDict[letter.upper()], end='')\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211002010825155\n\n\n\n\n\nimage-20211002010836838\n\n\n\n\n\nimage-20211002010853523\n\n\n\n\n\nimage-20211002010900102\n\n\n\n\nChapter 8\n\n\n\nimage-20211002011200723\n\n\n\n\n\nimage-20211002011208396\n\n\n\n\n\nimage-20211002011214020\n\n\n\n\n\nimage-20211002011743894\n\n\n\n\n\nimage-20211002012031902\n\n\n#!/usr/bin/env python3\n\"\"\"\nAuthor : runner <runner@c71caa78d29f>\nDate   : 2021-10-01\nPurpose: Rock the Casbah\n\"\"\"\n\nimport argparse\nimport os\n\n\n# --------------------------------------------------\ndef get_args():\n    \"\"\"Get command-line arguments\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description='Apples and bananas',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('text',\n                        metavar='text',\n                        help='Input text or file')\n\n    parser.add_argument('-v',\n                        '--vowel',\n                        help='The vowel to substitute',\n                        metavar='vowel',\n                        type=str,\n                        default='a',\n                        choices=['a', 'e', 'i', 'o', 'u'])\n\n    args = parser.parse_args()\n    if os.path.isfile(args.text):\n        args.text = open(args.text).read().rstrip()\n\n    return args\n\n\n# --------------------------------------------------\ndef main():\n    \"\"\"Make a jazz noise here\"\"\"\n\n    args = get_args()\n    text_arg = args.text\n    vowel_arg = args.vowel\n\n    if text_arg.isupper():\n        vowel_dict = {v: vowel_arg.upper() for v in ['a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U']}\n    else:\n        vowel_dict = {v: vowel_arg.lower() for v in ['a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U']}\n    new = ''\n    new = text_arg.translate(str.maketrans(vowel_dict))\n\n    print(new)\n\n\n\n# --------------------------------------------------\nif __name__ == '__main__':\n    main()\n\n\n\nimage-20211002013348976\n\n\n\n\n\nimage-20211002013427566\n\n\n\n\n\nimage-20211002013442012\n\n\n\n\n\nimage-20211002013456408\n\n\n\n\n\nimage-20211002013515338\n\n\n\n\n\nimage-20211002013521244"
  },
  {
    "objectID": "posts/2021-10-06-system-programming-week-6/2021-10-06-system-programming-week-6.html",
    "href": "posts/2021-10-06-system-programming-week-6/2021-10-06-system-programming-week-6.html",
    "title": "System Programming Week 6",
    "section": "",
    "text": "새로운 기술을 개발할 때 이전 기술과도 잘 동작하도록 설계하는 특성\n\n\n\nComplex\n\n\n\nInstruction set of Architecture\n\n\n\nAddress, Data, Instruction\nCondition code - Status를 표시해주는 코드\n\n\n\nPassing control\n\nto beginning of procedure code\nback to return point\n\nPassing data\n\nProcedure arguments\nreturn value\n\nMemory management\n\nAllocate during procedure execution\ndeallocate upon return\n\nMechanisms all implemented with machine instructions\nx86-64 implementation of a procedure uses only those mechanisms required.\n\n\n\nimage-20211008085851743\n\n\n\n\n\nRegion of memory managed with stack discipline\nGrows toward lower addresses\nRegister %rsp contains lowest stack address\n\naddress of top element\n\n\n\n\nimage-20211008085944955\n\n\nstack pointer는 stack의 끝을 나타내는 pointer\nstack은 위에서부터 아래로 쌓이고, heap은 아래에서부터 위로 쌓는다\n\n\n\nimage-20211008090441015\n\n\n\n\n\nimage-20211008090447952\n\n\n\n\n\nimage-20211008090455907\n\n\n\n\n\nimage-20211008090506338\n\n\n\n\n\nUse stack to support procedure call and return\nProcedure call: call label\n\nPush return address on stack\nJump to label\n\nReturn address:\n\nAddress of the next instruction right after call\n\nProcedure return: ret\n\npop address from stack\njump to address\n\n\n\n\nimage-20211008090731399\n\n\n\n\n\nimage-20211008090817579\n\n\n\n\n\nimage-20211008091002521\n\n\n\n\n\nimage-20211008091011760\n\n\n\n\n\nimage-20211008091019865\n\n\n\n\n\n\n\n\nimage-20211008091033753\n\n\n\n\n\nimage-20211008091223043\n\n\n\n\n\n\n\n\nimage-20211008091240033\n\n\n\n\n\nimage-20211008091429536\n\n\n\n\n\n\n\nimage-20211008091647636\n\n\n\n\n\n\nContent\n\nReturn information\nLocal storage (if needed)\nTemporary space (if needed)\n\n\n\n\nimage-20211008152722723\n\n\nManagement\n\nSpace allocated when enter procedure\n\nset-up code\nincludes push by call instruction\n\nDeallocated when return\n\nFinish code\nIncludes pop by ret instruction\n\n\n\n\n\nimage-20211008152753528\n\n\n일단 main에서 시작, yoo 호출\n\n\n\nimage-20211008152759216\n\n\n\n\n\nimage-20211008152815288\n\n\n\n\n\nimage-20211008152820416\n\n\n\n\n\nimage-20211008152827847\n\n\n\n\n\nimage-20211008152835905\n\n\n\n\n\nimage-20211008152840783\n\n\n\n\n\nimage-20211008152845893\n\n\n\n\n\nimage-20211008152850367\n\n\n\n\n\nimage-20211008152854981\n\n\n\n\n\nimage-20211008152900343\n\n\n\n\n\n\n\n\nimage-20211008152943633\n\n\nCurrent Stack Frame (Top to Bottom)\n\nArgument build - Parameters for function about to call\nLocal variables if can’t keep in registers\nSaved register context\nOld frame pointer (optional)\n\nCaller Stack Frame\n\nReturn address\n\npushed by call instruction\n\nArguments for this call\n\n\n\n\nimage-20211008153204164\n\n\n\n\n\nimage-20211008153315722\n\n\n\n\n\nimage-20211008153325561\n\n\n\n\n\nimage-20211008153405751\n\n\n\n\n\nimage-20211008153413906\n\n\n\n\n\nimage-20211008153421612\n\n\n\n\n\n\n\n\nimage-20211008153603592\n\n\nCPU내의 register들은 일종의 공유하는 책상 느낌. 내가 쓰다가 남이 쓰면 데이터가 날아갈 수 있는데, 이 데이터를 다른데 보관해둬야 함. 그런데 매번 책상의 모든 데이터를 다 옮기는 것은 불편하니까, 레지스터의 일부는 저장용도로 남겨두는 것.\n\n\n\nimage-20211008153715346\n\n\n경우에 따라 달라서 뭐가 더 좋은지에 대한 구분이 힘듦\n\n\n\nimage-20211008153839239\n\n\n\n\n\nimage-20211008153957750\n\n\n\n\n\nimage-20211008154055255\n\n\n\n\n\nimage-20211008154104800\n\n\n\n\n\n\n\n\nimage-20211008154154471\n\n\n\n\n\nimage-20211008154219682\n\n\n\n\n\nimage-20211008154335199\n\n\n\n\n\nimage-20211008154402627\n\n\n\n\n\nimage-20211008154409542\n\n\n\n\n\nimage-20211008154423654\n\n\n\n\n\n\n\n\nimage-20211008154442628\n\n\n\n\n\n\n\n\nimage-20211008154506229\n\n\n\n\n\n\n\n\nimage-20211008154733853\n\n\n\n\n\nimage-20211008154834886\n\n\n\n\n\nimage-20211008155015980\n\n\n\n\n\nimage-20211008155119244\n\n\nmovl (base, index, scale), Dest\n\n\n\nimage-20211008155227757\n\n\n\n\n\n\n\n\nimage-20211008155356930\n\n\n\n\n\nimage-20211008155434657\n\n\n\n\n\n\n\n\nimage-20211008155511418\n\n\n\n\n\nimage-20211008155544301\n\n\n\n\n\n\n\n\nimage-20211008155707066\n\n\n\n\n\nimage-20211008155715792\n\n\n\n\n\n\n\n\nimage-20211008155829959\n\n\n\n\n\nimage-20211008155842362\n\n\n\n\n\nimage-20211008160001130\n\n\n\n\n\n\n\n\nimage-20211008160031292\n\n\n\n\n\n\n\n\nimage-20211008160507000\n\n\n\n\n\nimage-20211008160606843\n\n\n\n\n\n\n\n\nimage-20211008161026504\n\n\n\n\n\n\n\n\nimage-20211008161101826\n\n\n\n\n\nimage-20211008161251491\n\n\n\n\n\n\n\n\nimage-20211008161301869\n\n\n\n\n\n\n\n\nimage-20211008161332164\n\n\n\n\n\n\n\n\nimage-20211008161401910\n\n\n\n\n\n\n\n\nimage-20211008161436606\n\n\n\n\n\n\n\n\nimage-20211008161600410"
  },
  {
    "objectID": "posts/2021-10-12-CFA-Level-2-Alternative-Investment/2021-10-12-CFA-Level-2-Alternative-Investment.html",
    "href": "posts/2021-10-12-CFA-Level-2-Alternative-Investment/2021-10-12-CFA-Level-2-Alternative-Investment.html",
    "title": "CFA Level 2 Alternative Investment",
    "section": "",
    "text": "image-20211012100005645\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebt\nEquity\n\n\n\n\nPrivate\nMortgage\nDirect investments such as sole ownership, partnerships, and other forms of commingled funds\n\n\nPublic\nMortgage-backed securities\nShares of REITs and REOCs\n\n\n\nREOC (Real Estate Operating Company)\n\n\n\nimage-20211012105538564\n\n\n\n\nPrivate\n\nLarge size\n\nundivided, less liquid\n\nActive management\n\nPublic\n\nSmall size\n\ndivided ownership\ndiversification\n\nPassive management\n\n\n\n\nEquity\n\nHigh risk, high return\nFinancial leverage\nUpside potential\n\nCapital gain\nRental income\n\n\nDebt\n\nSenior than equity\nSafe\nPromised cashflow\nNo upside\n\n\n\n\n\n\n\n\nHeterogeneity - No two property are exactly the same because of location, size, age, construction materials, and lease terms.\nHigh unit value\nActive management - Private real estate investment requires active property management.\nHigh transaction costs - appraisers, lawyers, brokers, and construction personnel\nDepreciation and desirability (매력도)\nCost and availability of debt capital\nLack of liquidity\nDifficulty in determining price\n\nAppraisals are usually to assess real estate values.\nThe combination of limited market participants and lack of knowledge of the local markets makes it difficult for an outsider to value property. As a result, the market is less efficient. However, investors with superior information and skill may have an advantage in exploiting the market inefficiencies.\n\n\nShares of a REIT are actively traded and more likely to reflect market value. In addition, investing in a REIT can provide exposure to a diversified real estate portfolio. Finally, investors don’t need property management expertise.\n\n\n\n\n\n\nimage-20211012110300224\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent income\nCapital appreciation\nInflation hedge\nDiversification - Real estate, especially private equity investment, is less than perfectly correlated with the returns of stocks and bonds.\nTax benefits - In the United States, the depreciable life of real estate is usually shorter than the actual life. As a result, depreciation expense is higher, and taxable income is lower resulting in lower income taxes. Also REITs do not pay taxes in some countries, which allow investors to escape double taxation.\n\n\n\n\n\nBusiness condition\nNew property lead time\nCost and availability of capital - Demand for real estate is reduced when debt capital is scarce and interest rates are high. Real estate price can be affected by capital market forces without changes in demand from tenants.\nUnexpected inflation - Real estate values may not keep up with inflation when markets are weak and vacancy rates are high.\nDemographic factors - The size and age distribution of the local market population, the distribution of socioeconomic groups, and new household formation rates.\nLack of liquidity\nAvailability of information\nManagement expertise - Operational decisions-such as negotiating lease, property maintenance, marketing, and renovating the property\nLeverage - High LTV -> higher risk -> NOI에 큰 영향\nOther factors - natural disaster, and acts of terrorism\n\n\n\n\nReal estate investment has both bond-like and stock-like characteristics.\n\n\n\n\n\n\nimage-20211012145035834\n\n\n\n\n\n\n\n\n\nOffice - Demand is heavily dependent on job growth\n\nIn a gross lease, the owner is responsible for the operating expenses, and in a net lease, the tenant if responsible.\nIn a net lease, the tenant bears the risk if the actual operating expenses are greater than expected.\nSome lease combine features from both gross and net lease.\n\nIndustrial - Demand is heavily dependent on the overall economy. Demand is also affected by import/export activity of the economy.\n\nNet lease are common\n\nRetail - Demand is heavily dependent on consumer spending.\n\nConsumer spending is affected by the overall economy, job growth, population growth, and savings rates.\nAn anchor tenant may receive favorable lease terms to attract them to the property. In turn, the anchor tenant will draw other tenants to the property.\nRetail tenants are often required to pay additional rent once sales reach a certain level. This unique feature is known as a percentage lease or percentage rent.\n\nMulti-family - Demand depends on population growth, especially in the age demographic that typically rents apartments.\n\nDemand is also affected by the cost of buying versus the cost of renting.\n\n\n\n\n\n\n\n\n\n\n\nimage-20211012150031551\n\n\n\n\n\n\n\nBecause of the difficulty in measuring depreciation and obsolescence, the cost approach is most useful when the subject property is relatively new. The cost approach is often used for unusual properties or properties where comparable transactions are limited.\n\n\n\nimage-20211012150045029\n\n\n\n\n\nimage-20211012150056453\n\n\n\n\n\nimage-20211012150103973\n\n\n\n\n\n\nThe concept of highest and best use is important in determining value. The highest and best use of a vacant site is not necessarily the use that results in the highest total value once a project is completed. Rather, the highest and best use of a vacant site is the use that produces the highest implied land value.\nNote that the highest and best use is not based on the highest value when the projects are completed but, rather, the highest implied land value.\n\n\n\nimage-20211012150113845\n\n\n\n\n\n\n\n\n\n\n\nWith the direct capitalization method, value is based on capitalizing the first year NOI of the property using a capitalization rate. With the discounted cash flow method, value is based on the present value of the property’s future cash flows using an appropriate discount rate. \\[\n\\\\\\text{Rental income if fully occupied}\n\\\\+ \\text{Other income}\n\\\\= \\text{Potential gross income}\n\\\\- \\text{Vacancy and collection loss}\n\\\\= \\text{Effective gross income}\n\\\\- \\text{Operating expense}\n\\\\= \\text{Net operating income}\n\\] \n\n\n\n\\[\n\\\\\\text{cap rate} = \\text{discount rate} - \\text{growth rate}\n\\\\\\text{cap rate} = \\frac{\\text{NOI}_{1}}{\\text{value}}\n\\\\V_{0} = \\frac{\\text{NOI}_{1}}{\\text{cap rate}}\n\\\\\\text{cap rate} = \\frac{\\text{NOI}_{1}}{\\text{comparable sales price}}\n\\]\nIt is important to observe several comparable transactions when deriving the cap rate. Implicit in the cap rate derived from comparable transactions are investors’ expectations of income growth and risk. In this case, the cap rate is similar to the reciprocal of the price-earnings multiple for equity securities.\nAll Risks Yield (ARY) - The ARY in the cap rate and will differ from the discount rate if an investor expects growth in rents and value. \\[\n\\\\\\text{value} = V_{0} = \\frac{\\text{rent}_{1}}{\\text{ARY}}\n\\] If rents are expected to increase at a constant rate each year, the internal rate of return (IRR) can be approximately by summing the cap rate and growth rate.\n\n\n\nimage-20211012154503538\n\n\n\n\n\nIf NOI is not representative of the NOI of similar properties because of a temporary issue, the subject property’s NOI should be stabilized. For example, suppose a property is temporarily experiencing high vacancy during a major renovation. In this case, the first-year NOI should be stabilized; NOI should be calculated as if the renovation is complete. Once the stabilized NOI is capitalized, the loss in value, as a result of the temporary decline in NOI, is subtracted in arriving at the value of the property.\n\n\n\nimage-20211012151124707\n\n\n \\[\n\\\\\\text{gross income multiplier} = \\frac{\\text{sales price}}{\\text{gross income}}\n\\\\\\text{value} = \\text{gross income} * \\text{gross income multiplier}\n\\] ball park figure (약식 계산 방법)\nA shortfall of the gross income multiplier is that it ignores vacancy rates and operating expenses. Thus, if the subject property’s vacancy rate and operating expenses are higher than those of the comparable transactions, an investor will pay more for the same rent.\n\n\n\nimage-20211012154616705\n\n\n\n\n\n\n\n\nimage-20211012154520033\n\n\n\n\n\n\n\n\nimage-20211012151506276\n\n\n\n\n\nimage-20211012151512213\n\n\n\n\n\nimage-20211012154633226\n\n\n\n\n\n\n\n\nimage-20211012154541858\n\n\nLease structures can vary by country. For example, in the UK, it is common for tenants to pay all expenses. In this case, the cap rate is known as the ARY. Adjustments must be made when the contract rent (passing or term rent) and the current market rent (open market rent) differ. Once the lease expires, rent will likely be adjusted to the current market rent. In the UK, the property is said to have reversionary potential when the contract rent expires.\n\n\n\nimage-20211012154100535\n\n\n\n\n\nimage-20211012154106156\n\n\nA variation of the term and reversion approach is the layer method.\n\n\n\nimage-20211012154125299\n\n\n\n\n\nimage-20211012154131166\n\n\nUsing the term and reversion approach and the layer method, different cap rates were applied to the term rent and the current market rent after review. Alternatively, a single discount rate, known as the equivalent yield, could have been used. The equivalent yield is an average, although not a simple average, of the two separate cap rates.\nUsing the discounted cash flow method requires the following estimates and assumptions.\n\nProject income from existing leases\nLease renewal assumptions\nOperating expense assumptions - Operating expenses can be classified as fixed, variable, or a hybrid of the two.\nVacancy assumption\nEstimated resale price\nAppropriate discount rate - The discount rate should be higher than the mortgage rate because more risk.\n\n\n\n\nimage-20211012154420042\n\n\n\n\n\nimage-20211012154426026\n\n\n\n\n\n\n\n\n\nimage-20211012154650128\n\n\nUnder the direct capitalization method, a cap rate or income multiplier is applied to first-year NOI. Implicit in the cap rate or multiplier are expected increases in growth.\nUnder the discounted cash flow (DCF) method, the future cash flows, including the capital expenditures and terminal value, are projected over the holding period and discounted to present at the discount rate. Future growth of NOI is explicit in the DCF method.\nBecause of the inputs required, the DCF method is more complex than the direct capitalization method, as it focuses on NOI over the entire holding period and not just NOI in the first year. Choosing the appropriate discount rate and terminal cap rate are crucial as small differences in the rates can significantly affect value.\nFollowing are some common errors made using the DCF method:\n\nThe discount rate does not adequately capture risk.\nIncome growth exceeds expense growth.\nThe terminal cap rate and the going-in-cap rate are not consistent.\nThe terminal cap rate is applied to NOI that is atypical.\nThe cyclicality of real estate markets is ignored.\n\n\n\n\n\n\n\nStep 1: Estimate the market value of the land.\nStep 2: Estimate the building’s replacement cost. Replacement cost is based on current construction costs and standards and should include any builder/developer’s profit.\n\nReplacement cost refers to the cost of a building having the same utility but constructed with modern building materials. (지금 구할 수 있는 소나무 사용) Reproduction cost refers to the cost of reproducing an exact replica of the building using the same building materials, architectural design, and quality of construction. (400년 된 소나무 사용) Replacement cost is usually more relevant for appraisal purposes because reproduction cost may be uneconomical.\n\nStep 3: Deduct depreciation including physical deterioration, functional obsolescence, locational obsolescence, and economic obsolescence. Physical deterioration is related to the building’s age and occurs as a result of normal wear and tear over time. Physical deterioration can be curable or incurable.\nAn item is incurable if the problem is not economically feasible to remedy. For example, the cost of fixing a structural problem might exceed the benefit of the repair. Since a incurable defect would not be fixed, depreciation can be estimated based on the effective age of the property relative to its total economic life. For example, the physical depreciation of a property with an effective age of 30 years and a 50-year total economic life is 60% (30 year effective age / 50 year economic life).\nFunctional obsolescence is the loss in value resulting from defects in design that impairs a building’s utility.\nLocational obsolescence occurs when the location is no longer optional. A prison is built down the street making the location of the apartment complex less desirable.\nEconomic obsolescence occurs when new construction is not feasible under current economic conditions. This can occur when rental rates are not sufficient to support the property.\n\n\n\nimage-20211012155820063\n\n\n\n\n\nimage-20211012155826189\n\n\nBecause of the difficulty in measuring depreciation and obsolescence, the cost approach is most useful when the subject property is relatively new.\nThe cost approach is sometimes considered the upper limit of value since an investor when never pay more than the cost to build a comparable building.\n\n\n\nimage-20211012160342110\n\n\n\n\n\n\n\nimage-20211012155931379\n\n\n\n\n\nimage-20211012155938894\n\n\nThe sales comparison approach is most useful when there are a number of properties similar to the subject that have been recently sold, as is usually the case with single-family homes. Even in an active market, there may be limited transactions of specialized property types, such as regional malls and hospitals. The sales comparison approach assumes purchaser are acting rationally. However, there are times when purchaser become overly exuberant and market bubbles occur.\n\n\n\nimage-20211012160351917\n\n\n\n\n\nAn important part of the appraisal process involves determining the final estimate of value by reconciling the value differences in the three approaches.\nFor example, an appraiser might apply a higher weight to the value obtained with the sales comparison approach when the market is active with plenty of comparable properties. Alternatively, if the subject property is old and estimating depreciation is difficult, an appraiser might apply a lower weight to the cost method.\n\n\n\n\n\nLease review and rental history\nConfirm the operating expenses by examining bills\nReview cash flow statement\nObtain an environmental report to identify the possibility of contamination\nPerform a physical/engineering inspection\nInspect the title and other legal documents for defeiciencies\nHave the property surveyed to confirm the boundaries and identify easements (사용권)\nVerify compliance with zoning laws, building codes, and environmental regulations\nVerify payment of taxes, insurance, special assessments, and other expenditures\n\n\n\n\n\n\nA popular index in the United States in the NCREIF Property Index (NPI). Member of NCREIF, mainly investment managers and pension fund sponsors, submit appraisal data quarterly, and NCREIF calculates the return as follows: \\[\n\\\\\n\\text{return} = \\frac{NOI - \\text{capital expenditures} + (\\text{end market value} - \\text{beg market value})}{\\text{beginning market value}} \\\\\n\\] Appraisal-based indices tend to lag actual transactions because actual transactions occur before appraisals are performed. Thus, a change in price may not be reflected in appraised values until the next quarter or longer if a property is not appraised every quarter. Also, appraisal lag tends to smooth the index; that is, reduce its volatility, much like a moving average reduces volatility. Finally, appraisal lag results in lower correlation with other asset classes. Appraisal lag can be adjusted by unsmoothing the index or by using a transaction-based index.\n\n\n\nimage-20211012161208117\n\n\n\n\n\nA repeat-sales index relies on repeat sales of the same property.\nA hedonic index requires only one sale. A regression is developed to control for differences in property characteristics such as size, age, location, and so forth.\n\n\n\n\n\n\n\\[\n\\\\\n\\text{DSCR} = \\frac{\\text{first-year\\, NOI}}{\\text{debt service}} \\\\\n\\text{LTV} = \\frac{\\text{loan amount}}{\\text{appraisal value}} \\\\\n\\]\n\n\n\n\\[\n\\\\\n\\text{equity dividend rate} = \\frac{\\text{first year cash flow}}{\\text{equity}} \\\\\n\\]\n\n\n\nIn order to calculate the IRR with leverage, we need to consider the cash flows over the entire holding period including the change in value of the original investment.\n\n\n\nimage-20211012162703898\n\n\nWe can see the effects of leverage by calculating an unleveraged IRR. In this case, the initial cash outflow is higher because no debt is incurred.\n\n\n\nimage-20211012162756918\n\n\n\n\n\nimage-20211012162821449\n\n\n\n\n\n\n\n\nREITs are tax-advantaged companies (trusts) that are for most part exempt from corporate income tax. Equity REITs are actively managed, own income-producing real estate. Still diversifying holdings by geography and other factors.\n\n\n\nREOCs are not tax-advantaged; rather, they ordinarily corporations that own real estate. A business will form as a REOC if it is ineligible to organized as REIT. For example, the firm may intend to develop and sell real estate rather than generating cash flow rental payments, or the firm may be based in a country that does not allow tax-advantaged REITs.\n\n\n\nResidential or commercial mortgage-backed securities are publicly traded asset-backed securitized debt obligations that receive cash flows from an underlying pool of mortgage loans. These loans may be for commercial properties (in the case of CMBS) or on residential properties (in the case of RMBS). Real estate debt securities represent a far larger aggregate market value than do publicly traded real estate equity securities.\n\n\n\nMortgage REITs invest primarily in mortgage, mortgage securities, or loans that are secured by real estate.\n\n\n\n\n\n\n\nSuperior liquidity - Because REIT and REOC shares trade daily on a stock exchange.\nLower minimum investment - REIT or REOC shares trade for much smaller dollar amounts.\nLimited liability - The financial liability of a REIT investor is limited to the amount invested.\nAccess to premium properties - Some prestigious properties, such as high-profile shopping malls or other prominent or landmark buildings, are difficult to invest in directly.\nActive professional management - REITs and REOCs employ professional management to control expenses, maximize rents and occupancy rates, and sometimes to acquire additional properties.\nProtections accorded to publicly traded securities - Investors benefit from these securities regulations and from having a board overseeing the management on behalf of investors. Additionally, having public investors monitor the actions of management and the board of directors leads to financial and operating efficiency.\nGreater potential for diversification - Through REITs, however, an investor can diversify across property type and geographical location.\n\n\n\n\n\nExemption from taxation - As long as certain requirements are met, REITs enjoy favorable taxation, because a major part of REIT distributions are treated as a return of capital and are thus not taxable.\nPredictable earnings - REITs’ rental income is fixed by contracts, unlike the income of companies of other industries.\nHigh yield - REITs are obligated to pay out mot of their taxable income as dividends. Because of this high income payout ratio, the yields of REITs are higher than the yields on most other publicly traded equities.\n\n\n\n\n\nTaxes versus direct ownership - Depending on local laws, investors that make direct investments in properties may be able to deduct losses on real estate from taxable income or replace one property for a similar property (“like-kind exchange” in the US) without taxation on the gains.\nLack of control\nCosts of a publicly traded corporate structure\nPrice is determined by the stock market - While the appraisal-based value of a REIT may be relative stable, the market-determined price of a REIT share is likely to be much more volatile. Appraisals tend to be infrequent and backward-looking, while the stock market is continuous and reflects forward-looking values.\nStructural conflicts of interest - When the opportunity arises to sell properties or take on additional borrowing, a particular action may have different tax implications for REIT shareholders and for the general partners, which may tempt the general partners to act in their own interest, rather than in the interest of all stakeholders.\n\n\n\n\n\nLimited potential for income growth - REIT’s high rates of income payout limit REIT’s ability to generate future growth through reinvestment.\nForced equity issuance - When credit is difficult to obtain, a REIT may be forced to issue equity at a disadvantageous price.\nLack of flexibility - The rules that qualify REITs for favorable taxation also have a downside: REITs are prevented from making certain kinds of investments and from retaining most of their income. These limits may prevent REITs from being as profitable as they might otherwise be. REOCs, on the other hand, do not need to meet these requirements, and thus are free to retain income and devote those funds to property development when the REOC managers see attractive opportunities. REOCs are also not restricted in their use of leverage.\n\n\n\n\n\n\n\nRelative importance of factors affecting REIT economic value\n\nShopping/Retail - Retail sales growth\nOffice - Job creation\nResidential - Population growth\nHealthcare - Population growth\nIndustrial - Retail sales growth\nHotel - Job creation\nStorage - Population growth\n\n\n\n\n\nExemption from corporate-level income taxes - In order to gain this status, REITs are required to distribute almost all of the REIT’s otherwise-taxable income, and a sufficient portion of assets and income must relate to rental income-producing real estate.\nHigh dividend yield - To maintain their tax-exempt status, REIT’s dividend yields are generally higher than yields on bonds or other equities.\nLow income volatility - REIT’s revenue streams tend to be relatively stable.\nSecondary equity offerings - Since REITs distribute most earnings, they likely to finance additional real estate acquisitions by selling additional shares.\n\n\n\n\nThe most risky REITs are those that invest in property sectors where significant mismatches between supply and demand are likely (particularly health care, hotel, and office REITs), as well as those sectors where the occupancy rates are most likely to fluctuate within a short period of time (especially hotels). Other items to consider in assessing the riskiness of a REIT relate to the properties’ financing, the leases that are in place, and the properties’ locations and quantity.\n\n\n\n\nRemaining lease terms - Short remaining lease terms provide an opportunity to raise rents in an expansionary economy, while long remaining lease terms are advantageous in a declining economy or softening rental market. Initial lease terms vary with the type of property-industrial and office buildings and shopping centers generally have long lease terms, while hotels and multi-family residential real estate have short lease terms.\nInflation protection - The level of contractual hedging against rising general price levels should be evaluated.\nIn-place rents versus market rents - An analyst should compare the rents that a REIT’s tenants are currently paying (in-place rents) with current rents in the market. If in-place rents are high, the potential exists for cash flows to fall going forward.\nCosts to re-lease space - When a lease expires, expenses typically incurred include lost rent, any new lease incentives offered, the costs of tenant-demanded improvements, and broker commissions.\nTenants’ financial health\nTenant concentration in the portfolio - Risk increases with tenant concentration.\nNew competition\nBalance sheet analysis - the amount of leverage, the cost of debt, and the debt’s maturity\nQuality of management\n\n\n\n\n\n\n\nInvestment characteristics\n\nStable revenue stream over the short term\n\nPrincipal risks\n\nDepends on consumer spending\n\nDue diligence considerations\n\nPer-square-foot sales and rental rates\nAnchor tenant - long-term fixed lease\nSmaller tenant - percentage lease\n\n\n\n\nInvestment characteristics\n\nLong (5-25 years) lease terms\nStable year-to-year income\n\nPrincipal risks\n\nChanges in office vacancy and rental rates\n\nDue diligence considerations\n\nNew space under construction\nQuality of office space (location, condition of building, and so on)\n\n\n\n\nInvestment characteristics\n\nOne-year leases\nStable demand\n\nPrincipal risks\n\nCompetition\nInducements (전입)\nRegional economy\nInflation in operating costs\n\nDue diligence considerations\n\nDemographics and income trends\nAge and competitive appeal\nCost of home ownership\nRent controls by local government\n\n\n\n\nInvestment characteristics\n\nLess cyclical than some other REIT types\n5-25 year net leases\nChange in income and values are slow\n\nPrincipal risks\n\nShifts in the composition of local and national industrial bases and trade\n\nDue diligence Considerations\n\nTrends in tenants’ requirements\nObsolescence of existing space\nNeed for new types of space\nProximity to transportation (airport, seaport, road)\nTrends in local supply and demand\n\n\n\n\nInvestment characteristics\n\nVariable income\nSector is cyclical because it is not protected by long-term leases\n\nPrincipal risks\n\nExposed to business-cycle\nChanges in business and leisure travle\nExposure to travel distruptions\n\nDue diligence considerations\n\nOccupancy, room rates, and operating profit margins vs. industry average\nRevenue per available room (RevPAR)\nTrends in forward bookings\nMaintenance expenditures\nNew construction in local markets\nFinancial leverage\nMargin level\nF&B sales\n\n\n\n\nself-storage, 개인 물건 보관 창고\nInvestment characteristics\n\nSpace is rented under gross leases and on a monthly basis\n\nPrincipal risks\n\nEase of entry can lead to overbuilding\n\nDue diligence considerations\n\nConstruction of new competitive facilities\nTrends in housing sales\nDemographic trends\nNew business start-up activity\nSeasonal trends in demand for storage facilities that can be significant in some markets\n\n\n\n\n\nNAVPS (net asset value per share) is the (per-share) amount by which assets exceed liabilities, using current market values rather than accounting book values. NAVPS is generally considered the most appropriate measure of the fundamental value of REITs (and REOCs). If the market price of a REIT varies from NAVPS, this is seen as a sign of over- or undervaluation.\n\n\n\nimage-20211012182300807\n\n\n\n\n\nimage-20211012182315331\n\n\n \\[\n\\\\\n\\text{step 1: }\\text{property value} = \\frac{\\text{1st\\, year\\, NOI}}{\\text{cap rate}} \\\\\n\\text{step 2: }\\text{Total Net Asset Value} = \\text{property value} + \\text{other tangible assets} - \\operatorname{Liabilities} \\\\\n\\text{step 3: }\\text{NAVPS} = \\frac{\\text{Total NAV}}{\\text{number of outstanding shares}} \\\\\n\\]\n\n\n\n\n\n\\[\n\\\\\n\\text{Accounting net earnings}\n\\\\+ \\text{Depreciation and amortization expense}\n\\\\- \\text{Gains from sales of property}\n\\\\+ \\text{Losses from sales of property}\n\\\\= \\text{Funds from operations} \\\\\n\\]\nDepreciation is added back under the premise that accounting depreciation often exceeds economic depreciation for real estate. Gains from sales of property are excluded because these are not considered to be part of continuing income.\n\n\n\nAFFO is also known as cash available for distribution (CAD) or funds available for distribution (FAD). \\[\n\\\\\\text{FFO\\, (funds\\, from\\, operations)}\n\\\\- \\text{Non-cash\\, (straight-line)\\, rent\\, adjustment}\n\\\\- \\text{Recurring\\, maintenance-type\\, capital\\, expenditures\\, and\\, leasing\\, commissions}\n\\\\= \\text{AFFO\\, (adjusted\\, funds\\, from\\, operations)} \\\\\n\\] Straight-line rent refers not to the cash rent paid during the lease but rather to the average contractual rent over a lease period-the two figures differ by non-cash rent, which reflects contractually-increasing rental rates. Capital expenditures related to maintenance, as well expenses related to leasing the space in properties, are subtracted from FFO because they represent costs that must be expended in order to maintain the value of the properties.\nAFFO is considered a better measure of economic income than FFO because AFFO considered the capital expenditures that are required to sustain the property’s economic income. However, FFO is more frequently cited in practice, because AFFO relies more on estimates and is considered more subjective.\n\n\n\n\n\n\n일종의 intrinsic value in private market <-> public market value, 통상 public market value가 premium을 가짐 (liquidity)\nNet asset value is an indication of a REIT’s assets to a buyer in the private market. There have historically been significant differences between NAV estimates and the prices at which REITs actually trade.\nIf, in general, the market is trading at a premium to NAVPS, a value investor would select the investments with the lowest premium (everything else held constant).\n\n\n\nThere are three key factors that impact that price-to-FFO and price-to-AFFO of REITs and REOCs:\n\nExpectations for growth of FFO and AFFO.\nThe level of risks inherent in the underlying real estate.\nRisk related to the firm’s leverage and access to capital.\n\n\n\n\nDividend discount and discounted cash flow models of valuation are appropriate for use with REITs and REOCs, because these two investment structures typically pay dividends and thereby return a high proportion of their income to investors. For dividend discount models, an analysts will develop near-term, medium-term, and long-term growth forecasts and then use these values at the basis for two- or three-stage dividend discount models. To build a discounted cash flow model, analysts will generally create intermediate-term cash flow projections plus a terminal value that is developed using historical cash flow multiples.\n\n\n\nimage-20211012184113171\n\n\n\n\n\n\n\n\n\nimage-20211012184030323\n\n\n\n\n\nimage-20211012184035719\n\n\n\n\n\nimage-20211012184041661\n\n\n\n\n\nimage-20211012184124743\n\n\n\n\n\nThe sources of this increased value are thought to come from the following:\n\nThe ability to re-engineering the portfolio company and operate it more efficiently.\nThe ability to obtain debt financing on more advantageous terms.\nSuperior alignment of interests between management and private equity ownership.\n\n\n\nExperienced industry CEOs, CFOs, and other former senior executives. These executives can share their expertise and contracts with portfolio company management.\n\n\n\nA second source of added value is from more favorable terms on debt financing. In PE firms, debt is more heavily utilized and is quoted as a multiple of EBITDA (earnings before interest, taxes, depreciation, and amortization) as opposed to a multiple of equity, as for public firm.\nThe central proposition of the Modigliani-Miller theorems is that the use of debt versus equity is inconsequential for firm value. However, once the assumption of no taxes is removed from their model, the tax savings from the use of debt increases firm value. The use of greater amount of financial leverage may increase firm value in the case of private equity firms. Because these firms have a reputation for efficient management and timely payment of debt interest, this helps to allay concerns over their highly leveraged positions and helps maintain their access to the debt markets.\nAccording to this view, the requirement to make interest payments forces the portfolio companies to use free cash flow more efficiently because interest payments must be made on the debt.\nMuch of the debt financing for private equity firms comes from the syndicated loan market, but the debt is often repackaged and sold as collateralized loan obligations (CLOs). Private equity firms may also issue high-yield bonds which are repackaged as collateralized debt obligations (CDOs).\nA third source of value added for PE firms is the alignment of interests between private equity owners and the managers of the portfolio companies they own.\n\n\n\n\nIn many private equity transactions, ownership and control are concentrated in the same hands. In buyout transactions, management often has a substantial stake in the company’s equity. In many venture capital investments, the private equity firm offers advice and management expertise.\nIn private equity firms, managers are able to focus more on long-term performance because, unlike public companies, private companies do not face the scrutiny of analysis, shareholders, and the broader market. This also allows the private equity firms to hire managers that are capable of substantial restructuring efforts.\n\n\nPrivate equity firms use a variety of mechanisms to align the interests of the managers of portfolio companies with the private equity firm’s interests.\n\nCompensation - Compensation that is closely linked to the company’s performance.\nTag-along, drag-along clauses\nBoard representation - The private equity firm is ensured control through board representation.\nNoncompete clauses - Company founders must agree to clauses that prevent them from competing against the firm within a prespecified period of time.\nPriority in claims - Private equity firms receive their distributions before other owners, often in the form of preferred dividends. They also have priority on the company’s assets if the portfolio company is liquidated.\nRequired approvals - Changes of strategic importance (e.g., acquisitions, divestitures, and changes in the business plan) must be approved by the private equity firm.\nEarn-outs - There are used predominantly in venture capital investments. Earn-outs tie the acquisition price paid by the private equity firm to the portfolio company’s future performance over a specified time period.\n\n\n\n\n\n경영권 획득 여부 차이\n\n\n\n\n\n\n\n\n\n\nCharacteristics\nVenture Capital Investments\nBuyout Investments\n\n\n\n\nCash Flows\nLow predictability with potentially unrealistic projections\nStable and predictable cash flows\n\n\nProduct Markets\nNew product market with uncertain future\nStrong market position with a possible niche position\n\n\nProducts\nProduct is based on new technology with uncertain prospects\nEstablished products\n\n\nAsset Base\nWeak\nSubstantial base that can serve as collateral\n\n\nManagement Team\nNew team although individual members typically have a strong entrepreneurial record\nStrong and experienced\n\n\nFinancial Leverage\nLow debt use with a majority of equity financing\nHigh amount of debt with a large percentage of senior debt and substantial amounts of junior and mezzanine debt\n\n\nRisk Assessment\nRisk is difficult to estimate due to new technologies, markets, and company history\nRisk can be estimated due to industry and company maturity\n\n\nExit\nExit via IPO or company sale is difficult to forecast\nExit is predictable\n\n\nOperations\nHigh cash burn rate required due to company and product immaturity\nPotential exists for reduction in inefficiencies\n\n\nWorking Capital Required\nIncreasing requirements due to growth\nLow requirements\n\n\nDue Diligence Performed by Private Equity Firms\nPrivate equity firms investigate technological and commercial prospects; investigation of financials is limited due to short history\nPrivate equity firms perform extensive due diligence\n\n\nGoal Setting\nGoals are milestone set in business plan and growth strategy\nGoal reference cash flows, strategic plan, and business plan\n\n\nPrivate Equity Investment Returns\nHigh returns come from a few highly successful investments with write-offs from less successful investments\nLow variability in the success of investments with failures being rare\n\n\nCapital Market Presence\nGenerally not active in capital markets\nActive in capital markets\n\n\nSales Transactions\nMost companies are sold as a result of the relationship between venture capital firm and entrepreneurs\nCompanies are typically sold in an auction-type process\n\n\nAbility to Grow Through Subsequent Funding\nCompanies are less scalable as subsequent funding is typically smaller\nStrong performances can increase subsequent funding amounts\n\n\nSource of General Partner’s Variable Revenue\nCarried interest is most common, transaction and monitoring fees are less common\nCarried interest, transaction fees, and monitoring fees\n\n\n\n\n\n\nPublic companies are bough and sold on regulated exchange daily. Private companies, however, are bought by buyers with specific interests at specific points in time, with each potential buyer possibly having a different valuation for the company. Furthermore, valuing a private company is more difficult than valuing public companies because PE firms often transform and reengineering the portfolio company such that cash flow estimates are difficult to obtain.\n\n\n\n\nDiscounted cash flow (DCF) analysis is most appropriate for companies with a significant operating history.\nA relative value or market approach applies a price multiple, such as the price-earnings ratio. This approach requires predictable cash flows and a significant history.\nReal option analysis is applicable for immature companies with flexibility in their future strategies.\nReplacement cost - It is generally not applicable to mature companies.\nVenture capital method and the leveraged buyout method\n\n\n\n\nimage-20211012195838937\n\n\n\n\n\nOther considerations for valuing private equity portfolio companies are control premiums, country risk, and marketability and illiquidity discounts.\n\n\n\nTo value private equity portfolio companies, many investors use market data from similar publicly traded companies, most commonly the price multiples from comparable public companies. However, it is often difficult to find public companies at the same stage of development, same line of business, same capital structure, and same risk. A decision must also be made as to whether trailing or future earnings are used. For these reasons, a relative value or market approach should be used carefully.\n\n\n\nMarket data is also used with discounted cash flow (DCF) analysis, with beta and the cost of capital estimated from public companies while adjusting for differences in operating and financial leverage between the private and public comparables. In DCF analysis, an assumption must be made regarding the company’s future value. Typically a terminal value is calculated using a price multiple of the company’s EBITDA.\n\n\n\n\n\n\nimage-20211012195849511\n\n\n\n\n\nimage-20211012195945557\n\n\n\n\nIn a buyout transaction, the buyer acquires a controlling equity position in a target company. Buyouts include takeovers, management buyouts (MBOs), and leveraged buyouts(LBOs). This review focuses on LBOs, in which a high amount of debt is used to finance a substantial portion of the acquisition. The financing of a LBO typically involves senior debt, junk bonds, equity, and mezzanine finance. Mezzanine finance is a hybrid debt and equity and can be structured to suit each particular transaction.\n\n\n\n\nThe view of the LBO transaction, referred to as the LBO model, is not a form of valuation but rather a method of factoring in the company’s capital structure and other parameters to determine the return the private equity firm should expect from the transaction. The objective is not to value the company but to determine the maximum price in negotiation that the private equity firm should pay for its stake.\n\n\n\n\n\n\nimage-20211012195905223\n\n\nthree main inputs:\n\nThe target company’s forecasted cash flows\nThe expected returns to the providers of the financing\nThe total amount of financing\n\n\n\n\n\n\n\n\\[\n\\\\\\text{investment cost} + \\text{earnings growth} + \\text{increase in price multiple} + \\text{reduction in debt} = \\text{exit value}\n\\]\n\n\n\nimage-20211012211657003\n\n\n\n\n\nimage-20211012211702287\n\n\nThe components of the return are:\n\nThe return on the preference shares for the private equity firm.\nThe increased multiple upon exit.\nThe reduction in the debt claim.\n\nIn most LBOs, most of the debt is senior debt that will amortize over time. In the preceding example, the debtholders’ claim on assets was reduced from $600 to $250. The use of debt in this example is advantageous and magnifies the returns to the equityholders.\n\n\n\nimage-20211012212834424\n\n\n\n\n\n\n\n\nimage-20211012212847111\n\n\n\n\nThe post-money valuation of the investee company is: \\[\n\\\\\\operatorname{PRE} + \\operatorname{INV} = \\operatorname{POST}\n\\\\\\text{The ownership proportion of the venture capital investor} = \\operatorname{INV} / \\operatorname{POST}\n\\]\n\n\n\n\nThe pre-money valuation and investment will be negotiated between the investee company and the VC investor. Additionally, the VC investor should keep in mind that his ownership could be diluted in the future due to future financing, conversion of convertible debt into equity, and the issuance of stock options to management.\nIt is difficult to forecast the cash flows for a VC portfolio company. Discounted cash flow analysis (the income approach) is not usually used as the primary valuation method for VC companies. It is also difficult to use a relative value or market approach. This is because a VC company is often unique, and there many be no comparable companies to estimate a benchmark price multiple from. A replacement cost approach may also be difficult to apply. Alternative methodologies include real option analysis and the venture capital method.\nTo estimate the pre-money valuation, the VC investor typically examines the company’s intellectual property and capital, the potential for the company’s products, and its intangible assets.\n\n\n\n\n\n\n\n\n\n\n\nValuation Issue\nBuyout\nVenture Capital\n\n\n\n\nApplicability of DCF Method\nFrequently used to estimate value of equity\nLess frequently used as cash flows are uncertain\n\n\nApplicability of Relative Value Approach\nUsed to check the value from DCF analysis\nDifficult to use because there may be no truly comparable companies\n\n\nUse of Debt\nHigh\nLow as equity is dominant form of financing\n\n\nKey Drivers of Equity Return\nEarnings growth, increase in multiple upon exit, and reduction in the debt\nPre-money valuation, investment, and subsequent dilution\n\n\n\nValuation methodologies for buyouts need to factor in the level and pattern of leverage over the investment term.\nEarn-out - target company의 상승을 따라가는 것\n\n\n\n\n\n\nThe means and timing of the exit strategy influence the exit value.\n\nan initial public offering (IPO)\nsecondary market sale\nmanagement buyout (MBO)\nliquidation\n\n\n\n\nAn IPO usually results in the highest exit value due to increased liquidity, greater access to capital, and the potential to hire better quality managers. However, an IPO is less flexible, more costly, and a more cumbersome process than the other alternatives.\nIPOs are most appropriate for companies with strong growth prospects and a significant operating history and size. The timing of an IPO is key.\nVC\n\n\n\nIn a secondary market sale, the company is sold to another investor or to another company interested in the purchase for strategic reasons. Secondary market sales from one investor to another are quite frequent, especially in the case of buyouts. VC portfolio companies are sometimes exited via a buyout to another firm, but VC companies are usually too immature to support a large amount of debt. Secondary market sales result in the second highest company valuation after IPOs.\nTrade sale, M&A\nBuyout\n\n\n\nIn an MBO, the company is sold to management, who utilize a large amount of leverage. The resulting high leverage may limit management’s flexibility.\n\n\n\nWhen the company is deemed no longer visible and usually results in a low value. There is potential for negative publicity as a result of displaced employees and from the obvious implications of the company’s failure to reach its objectives.\n\n\n\nFor example, if a portfolio company cannot be sold due to weak capital markets, the private equity firm may want to consider buying another portfolio company at depressed prices, merging the two companies, and waiting until capital market conditions improve to sell both portfolio companies as one.\nWhen an exit is anticipated in the next year or two, the exit valuation multiple can be forecasted without too much error. Beyond this time horizon, however, exit multiples become much more uncertain and stress testing should be performed on wide range of possible values.\n\n\n\n\n\n\n\nimage-20211012222056320\n\n\n\n\nThe limited partners (LPs) provide funding and do not have an active role in the management of the investments. Their liability is limited to what they have invested. The general partner (GP) in a limited partnership is liable for all the firm’s debts and, thus, has unlimited liability. The GP is the manager of the fund.\nAnother form of private equity fund structure is the company limited by shares. It offers better legal protection to the partners, depending on the jurisdiction. Most fund structures are closed end, meaning that investors can only redeem the investment at specified time periods.\nThe private equity firm usually spends a year or two raising funds. Funds are then drawn down (capital call) for investment, after which returns are realized. Most private equity funds last 10 to 12 years but can have their life extended another 2 to 3 years.\n\n\n\nPrivate equity investments are often only available to qualified investors, the definition of which depends on the jurisdiction. In the United States, the individual must have at least $1 million in assets.\nIf the fund is oversubscribed, the GP has greater negotiating power.\nThe terms of the fund should be focused towards aligning the interests of the GP and LPs and specifying the compensation of the GP.\n\n\n\nCommitted capital is the amount of funds promised by investors to private equity funds. Paid-in capital is the amount of funds actually received from investors (also referred to as invested capital).\n\n\nThese are fees paid to the GP on annual basis as a percent of committed capital. Management fees could instead be based on NAV or paid-in capital.\n\n\n\nThere are paid by third parties to the GP in their advisory capacity (e.g., for investment banking services, such as arranging a merger).\n\n\n\nThis is the GP’s share of the fund profits.\n\n\n\nThis specifies the allocation of equity between stockholders and management of the portfolio company and allows management to increase their allocation, depending on company performance.\n\n\n\nThis is the IRR that the fund must meet before the GP can receive carried interest.\n\n\n\nThe stated total maximum size of the PE fund, specified as an absolute figure. It is a negative signal if actual funds ultimately raised are significantly lower than targeted.\n\n\n\nThis is the year the fund was started and facilitates performance comparisons with other funds.\n\n\n\nAs discussed previously, this is the life of the firm and is usually ten years.\n\n\n\nimage-20211012222106796\n\n\n\n\n\nimage-20211012214827897\n\n\n\n\n\n\n\n\nIf a key named executive leaves the fund or does not spend a sufficient amount of time at the fund, the GP may be prohibited from making additional investments until another key executive is selected.\n\n\n\nThis specified the fund performance information that can be disclosed. Note that the performance information for underlying portfolio companies is typically not disclosed.\n\n\n\nIn a deal-by-deal method, carried interest can be distributed after each individual deal. The disadvantage of this method from the LP’s perspective is that one deal could earn $10 million and another could lose $10 million, but the GP will received carried interest on the first deal, even though the LPs have not earned an overall positive return.\nIn the total return method, carried interest is calculated on the entire portfolio. There are two variants of the total return method: 1) carried interest can be paid only after the entire committed capital is returned to LPs; or 2) carried interest can be paid when the value of the portfolio exceeds invested capital by some minimum amount.\n\n\n\n\n\n\nThis clause allows a GP to be fired if a supermajority (usually 75% or more) of the LPs agree to do so.\n\n\n\nThis provision allows for the firing of the GP or the termination of a fund given sufficient cause (e.g., a material breach of fund prospectus).\n\n\n\nThese specify leverage limits, a minimum amount of diversification, etc.\n\n\n\nThis provision allows the LPs to invest in other funds of the GP at low or no management fees. The provision also prevents the GP from using capital from different funds to invest in the same portfolio company.\n\n\n\nimage-20211012215558904\n\n\n\n\n\nimage-20211012215603136\n\n\n\n\n\n\n\n\n\nAt cost, adjusting for subsequent financing and devaluation.\nAt the minimum of cost or market value.\nBy revaluing a portfolio company anytime there is new financing.\nAt cost, with no adjustment until exit.\nBy using a discount factor for restricted securities (e.g, those that can only be sold to qualified investors).\nLess frequently, by applying illiquidity discounts to values based on those of comparable publicly traded companies.\n\n\n\n\n\nFirst, if the NAV is only adjusted when there are subsequent rounds of financing, then the NAV will be more stable when financing are infrequent.\nSecond, there is no definitive method of calculating NAV for a private equity fund because the market value of portfolio companies is usually not certain until exit.\nThird, undrawn LP capital commitments are not included in the NAV calculation but are essentially liabilities for the LP.\nFourth, the investor should be aware that funds with different strategies and maturities may use different valuation methodologies. In the early stages, method based on comparables may be used.\nFinally, it is usually the GP who values the fund. LPs are increasingly using third parties to value private equity funds.\n\n\n\n\nimage-20211012222118978\n\n\n\n\n\n\n\nFirst, private equity funds have returns that tend to persist.\nSecond, the return discrepancy between outperformers and underperformers is very large and can be as much as 20%.\nThird, private equity investments are usually illiquid, long-term investments.\n\n\n\n\n\n\n\n\nimage-20211012222202864\n\n\n\n\n\nLiquidity risk\nUnquoted investments risk\nCompetitive environment risk - The competition for finding reasonably-priced private equity investments may be high.\nAgency risk - The managers of private equity portfolio companies may not act in the best interests of the private equity firm and investors.\nCapital risk - Increases in business and financial risks may result in a withdrawal of capital. Portfolio companies may find the subsequent rounds of financing are difficult to obtain.\nRegulatory risk\nTax risk\nValuation risk - The valuation of private equity investments reflects subjective, not independent, judgment.\nDiversification risk - Private equity investments may be poorly diversified.\nMarket risk - Private equity is subject to long-term changes in interest rates, exchange rates, and other market risks. Short-term changes are usually not significant risk factors.\n\n\n\n\n\nTransaction costs - These costs include those from due diligence, bank financing, legal fees from acquisition, and sales transactions in portfolio companies.\nInvestment vehicle fund setup costs - The legal and other costs of setting up the fund are usually amortized over the life of the fund.\nAdministrative costs - These are charged on a yearly basis and include custodian, transfer agent, and accounting costs.\nAudit costs\nManagement and performance costs - 2% for the management fee and a 20% fee for performance.\nDilution costs - Additional rounds of financing and stock options granted to portfolio company management will result in dilution. This is also true for options issued to the private equity firm.\nPlacement fees - Placement agents who raise funds for private equity firms may charge up-front fees as much as 2% or annual trailer fees as a percent of funds raised through limited partners.\n\n\n\n\n\n\n\nThe return metric recommended for private equity by the Global Investment Performance Standards (GIPS) is the IRR. The IRR is a cash-weighted (money-weighted) return measure. Although the private equity fund portfolio companies are actually illiquid, IRR assumes intermediate cash flows are reinvested at the IRR.\n\n\n\nThe IRR can be calculated gross or net of fees. Gross IRR is the relevant measure for the cash flows between the fund and portfolio companies.\n\n\n\nNet IRR can differ substantially from Gross IRR because it is net of management fees, carried interest, and other compensation to the GP. Net IRR is the relevant measure for the cash flows between the fund and LPs and is therefore the relevant return metric for the LPs.\n\n\n\nMultiples are a popular tool of LPs due to their simplicity, ease of use, and ability to differentiate between realized and unrealized returns. Multiples, however, ignore the time value of money.\n\n\nPIC (paid-in capital)\nDPI (distributed to paid-in capital)\nRVPI (residual value to paid-in capital)\nTVPI (total value to paid-in capital)\n\n\n\n\nThe realized investments, with an evaluation of successes and failrues.\nThe unrealized investments, with an evaluation of exit horizons and potential problems.\nCash flow projections at the fund and portfolio company level.\nFund valuation, NAV, and financial statements.\n\nAs an example, consider a fund that was started before the financial market collapse of 2007. If the RVPI is large relative to the DPI, this indicates that the firm has not successfully harvested many of its investments and that the fund may have an extended J-curve (it is taking longer than realized to earn a positive return on its investments). The investor should carefully examine the GP’s valuations of the remaining portfolio companies, potential write-offs, and whether the routes for future exit have dried up.\n\n\n\n\nThe benchmarking of private equity investments can be challenging. Private equity funds vary substantially from one to another.\nBecause there are cyclical trends in IRR returns, the Net IRR should be benchmarked against a peer group of comparable private equity funds of the same vintage and strategy.\nNote also that the private equity IRR is cash flow weighted whereas most other asset class index returns are time weighted.\n\n\n\nimage-20211012224009277\n\n\n\n\n\nimage-20211012224104884\n\n\n\n\n\n\n\n\n\nimage-20211012224114211\n\n\n \\[\n\\\\\\operatorname{NAV before distribution}_{t} = \\operatorname{NAV after distribution}_{t-1} + \\operatorname{capital called down}_{t} - \\operatorname{management fees}_{t} \\pm\\operatorname{Operation result}_{t}\n\\\\\\operatorname{NAV before distribution} - \\operatorname{GP carry} - \\operatorname{LP distribution} = \\operatorname{NAV after distribution}\n\\] \n\n\n\n\n\n\n\n\n\nimage-20211012224507257\n\n\n\n\n\nimage-20211012224512707\n\n\n\n\n\n\nThe discount rate used and the estimate of terminal value will strongly influence the current valuation.\nProjections by entrepreneurs are typically overly optimistic and based on an assumption that the company will not fail. Instead of arguing over the validity of the projections with the entrepreneurs, most investors simply apply a high discount rate that reflects both the probability of failure and lack of diversification available in these investments.\n\n\nTo adjust the discount rate to reflect the risk that the company may fail in any given year. \\[\n\\\\\\text{probability of failure} = q\n\\\\r^{*} = \\frac{1 + r}{1 - q} - 1\n\\] Alternatively, the investor could have deflated each future cash flow for the cumulative probability that the company will fail. The adjusted discount rate approach is more straightforward.\n\n\n\nimage-20211012225115729\n\n\n\n\n\nA second approach to generating a realistic valuation is to adjust the terminal value for the probability of failure or poor results. We should therefore use scenario analysis to calculate an expected terminal value, reflecting the probability of different terminal value under different assumptions.\nDifficulties\n\n진정으로 유사한 comparable 찾기 어려움\nvolatile multiple <- scenario analysis 적용\n\n\n\n\nimage-20211012225254131\n\n\nIn summary, VC valuation is highly dependent on the assumption used and how risk is accounted for. Additionally, scenario and sensitivity analysis should be used to determine how changes in the input variables will affect the valuation of the company.\nThe purpose is to place some bounds on the value of the company before negotiations begin between the startup (investee) company and the private equity firm. The final price paid for the investee company will also be affected by the bargaining power of the respective parties.\n\n\n\n\n\nEnergy - crude oil, natural gas, and refined petroleum products\nIndustrial metals - aluminum, nickel, zinc, lead, tin, iron, and copper\nGrains - wheat, corn, soybeans, and rice\nLivestock - hogs, sheep, cattle, and poultry\nPrecious metals - gold, silver, and platinum\nSofts (cash crops) - coffee, sugar, cocoa, and cotton\n\n\n\n\nViscosity / Sulfur\nStorage - 장기보관\nDrilling / Extraction technology\nRefining technology\nEconomic cycle\nAlternative source of energy\nEnvironmental concern - ESG\nPolitical risk\n\n\n\n\n\nOnly stored for short period\nGeographic concentration\nSeasonal effect - vacation, travel, heating oil\n\n\n\n\n\nTransportation cost\nVery little processing\nAssociated gas / Unassociated gas\nSimilar to crude oil supply / demand\nMore influence from seasonality - heating, cooling\n\n\n\n\n\nGDP growth, business cycle\nLow storage cost\nPolitical factors - Union strike, environment\nLarge scale\n\n\n\n\n\nJewelry - value storage, hedge for inflation\nIndustrial - business cycle\n\n\n\n\n\nGrain price - Grain price up -> 도살 증가 -> price down -> 후에 price up\nWeather\nDisaster\nIncome growth in developing countries\n\n\n\n\n\nWeather - mild climate\nDisease\nIncome in developing countries\nConsumer taste\n\n\n\n\n\n\n\nDrill -> Extract -> Transport -> Store -> Refine -> Transport\n\n\n\n\nMinimal processing\nCooled to liquid\nDeliver year round-seasonal demand\n\n\n\n\n\nSmelted\nStore indefinitely\nMonthly futures available\nEconomy of scale - Billions of dollars, significant time\n가격이 하락해도 계속 생산 -> chicken game\n\n\n\n\n\n동물별로 cycle이 다름\n냉동보관\n계절적 요인\n\n\n\n\n\nSeasonal\n수요예측 후 재배 시작\n남반구 / 북반구 rotation으로 서로 보완\n\n\n\n\n\n거의 매월 수확 - 전세계\n4년 후 수확 - 투자와 회수 time gap 존재\n생산자 futures 매도로 hedge\n\n\n\n\n\nUnlike stocks and bonds, commodities are physical assets, have no cash flows, and may incur storage and transportation costs.\nStocks and bonds (financial assets) can be valued by calculating the present value of their expected future cash flows. Commodities produce no earnings or cash flow; however, the current (spot) price of a commodity can be viewed as the discounted value of the expected selling price at some future date. Storage costs for commodities can lead to forward prices that are higher the further the forward settlement date is in the future.\n\n\n\nTraders and investors in the commodities market can be classified as informed investors-those who provide liquidity to the markets-and arbitrageurs. Hedger are considered informed investors because they either produce or use the commodity. Hedgers reduce their risk by buying (going long) or selling (going short) futures contracts. A corn farmer can reduce the uncertainty about the price she will receive for her corn by selling corn futures. A cattle producer, however, would hedge his price risk by buying corn futures to reduce his uncertainty about the cost of feed for the cattle.\nSpeculators take on commodity risk in futures markets and may act as informed investors, seeking to exploit an information or information processing advantage to profit from trading with hedgers. Speculators can also earn profits by providing liquidity to markets: buying futures when short hedgers (commodity producers) are selling and selling futures when long hedgers (commodity users) are buying. - market making, liquidity 제공\nArbitrageurs in the commodity markets are often those in the business of buying, selling, and storing the physical commodities when the difference between spot and futures prices is too large or too small based on the actual cost of storing the commodity. When the difference is too large, an arbitrageur can buy and store the commodity and sell it at its (too high) futures price. When the difference is too small, an arbitrageur can effectively “not store” the commodity by selling from his own inventory and going long futures, replacing the inventory at the future date.\nlarge difference -> long spot, short futures\nsmall difference -> short spot, long futures\nCommodity exchanges operate in many of the world’s financial centers to reflect the worldwide operation and consumption of commodities as well as the globalization of financial markets in general.\nCommodity market analysts, considered non-market participants, use market information to perform analytical work for various entities including governments, universities, economic forecasters, and commercial data analysis firms.\nCommodity regulators are responsible for the regulation of commodities markets around the world.\n\n\n\n\n\n\nimage-20211012235307425\n\n\nThe difference between the spot (cash) market price and the futures price for a date in the future in referred to as the basis of the particular contract. The difference between the futures price of a nearer maturity and the futures price of a more-distant maturity is known as the calendar spread.\nWhen futures price are higher at dates further in the future, the futures market (or futures curve) is said to be in contango. In a contango market, the calendar spread and basis are negative. Conversely, if futures prices are lower at dates further in the future, the market is said to be in backwardation, and the basis and calendar spread are positive.\nWhen a future market is in backwardation, long futures positions have a positive return component. Since futures prices converge to spot prices over the term of a future contract, there is a positive returns component from the passage of time.\nWhen a future market is in contango, there is a negative returns component for long futures positions. Convergence of futures prices to spot prices results in a decrease in the value of a long futures position.\n\n\n\n\n\n\n생산자 - 미래가격 확정희망\n그래서 Futures < spot\nNormal backwardation\n실증연구결과 - extra return X, 현실에서 contango 만연\n\n\n\n\n\nProducer and consumer hedging pressure 존재\nProducer pressure up -> backwardation / Consumer pressure up -> contango\n실제, producer pressure > consumer pressure\nProducer & Consumer는 speculation도 한다 -> hedging pressure과 backwardation/contango 상관관계 검증이 어려움\n\n\n\n\n\nBackwardation / contango rely on relationship between cost and benefit of storage\nCost > benefit -> Futures > Spot -> Contango / Cost < benefit -> Futures < spot -> Backwardation\nBenefit = convenience yield\nStorage cost up -> futures price up / convenience yield up -> futures price down\n\n\n\n\n\n\n\n\nimage-20211013000454237\n\n\nAn investor who desires long exposure to a commodity price will typically achieve this exposure through a derivative investment in forwards or futures.\nCollateral return, price return, and roll return\nIf US Treasury bills are deposited as collateral, the collateral return or collateral yield is simply the holding period yield on the T-bill.\nThe price return or spot yield on an investment in commodity futures is the change in spot prices. \\[\n\\\\\\operatorname{price return} = (\\operatorname{current price} - \\operatorname{previous price}) / \\operatorname{previous price}\n\\] This process is referred to as rolling over the position and leads to gains or losses which are termed the roll return or roll yield. The roll return can be positive if the futures price curve is in backwardation or negative if the futures price curve is in contango. \\[\n\\\\\\operatorname{roll return} = \\frac{\\operatorname{price of expiring futures contract} - \\operatorname{price of new futures contract}}{\\operatorname{price of expiring futues contract}}\n\\] Roll return has a relatively small impact on overall returns on commodity futures over the short term but can have a meaningful impact over longer periods.\n\n\n\n\n\n\n\n\n\nimage-20211013000543780\n\n\nIn total return swap the swap buyer (the long) will receive periodic payments based on the change in the futures price of a commodity plus the return on the collateral, in return for a series of fixed payments.\nThe swap buyer instead may make a single payment at the initiation of the swap and then receive periodic payments based on the total returns, excess returns, or price volatility of a commodity, essentially “buying” exposure to the underlying risk factor.\nIn an excess return swap, a party may make a single payment at the initiation of the swap and then receive periodic payments of any percentage by which the commodity price exceeds some fixed or benchmark value, times the notional value of the swap. In months in which the commodity price does not exceed the fixed value, no payments are made.\nIn a basis swap, the variable payments are based on the difference between the prices of two commodities. Often two commodities are one that has liquid traded futures available for hedging and the other (the one the swap buyer actually uses in production) with no liquid futures contracts available. By combining a hedge using the liquid futures with a basis swap, the swap buyer can hedge the price risk he faces from the input that does not have a liquid futures market.\nIn a commodity volatility swap, the underlying factor is the volatility of the commodity’s price. If the volatility of the commodity’s price is higher than the expected level of volatility specified in the swap, the volatility buyer receives a payment. When actual volatility is lower than the specified level, the volatility seller receives a payment.\n\n\n\n\n\n\nimage-20211013001132649\n\n\nTo be most useful, an index should be investable.\nThe available commodity indexes differ in the following dimensions:\n\nWhich commodities are included\nThe weighting of the commodities in the index\nThe method of rolling contracts over as they near expiration\nThe method of rebalancing portfolio weights\n\nWhile no index methodology will consistently outperform another index methodology, differences in methodology do result in returns differences, at least over shorter periods. Over long periods, differences between the mix and weights of constituent commodities in individual indexes will result in differences between returns, as some commodities outperform others.\nIndexes may be equal weighted or weighted on some factor, such as the value of global production of an individual commodity or commodity sector. A production value weighted index will have more exposure to energy than to livestock or softs, for example.\nWith regard to roll methodology, a passive strategy may be a simply roll the expiring futures contracts into the near-month contract each month. A more active strategy would be to maximize roll return by selecting the further-out contracts with the greatest backwardation or smaller contango.\nThe frequency of rebalancing will also affect commodity index returns. Rebalancing portfolio weights will decrease return when prices are trending but increase returns when price changes are choppy and mean-reverting.\nWhile differences in index construction methodology will lead to difference among index returns over relative shorter periods, no one methodology is necessarily superior over longer periods. Correlations between returns on different indexes have been relatively high, while correlations between commodity indexes and returns on stocks and bonds have been low."
  },
  {
    "objectID": "posts/2021-10-18-digital-system-circuits-week-7/2021-10-18-digital-system-circuits-week-7.html",
    "href": "posts/2021-10-18-digital-system-circuits-week-7/2021-10-18-digital-system-circuits-week-7.html",
    "title": "Digital System Circuits Week 7",
    "section": "",
    "text": "HDL = Hardware Description Language\n\n\n\nimage-20211018084333616\n\n\n\n\n\n\n\n\nimage-20211018084430524\n\n\nNet list\n\n\n\n\n\n\nimage-20211018084548539\n\n\n\n\n\n\n\n\nimage-20211018085017419\n\n\n\nServe as input for synthesis tools\n\n\n\n\n\n\n\nimage-20211018085503030\n\n\n\n\n\n\n\n\nimage-20211018090138632\n\n\n\n\n\n\n\n\nimage-20211018090219746\n\n\n\n\n\n\n\n\nimage-20211018090432186\n\n\n\n\n\n\n\n\nimage-20211018090447851\n\n\n\n\n\n\n\n\nimage-20211018090657522\n\n\n\n\n\nimage-20211018090718849\n\n\n\n\n\n\n\n\nimage-20211018092000735\n\n\n\n\n\n\n\n\nimage-20211018091853424\n\n\n\n\n\nimage-20211018092401575\n\n\n\n\n\nimage-20211018092744449\n\n\n\n\n\nimage-20211018092818681\n\n\ncircle 하나 당 term 하나 -> 최대한 큰 circle 하나를 그리는 것이 식이 간단해짐\n\n\n\n\n\n\nimage-20211018093005514\n\n\n\n\n\nimage-20211018093131691\n\n\n\n\n\nimage-20211018093138860\n\n\n\n\n\nimage-20211018093301896\n\n\n\n\n\n\n\n\nimage-20211018093520452\n\n\nX는 1에도 포함 될 수도 있고, 0에도 포함 될 수 있다."
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html",
    "title": "Digital System Circuits Week 6",
    "section": "",
    "text": "image-20211007131425282\n\n\n\n\n\nimage-20211007131434954\n\n\n\n\n\nimage-20211007131536125\n\n\n\n\n\nimage-20211007131654062\n\n\n\n\n\n\n\nimage-20211007131705582\n\n\n\n\n\n\n\n\nimage-20211007132127367\n\n\n\n\n\n\n\n\nimage-20211007132235512\n\n\n\n\n\nimage-20211007132708963\n\n\n\n\n\n\n\n\nimage-20211007132821767\n\n\n\n\n\nimage-20211007132951199\n\n\n\n\n\nimage-20211007133116999\n\n\nDecoder: Code -> Signal / Encoder: Signal -> Code\nMux\n\n\n\nimage-20211007133339738\n\n\nDemux\n\n\n\nimage-20211007133402448\n\n\nMux / Demux는 회선의 방향을 결정할 뿐, 0과 1의 결정에는 관여하지 않는다. 어디로 출력이 나갈지, 어디로 입력을 받을지 결정하는 것.\n\n\n\n\n\nimage-20211007133723156\n\n\n\n\n\nimage-20211007133843501\n\n\n\n\n\nimage-20211007134315165\n\n\n\n\n\n\n\n\n\nimage-20211007134739202\n\n\n\n\n\n\n\n\nimage-20211007134942656\n\n\n\n\n\n\n\n\nimage-20211007135105884\n\n\n\n\n\nHow can we build better circuits?\n\nDelay - the time from input changing to new correct stable output\nSize - the number of transistors\n\n\n\n\nimage-20211008024400694\n\n\n\n\n\nimage-20211008024412197\n\n\ntransistor 갯수는 입력 당 2개 생각하면 됨\n\n\n\nImproves some, but worsens other, criteria of interest\n\n\n\nimage-20211008024628036\n\n\nOptimization은 해야 하는 것, Tradeoff는 선택의 문제\n\n\n\nOptimizations\n\nAll criteria of interest are improved (or at least kept the same)\n\nTradeoff\n\nSome criteria of interest are improved, while others are worsened.\n\nWe obviously prefer optimizations, but often must accept tradeoffs.\n\nYou can’t build a car that is the most comfortable and has the best fuel efficiency, and is the fastest - you have to give up something to gain other things."
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#algebraic-two-level-size-optimization",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#algebraic-two-level-size-optimization",
    "title": "Digital System Circuits Week 6",
    "section": "Algebraic two-level size optimization",
    "text": "Algebraic two-level size optimization\n\n\n\nimage-20211008025347002"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#karnaugh-maps-for-two-level-size-optimization",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#karnaugh-maps-for-two-level-size-optimization",
    "title": "Digital System Circuits Week 6",
    "section": "Karnaugh maps for two-level size optimization",
    "text": "Karnaugh maps for two-level size optimization\nKarnaugh maps (K-maps)\n\n\n\nimage-20211008025721847\n\n\nK-map에서 인접한 11은 줄일 수 있다는 의미"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#what-is-an-hdl",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#what-is-an-hdl",
    "title": "Digital System Circuits Week 6",
    "section": "What is an HDL?",
    "text": "What is an HDL?\n\n\n\nimage-20211008032320925\n\n\nIt looks like a programming language but not a programming language.\n\nIt is always critical to recall you are describing hardware.\nThis code’s primary purpose is to generate hardware.\nThe hardware this code describes (a counter) can be simulated on a computer. In this secondary use of the language it does act more like a programming language."
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#simulating-validating-hdl",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#simulating-validating-hdl",
    "title": "Digital System Circuits Week 6",
    "section": "Simulating / Validating HDL",
    "text": "Simulating / Validating HDL\n\n\n\nimage-20211008032329562"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#what-is-synthesis",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#what-is-synthesis",
    "title": "Digital System Circuits Week 6",
    "section": "What is synthesis?",
    "text": "What is synthesis?\n\nTakes a description of what a circuit does\nCreates the hardware to do it\n\n\n\n\nimage-20211008032546838"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#synthesizing-the-hardware-described",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#synthesizing-the-hardware-described",
    "title": "Digital System Circuits Week 6",
    "section": "Synthesizing the hardware described",
    "text": "Synthesizing the hardware described\n\nAll hardware created during synthesis\n\nEven if a is true, still computing d&c\n\nlearn to understand how descriptions are translated to hardware\n\n\n\n\nimage-20211008032827627"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#why-use-an-hdl",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#why-use-an-hdl",
    "title": "Digital System Circuits Week 6",
    "section": "Why use an HDL?",
    "text": "Why use an HDL?\nEnables larger designs via rich syntax, modularity\n\nmore abstract than schematics, allows larger designs\n\nregister transfer level description\nwide datapaths (16, 32, or 64 bits wide) can be abstracted to a single vecotr\nsynthesis tool does the bulk of the tedious repetitive work vs schematic capture\n\n\nPortable design\n\nBehavioral or dataflow Verilog can be synthesized to a new process library with little effort\n\n\n\n\nimage-20211008033148367\n\n\n\n\n\nimage-20211008033334873"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#week-5",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#week-5",
    "title": "Digital System Circuits Week 6",
    "section": "Week 5",
    "text": "Week 5\n\n\n\nimage-20211008034552330\n\n\n\n\n\nimage-20211008034606282\n\n\n\n\n\nimage-20211008034615692\n\n\n\n\n\nimage-20211008034623776\n\n\n\n\n\nimage-20211008034630789\n\n\n\n\n\nimage-20211008034638331\n\n\n\n\n\nimage-20211008034644713"
  },
  {
    "objectID": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#week-6",
    "href": "posts/2021-10-07-digital-system-circuits-week-6/2021-10-07-digital-system-circuits-week-6.html#week-6",
    "title": "Digital System Circuits Week 6",
    "section": "Week 6",
    "text": "Week 6\n\n\n\nimage-20211008034716106\n\n\n\n\n\nimage-20211008034722899\n\n\n\n\n\nimage-20211008034735938\n\n\n\n\n\nimage-20211008041829289\n\n\n\n\n\nimage-20211008034747145\n\n\n\n\n\nimage-20211008034754012\n\n\n\n\n\nimage-20211008034801316\n\n\n\n\n\nimage-20211008034811025\n\n\n\n\n\nimage-20211008044410708\n\n\n\n\n\nimage-20211008034817913"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Siyun Min",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUCB\n\n\nCoursera\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2022\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSSU\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2021\n\n\nSiyun Min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCFA\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2021\n\n\nSiyun Min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Siyun Min",
    "section": "",
    "text": "I’m still building this blog."
  }
]