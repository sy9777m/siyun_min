<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siyun Min">
<meta name="dcterms.date" content="2022-12-06">

<title>Siyun Min - Artificial Intelligence 20221206</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Siyun Min</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">Siyun Min</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/sy9777m"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kevinmin77/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Artificial Intelligence 20221206</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">SSU</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Siyun Min </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 6, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="section" class="level1">
<h1>20221206</h1>
</section>
<section id="가치-함수-value-function" class="level1">
<h1>가치 함수 (value function)</h1>
<section id="상태-가치-함수-state-value-function-v_pis" class="level2">
<h2 class="anchored" data-anchor-id="상태-가치-함수-state-value-function-v_pis">상태 가치 함수 (state value function) <span class="math inline">\(V_\pi(s)\)</span></h2>
<p>상태 s에서 시작하여 정책 <span class="math inline">\(\pi\)</span>에 따라 행동을 할 때 얻게 되는 기대 누적 보상 (expected reward summation)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
<section id="상태-행동-가치-함수-state-action-value-function-q_pis-a" class="level2">
<h2 class="anchored" data-anchor-id="상태-행동-가치-함수-state-action-value-function-q_pis-a">상태-행동 가치 함수 (state-action value function) <span class="math inline">\(Q_\pi(s, a)\)</span></h2>
<p>상태 s에서 행동 a를 한 다음, 정책 <span class="math inline">\(\pi\)</span>에 따라 행동을 할 때 얻게 되는 기대 누적 보상</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
<section id="벨만-기대-방정식-bellman-expectation-equation" class="level2">
<h2 class="anchored" data-anchor-id="벨만-기대-방정식-bellman-expectation-equation">벨만 기대 방정식 (Bellman expectation equation)</h2>
<p>상태 가치 함수와 상태-행동 가치 함수의 관계</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
<p>기대 값이 계산 가능한 형태의 벨만 기대 방정식</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="최적-정책-optimal-policy" class="level1">
<h1>최적 정책 (optimal policy)</h1>
<section id="최적-정책-optimal-policy-pi과-최적-상태-가치-함수-v" class="level2">
<h2 class="anchored" data-anchor-id="최적-정책-optimal-policy-pi과-최적-상태-가치-함수-v">최적 정책 (optimal policy) <span class="math inline">\(\pi^*\)</span>과 최적 상태 가치 함수 <span class="math inline">\(V^*\)</span></h2>
<p><span class="math display">\[
\pi^*(s) = maxV_\pi(s)~or~argmaxQ^*(s, a)
\]</span></p>
</section>
<section id="bellman-최적-방정식-optimality-equation" class="level2">
<h2 class="anchored" data-anchor-id="bellman-최적-방정식-optimality-equation">Bellman 최적 방정식 (optimality equation)</h2>
<p>최적 정책에 따른 가치 함수들이 만족하는 성질</p>
<section id="최적-상태-가치-함수" class="level3">
<h3 class="anchored" data-anchor-id="최적-상태-가치-함수">최적 상태 가치 함수</h3>
<p><span class="math display">\[
V^*(s) = argmax\sum_{s'}{P_{ss'}^a [r_{ss'}^a + \gamma V^*(s')]}
\]</span></p>
<p>모든 가능한 행동 중에서 가장 큰 기대 누적 보상값을 주는 상태 가치</p>
</section>
<section id="최적-행동-가치-함수" class="level3">
<h3 class="anchored" data-anchor-id="최적-행동-가치-함수">최적 행동 가치 함수</h3>
<p><span class="math display">\[
Q^*(s, a) = \sum_{s'}{P_{ss'}^a [r_{ss'}^a + \gamma V^*(s')]}
\]</span></p>
</section>
</section>
</section>
<section id="mdp-동적-프로그래밍-dynamic-programming" class="level1">
<h1>MDP 동적 프로그래밍 (dynamic programming)</h1>
<section id="dynamic-programming-dp-동적-프로그래밍" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-programming-dp-동적-프로그래밍">Dynamic programming, DP (동적 프로그래밍)</h2>
<p>큰 문제를 작은 문제들로 분할하여 작은 문제의 해결을 통해 큰 문제의 해를 찾는 방법</p>
<p>재귀적 최적화 문제를 해결</p>
</section>
<section id="정책-반복-policy-iteration" class="level2">
<h2 class="anchored" data-anchor-id="정책-반복-policy-iteration">정책 반복 (policy iteration)</h2>
<section id="정책-평가-policy-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="정책-평가-policy-evaluation">정책 평가 (policy evaluation)</h3>
<p>상태가치함수의 벨만 기대 방정식을 반복적으로 수행</p>
<p><span class="math display">\[
V_{k + 1}(s) = \sum_{a \in A}{\pi(a | s)} \sum_{s' \in S_{t + 1}}{P_{ss'}^a [r_{ss} + \gamma V_k(s'))]}
\]</span></p>
</section>
<section id="정책-개선-policy-improvement" class="level3">
<h3 class="anchored" data-anchor-id="정책-개선-policy-improvement">정책 개선 (policy improvement)</h3>
<p>벨만 최적 가치 함수를 반복적으로 최대화하는 정책을 결정</p>
<p><span class="math display">\[
\pi^*(s) = max V_\pi(s)
\]</span></p>
</section>
</section>
<section id="가치-반복-value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="가치-반복-value-iteration">가치 반복 (value iteration)</h2>
<p>벨만 최적 상태 가치 함수를 반복적으로 계산하는 방법</p>
<p><span class="math display">\[
V_{k + 1}(s) = argmax \sum_{s'}{P_{ss'}^a [r_{ss'}^a + \gamma V_k(s')]}
\]</span></p>
</section>
<section id="정책-평가-policy-evaluation-pi-to-v_pi" class="level2">
<h2 class="anchored" data-anchor-id="정책-평가-policy-evaluation-pi-to-v_pi">정책 평가 (policy evaluation) <span class="math inline">\(\pi \to V_\pi\)</span></h2>
<p>주어진 정책 <span class="math inline">\(\pi\)</span>를 따를 때, 각 상태에서 얻게 되는 기대 보상 값 <span class="math inline">\(V_\pi\)</span> 계산</p>
<p><span class="math display">\[
V_{k + 1}(s) = \sum_{a}{\pi(s, a)} \sum_{k'}{P_{ss'}^a [r_{ss'}^a + \gamma V_k(s')]}
\]</span></p>
<p>임의의 가치함수 <span class="math inline">\(V_0\)</span>에서 시작하여, <span class="math inline">\(V_k\)</span>가 수렴할 때까지 반복</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
<section id="정책-개선-plicy-improvement-v_pi-to-pi" class="level2">
<h2 class="anchored" data-anchor-id="정책-개선-plicy-improvement-v_pi-to-pi">정책 개선 (plicy improvement) <span class="math inline">\(V_\pi \to \pi\)</span></h2>
<p>상태가치함수 V(s) 값으로부터 정책 <span class="math inline">\(\pi\)</span> 결정</p>
<p><span class="math display">\[
\pi'(s) = argmaxQ^\pi(s, a) = argmax \sum_{s'}{P_{ss'}^a [r_{ss'}^a + \gamma V^\pi(s')]}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
<section id="정책-반복-policy-iteration-학습" class="level2">
<h2 class="anchored" data-anchor-id="정책-반복-policy-iteration-학습">정책 반복 (policy iteration) 학습</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
<p>임의의 정책 <span class="math inline">\(\pi\)</span>에서 시작하여, <span class="math inline">\(\pi\)</span>에 대해서 Bellman 방정식이 수렴할 때까지 (즉, 바뀌지 않을 때까지) 적용하여 <span class="math inline">\(V_\pi\)</span>를 계산하고, <span class="math inline">\(V_\pi\)</span>를 사용하여 <span class="math inline">\(\pi\)</span>를 개선하는 과정을 정책 <span class="math inline">\(\pi\)</span>가 수렴할 때까지 반복</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 7.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
<section id="값-반복-value-iteration-학습" class="level2">
<h2 class="anchored" data-anchor-id="값-반복-value-iteration-학습">값 반복 (value iteration) 학습</h2>
<p><span class="math display">\[
V_{k + 1}(s) = max \sum_{s'}{P_{ss'}^a [r_{ss'}^a + \gamma V_k(s')]}
\]</span></p>
<p>임의의 가치 함수 <span class="math inline">\(V_0\)</span>에서 시작하여 정책은 계산하지 않고 가치 함수가 수렴할 때까지 반복</p>
<p>수렴한 가치함수 V를 사용하여 정책 <span class="math inline">\(\pi\)</span>를 결정</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 8.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="강화학습-기초" class="level1">
<h1>강화학습 기초</h1>
<section id="정책-반복-가치-반복-학습-알고리즘-동적-프로그래밍" class="level2">
<h2 class="anchored" data-anchor-id="정책-반복-가치-반복-학습-알고리즘-동적-프로그래밍">정책 반복, 가치 반복 학습 알고리즘 (동적 프로그래밍)</h2>
<p>정확한 MDP 모델이 필요 (상태 천이 확률 <span class="math inline">\(P_{ss'}^ar_{ss'}\)</span>)</p>
<p>실제 상황에서는 정확한 MDP 모델을 모르는 경우가 많음</p>
</section>
<section id="강화-학습의-특징" class="level2">
<h2 class="anchored" data-anchor-id="강화-학습의-특징">강화 학습의 특징</h2>
<p>완전한 상태 천이 확률과 보상 함수를 미리 알 수 없음</p>
<p>상태 집합이 무한함, 강화학습을 적용하는 대부분의 문제들은 상태가 연속적이라 상태의 수가 큼</p>
</section>
<section id="model-based-vs.-model-free" class="level2">
<h2 class="anchored" data-anchor-id="model-based-vs.-model-free">Model-based vs.&nbsp;Model-free</h2>
<p>MDP에서 이 상태 천이 확률과 보상 함수를 모델(model)이라 함</p>
<section id="model-based-강화학습" class="level3">
<h3 class="anchored" data-anchor-id="model-based-강화학습">Model-based 강화학습</h3>
<p>MDP에서 상태 천이 확률과 보상함수를 정할 수 있는 경우</p>
<p>Model-based로 풀 수 있는 문제는 매우 제한적</p>
<p>MDP와 동적 프로그래밍이 model-based에 해당</p>
</section>
<section id="model-free-강화학습" class="level3">
<h3 class="anchored" data-anchor-id="model-free-강화학습">Model-free 강화학습</h3>
<p>행동 공간이 적고 미리 상태 천이 확률과 보상 함수를 알고 있는 경우</p>
<p>대부분의 강화학습 기법들은 model-free임</p>
</section>
</section>
<section id="bootstrap-부트스트랩" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap-부트스트랩">bootstrap (부트스트랩)</h2>
<p>다음 상태에 대한 가치 함수 값으로 현재 상태의 가치 함수 값을 예측하는 방식</p>
<section id="에피소드-episode" class="level3">
<h3 class="anchored" data-anchor-id="에피소드-episode">에피소드 (episode)</h3>
<p>강화학습에서 종료 상태 (terminal state)까지의 상태 천이가 진행된 일렬의 데이터</p>
<p>부트스트랩을 사용하면 에피소드가 진행되는 동안에 상태 가치 함수를 업데이트 할 수 있음</p>
</section>
</section>
<section id="on-policy-vs.-off-policy" class="level2">
<h2 class="anchored" data-anchor-id="on-policy-vs.-off-policy">On-policy vs.&nbsp;Off-policy</h2>
<p>On-policy 강화학습은 학습에 사용한 경험을 정책이 업데이트되면 이 경험들을 학습에 사용할 수 없음</p>
<p>데이터 사용 효율 관점에서 off-policy가 좋음</p>
<section id="on-policy" class="level3">
<h3 class="anchored" data-anchor-id="on-policy">On-policy</h3>
<p>행동을 결정하는 정책 (policy)과 학습할 정책이 같은 강화학습</p>
</section>
<section id="off-policy" class="level3">
<h3 class="anchored" data-anchor-id="off-policy">Off-policy</h3>
<p>행동하는 정책과 학습하는 정책이 다른 방법</p>
</section>
</section>
<section id="탐색-exploitation과-탐험-exploration" class="level2">
<h2 class="anchored" data-anchor-id="탐색-exploitation과-탐험-exploration">탐색 (exploitation)과 탐험 (exploration)</h2>
<section id="exploitation-탐색" class="level3">
<h3 class="anchored" data-anchor-id="exploitation-탐색">exploitation (탐색)</h3>
<p>현 상태에서 알고 있는 한 가장 최적의 행동을 선택하는 것</p>
<p>탐욕(greedy)적으로 행동을 선택</p>
</section>
<section id="탐험-exploration" class="level3">
<h3 class="anchored" data-anchor-id="탐험-exploration">탐험 (exploration)</h3>
<p>다양한 경험을 쌓기 위해 무작위 행동을 선택</p>
<p>탐욕적으로만 행동을 선택하다 보면 정작 정말 최적인 행동을 전혀 선택하지 못할 수도 있음</p>
</section>
</section>
<section id="몬테카를로-학습-monte-carlo-learning-mc" class="level2">
<h2 class="anchored" data-anchor-id="몬테카를로-학습-monte-carlo-learning-mc">몬테카를로 학습 (Monte-Carlo learning, MC)</h2>
<section id="monte-carlo-prediction-몬테카를로-예측" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-prediction-몬테카를로-예측">Monte-Carlo prediction (몬테카를로 예측)</h3>
<p>샘플링을 통해 상태 가치 함수를 학습</p>
<p>충분한 에피소드 (episode, a sample)를 거치고 나면 상태 가치 함수가 정답에 가까워 짐</p>
<p><span class="math display">\[
V_{n + 1}(s) = \frac{1}{n}\sum_{i = 1}^n{G_i} = \frac{1}{n}(G_n + \sum_{i = 1}^{n - 1}{G_i}) = \frac{1}{n}(G_n + (n - 1) \frac{1}{n - 1}\sum_{i = 1}^{n - 1}{G_i}) = \frac{1}{n}(G_n + (n - 1)V_n(s)) = V_n(s) + \frac{1}{n}(G_n - V_n(s))
\]</span></p>
<p>이전 상태 가치 함수와 새로운 획득 보상과의 차를 더해 상태 가치 함수를 업데이트 함</p>
<p><span class="math inline">\(G_n - V_n(s)\)</span>: 오차 (error)</p>
<p>1/n을 <span class="math inline">\(\alpha\)</span>로 치환하여 학습 속도 (learning rate)로 여김 →</p>
<p><span class="math display">\[
V_{n + 1}(s) = V_n(s) + \alpha(G_n - V_n(s))
\]</span></p>
<p>총 획득 보상 G는 에피소드가 끝나야 알 수 있음</p>
</section>
<section id="몬테카를로-제어" class="level3">
<h3 class="anchored" data-anchor-id="몬테카를로-제어">몬테카를로 제어</h3>
<p>상태가치 함수 대신 행동가치 함수를 사용</p>
<p><span class="math display">\[
Q_{n + 1}(s, a) = Q(s, a) + \alpha(G_n - Q_n(s, a))
\]</span></p>
</section>
</section>
<section id="시간차-학습-temporal-difference-learning-td" class="level2">
<h2 class="anchored" data-anchor-id="시간차-학습-temporal-difference-learning-td">시간차 학습 (temporal-difference learning, TD)</h2>
<section id="시간차-예측" class="level3">
<h3 class="anchored" data-anchor-id="시간차-예측">시간차 예측</h3>
<p><span class="math display">\[
V(S_t) = V(S_t) + \alpha (R_t - V(S_t)) \gets G_n = r_{t + 1} + \gamma V(S_{t + 1}) = V(S_t) + \alpha (r_{t + 1} + \gamma V(S_{t + 1}) - V(S_t))
\]</span></p>
<p>몬테카를로 예측과 다르게 매 시간마다 가치 함수를 갱신할 수 있음</p>
<p>다음 상태 가치로 가치 함수를 갱신함으로 TD는 부트스트랩 알고리즘</p>
</section>
</section>
<section id="sarsa-state-action-reward-state-action-강화학습" class="level2">
<h2 class="anchored" data-anchor-id="sarsa-state-action-reward-state-action-강화학습">SARSA (State-Action-Reward-State-Action) 강화학습</h2>
<p>On-policy 시간차 제어 (on-policy temporal-difference control)</p>
<section id="정책-결정" class="level3">
<h3 class="anchored" data-anchor-id="정책-결정">정책 결정</h3>
<p>Greedy method (탐욕 방법)</p>
<ul>
<li>현재 상태에서 가장 큰 행동 가치함수 값을 주는 행동을 선택</li>
</ul>
<p><span class="math display">\[
\pi(s) = argmax Q(s, a)
\]</span></p>
<p><span class="math inline">\(\epsilon\)</span>-greedy method</p>
<ul>
<li><span class="math inline">\(1 - \epsilon\)</span>의 확률로는 탐욕방법처럼 수행하고, <span class="math inline">\(\epsilon\)</span>의 확률로는 무작위로 행동을 선택</li>
</ul>
<p>정책 결정으로부터 샘플 (s, a, r, s’, a’)를 구성해서 Q를 계산함</p>
<p><span class="math display">\[
Q(s, a) = Q(s, a) + \alpha(r + \gamma Q(s' + a') - Q(s, a))
\]</span></p>
<p>다음 상태 s’에서 가장 큰 Q값을 이용하여 Q함수를 갱신</p>
</section>
</section>
<section id="q-learning-알고리즘과-dqn-deep-q-network-강화학습" class="level2">
<h2 class="anchored" data-anchor-id="q-learning-알고리즘과-dqn-deep-q-network-강화학습">Q-learning 알고리즘과 DQN (Deep Q-network) 강화학습</h2>
<p>현재 상태 s에서 <span class="math inline">\(\epsilon\)</span>-greedy 방법을 적용함으로써 현재 상태 s에서 행동 a를 실행하여 보상 r을 받고, 다음 상태 s’를 결정하여, 샘플 (s, a, r, s’)를 구성</p>
<p>Bellman 최적 방정식을 이용하여 Q(s, a)를 갱신함</p>
<p><span class="math display">\[
Q(s, a) = Q(s, a) + \alpha (r + \gamma argmax Q(s', a') - Q(s, a))
\]</span></p>
<ul>
<li>Q함수를 갱신하기 위해 다음 상태 s’에서의 행동 a’를 결정하는 정책을 최대로 하는 행동 a’, 실제로 다음 상태로 가서 하는 행동을 결정하는 정책이 다름 (off-policy 정책)</li>
</ul>
<p>여기서, Q함수를 심층신경망을 사용하면 DQN (Deep Q-network)이 됨</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Untitled 9.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Untitled</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="강화학습-알고리즘" class="level1">
<h1>강화학습 알고리즘</h1>
<p>몬테카를로 방법 (Monte Carlo method)</p>
<p>시간 차이 학습 (temporal difference learning, TD-learning)</p>
<p>정책 그레디언트 알고리즘 (policy gradient algorithm)</p>
<ul>
<li>연속구간 행동을 갖는 강화학습</li>
</ul>
</section>
<section id="역강화-학습-inverse-reinforcement-learning" class="level1">
<h1>역강화 학습 (inverse reinforcement learning)</h1>
<p>보상함수가 직접적으로 제공되지 않는 경우 적용</p>
<p>전문가의 바람직한 행동 시연이 가능한 상황</p>
<p>시연을 관측한 데이터로부터 보상함수를 학습 → 보상함수를 사용하여 가치함수를 학습하고 정책 결정</p>
<p>상태 s에 대한 전형적인 보상함수 R(s)의 표현</p>
<ul>
<li>상태 s의 특징 <span class="math inline">\(\phi_i(s)\)</span>들에 대한 선형결합 표현</li>
</ul>
<p><span class="math display">\[
R(s) = \sum_{i = 1}^N{\omega_i \phi_i(s)}
\]</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>